---
layout: home
title: Apunts Processament del llenguatge natural
parent: 7. Processament del llenguatge natural
math: mathjax3
---

## 7. Processament del llenguatge natural

### Models d'intel¬∑lig√®ncia artificial

![NLP-for-Beginners-Pythons-Natural-Language-Toolkit-NLTK_Watermarked.webp](../images%2FNLP-for-Beginners-Pythons-Natural-Language-Toolkit-NLTK_Watermarked.webp)

# Processament del llenguatge natural

* **Definici√≥:** camp de la IA que tracta de la interacci√≥ entre els ordinadors i el llenguatge hum√†.
* Se centra en la **comprensi√≥** i **generaci√≥** de llenguatge hum√†.
* Un dels camps m√©s actius i complexos de la IA.

![right fit](../images%2Fperiodic-table-of-nlp-tasks-high.png)

## Aplicacions

* Traducci√≥ autom√†tica.
* Reconeixement de veu.
* S√≠ntesi de veu.
* Generaci√≥ i resum de text.
* An√†lisi de sentiments.
* Classificaci√≥ de text.

## Introducci√≥ (I)

* Camp multidisciplinari que combina:
    * Ling√º√≠stica.
    * Intel¬∑lig√®ncia artificial.
    * Ci√®ncies cognitives.
    * Inform√†tica.
    * etc.

## Introducci√≥ (II)

* √âs un problema **dif√≠cil** perqu√®:
    * El llenguatge hum√† √©s **ambigu**.
    * El llenguatge hum√† √©s **ric**.
    * El llenguatge hum√† √©s **contextual**.
    * El llenguatge hum√† √©s **cultural**.
* Aquestes caracter√≠stiques fan que el llenguatge hum√† no sigui **formal** i, per tant, no es puga tractar amb les t√®cniques de la IA tradicional.

# El text com a dada

![](../images%2Ftext.jpg)

## Introducci√≥

* El text √©s una font de dades **molt important**.
* Els humans **generem** i **consumim** text de forma **massiva**
* El saber com tractar el text √©s **cr√≠tic** per a moltes aplicacions.

> Anomenem **text** a una seq√º√®ncia coherent de s√≠mbols que pot ser interpretada com a un conjunt de paraules, utilitzant les regles gramaticals i sint√†ctiques d'una llengua.

## Significat

* Qu√© entenem per **significat**?:
    * el significat d'una paraula √©s el concepte que representa.
    * el significat d'una frase √©s el concepte que representa la combinaci√≥ de les paraules que la formen.
    * el significat d'un text √©s el concepte que representa la combinaci√≥ de les frases que el formen.

$$\text{significant}(\text{simbol}) \Leftrightarrow \text{significat}(\text{idea})$$
$$\text{arbre} \Leftrightarrow \text{\{üå≤, üå≥, üå¥, } \dots \}$$

## Significat en els ordinadors

* Com poden coneixer els ordinadors el significat de les paraules, frases i textos?.
* T√©cniques classiques: bases de dades de sin√≥nims i hyper√≤nims.
    * `WordNet`: base de dades l√®xica que relaciona paraules entre si.
    * Els sin√≥nims permeten relacionar paraules amb el mateix significat.
        * Ex: `ro√Øn` √©s un sin√≤nim de `dolent`, `malparit`, `miserable`, etc.
    * Els hyper√≤nims permeten relacionar paraules amb significats m√©s generals.
        * Exemple: `carnivor`, `vertebrat` serien hyper√≤nims de `gat`.

## Problemes de WordNet i semblants

* Molt √∫til per√≤ sense mat√≠s sem√†ntic.
    * Ex: `gat` i `fel√≠` s√≥n sin√≤nims, per√≤ no tenen el mateix significat.
* Tots els s√≠nonims no seran √∫tils en tots els contextos.
    * Ex: `cabr√≥` √©s un sin√≤nim de `ro√Øn`, per√≤ no sempre es poden intercanviar.
* Actualitzar la base de dades √©s un proc√©s **manual**, **cost√≥s** i subjectiu.
    * Ex: `the shit` √©s un sin√≤nim de `the best` en angl√®s que no apareix en WordNet.
* Les solucions modernes es basen en les representacions del text.

## Representacions del text (I)

* Els ordinadors necessiten representar el text com a dades num√®riques.
* Quina unitat de text triem per a representar el text i com ho fem?.
* Aquestes decisions determinaran la **complexitat** del model i la **qualitat** del resultat.
* Els models de representaci√≥ del text s√≥n **molt importants** en el processament del llenguatge natural.
* A continuaci√≥ veurem algunes de les t√®cniques m√©s utilitzades.

## Representaci√≥ de car√†cters

* Cada car√†cter es representa per un n√∫mero.
    * El diccionari ser√† molt petit (en el cas de l'ASCII, 128 car√†cters).
    * El model ha de ser molt complex, ja que ha d'aprendre a combinar els car√†cters per a formar paraules.
    * Exemple: "AND" es pot representar com a $$[65, 78, 68]$$

![right fit](../images%2Fascii-character-map.png)

## Representaci√≥ de paraules

* Cada paraula √©s representada per un n√∫mero.
    * El diccionari ser√† molt gran, ja que cada paraula ser√† una entrada en el diccionari.
    * El model ser√† m√©s senzill, ja que les paraules s√≥n unitats sem√†ntiques.
    * Exemple: "BE" es pot representar com a 2.

![inline 50%](../images%2Fbow_2.png)

## Representaci√≥ de subparaules

* Cada subparaula √©s representada per un n√∫mero.
    * El diccionari ser√† m√©s petit que el de paraules, ja que les subparaules s√≥n unitats sem√†ntiques.
    * El model ser√† m√©s senzill, ja que les subparaules s√≥n unitats sem√†ntiques.
        * Permet representar paraules rares i que no estan en el vocabulari.
        * √ötil per a lleng√ºes amb moltes paraules compostes i derivades.

## Tokens i tokenitzaci√≥ (I)

* Indepententment de l'enfocament triat, el text ha de ser **dividit** en **tokens**.
    * Ex: "New York in winter" $$\rightarrow$$ `["New", "York", "in", "winter"]`
* **Token**: unitat m√≠nima de text que pot ser considerada com a una unitat sem√†ntica.
    * Pot ser una paraula, una subparaula, un signe de puntuaci√≥, un n√∫mero, etc; qualsevol unitat que es tri√Ø per a ser tractada com a unitat m√≠nima.
* **Tokenitzaci√≥**: proc√©s de dividir un text en tokens i que facilita el tractament i comprensi√≥ del text.
    * √âs un proc√©s **no trivial**. Dep√®n de la llengua i del domini.
        * Exemple: "New York" √©s un token o dos?

## Tokens i tokenitzaci√≥ (II)

* Q√ºestions a tindre en compte:
    * **Puntuaci√≥**: es considera un token o no?. Pot variar la interpretaci√≥ del text.
    * **Majuscules/Min√∫scules**: es consideren tokens diferents o no?.
    * **Stopwords**: paraules que no aporten informaci√≥ al text (articles, preposicions, etc.).
    * **Idioma i domini**: el proc√©s de tokenitzaci√≥ dep√®n de l'idioma i del domini del text.

## Tokens i tokenitzaci√≥ (III)

* N-grams: seq√º√®ncies de n tokens consecutius.
* Algunes paraules tenen significat propi, per√≤ la seva combinaci√≥ amb altres paraules tamb√© t√© un significat. Ex: "New York".
* Els n-grams permeten representar aquestes combinacions de paraules, augmentant el vocabulari amb les combinacions d'n-tokens que triem.
* Bigrams: seq√º√®ncies de dos tokens consecutius, trigrams: seq√º√®ncies de tres tokens consecutius, etc.
* Problema: augmenta molt el vocabulari i la complexitat del model.

## Vectoritzaci√≥

* La **vectoritzaci√≥** √©s el proc√©s de convertir un text en un vector num√®ric.
* Els vectors s√≥n m√©s f√†cils de manipular per a les m√†quines i de comparar.
* Algunes t√©cniques de vectoritzaci√≥ (e
* mbeddings):
    * **NNLM**: model basat en xarxes neuronals. El nombre de dimensions √©s fixe.
    * **Word2Vec**: cada paraula √©s un vector en un espai sem√†ntic. El nombre de dimensions √©s fixe.
    * **FastText**: creat per Facebook. Similar a Word2Vec, per√≤ permet representar paraules rares i que no estan en el vocabulari.
    * **GloVe**: model basat en NNLM i Word2Vec.

## Word2Vec

* Es basa en la idea que les paraules amb significats similars apareixen en contextos similars.
    * "You shall know a word by the company it keeps" (J.R. Firth, 1957).
* Quan una paraula **p** apareix en un text, les paraules properes a `p` s√≥n el seu **context**.
* Els diferents contextos de `p` defineixen el significat de `p`.

![inline 70%](../images%2FCaptura%20de%20pantalla%202024-01-14%20a%20las%2021.35.56.png)

## Word2Vec (II)

* Per cada paraula obtenim un vector **dens** i de **longitud fixa**.
* Cada dimensi√≥ del vector representa un **aspecte sem√†ntic** de la paraula.
* Representen la seva **posici√≥** en un espai sem√†ntic n-dimensional.
* Els vectors de paraules amb contextos semblants estaran propers en l'espai sem√†ntic.

> Facilita calcular la similitud entre paraules.

![right fit](../images%2Fembeddings.png)

## Word2Vec (III)

* Els vectors de parales tamb√© s'anomenen **embeddings** o **representacions de xarxa**.
* **FastText** √©s una variant de **Word2Vec** que utilitza subparaules.
    * Permet representar paraules rares i que no estan en el vocabulari.
    * √ötil per a lleng√ºes amb moltes paraules compostes i derivades.
* Els _embeddings_ generats poden ser utilitzats en una gran varietat de tasques, com pot ser la classificaci√≥ de textos, an√†lisi de sentiments, etc.
* S√≥n models que necessiten un entrenament previ amb totes les paraules del vocabulari. A continuaci√≥ veurem un exemple.

## Generaci√≥ de _embeddings_ amb Word2Vec i la llibreria Gensim

```python
from gensim.models import Word2Vec

sentences = [
    ["Gavi", "company", "Pedri"],
    ["Xavi", "entrena", "Bar√ßa"],
    ["Gavi", "juga", "Bar√ßa"],
    ["Gavi", "chuta"],
    ["Kepa", "para"],
    ["Xavi", "entrena"],
]
model = Word2Vec(sentencias, min_count=1)
```

## Visualitzaci√≥ dels _embeddings_

```python
gavi = model.wv["Gavi"]
print(gavi)
```

```python
[
    -0.00536227  0.00236431  0.0510335   0.09009273
                                         - 0.0930295 - 0.07116809 0.06458873  0.08972988
                                         - 0.05015428 - 0.03763372
]
```

## Busquem paraules similars

```python
model.wv.most_similar("Gavi")
```

```python
[('Bar√ßa', 0.5436005592346191),
 ('Pedri', 0.3792896568775177),
 ('entrena', 0.3004249036312103),
 ('Xavi', 0.10494352877140045),
 ('juga', -0.1311161071062088),
 ('chuta', -0.1897382140159607),
 ('para', -0.22418655455112457),
 ('Kepa', -0.2726020812988281),
 ('company', -0.7287455797195435)]
```

## Similitud del cosinus

* Els _embeddings_ generats per Word2Vec s√≥n vectors de _n_ dimensions.
* Per a calcular la similitud entre dos vectors s'utilitza la **similitud del cosinus**.
* El resultat √©s un valor entre -1 i 1.
    * -1: vectors oposats.
    * 0: vectors ortogonals.
    * 1: vectors iguals.
* Aquesta mesura √©s molt utilitzada en el processament del llenguatge natural.

# Similitud entre textos

![](../images%2Fsimilarity.jpg)

## Representaci√≥ de textos

* Fins ara hem vist com representar paraules.
* Per veure la similitud entre textos hem de representar els textos en la seva totalitat.
* Solament aix√≠ podrem veure les relacions entre les paraules que el formen.
* Algunes de les t√®cniques utilitzades s√≥n:
    * **One-hot encoding**: cada token $$\rightarrow$$ una dimensi√≥; valor $$\rightarrow$$ 1 si el token est√† i 0 si no.
    * **Bag of Words**: model basat en freq√º√®ncies. El valor de cada cel¬∑la son les aparicions del token en el document. Simple i encara molt utilitzat.
    * **TF-IDF**: model basat en freq√º√®ncies i inversa de freq√º√®ncies
    * **Word Embeddings**. Vectoritzaci√≥ de paraules amb _Word2Vec_, _FastText_, etc

### One-hot encoding

* El model **one-hot encoding** √©s un model basat en tokens.
* Els vectors generats s√≥n **dispersos** i **grans**, ja que cada token √©s una dimensi√≥.
* Cada token √©s una dimensi√≥ i el valor de cada cel¬∑la √©s 1 si el token est√† i 0 si no.
* Els vectors generats s√≥n **independents** de la sem√†ntica.
* No facilita calcular la similitud entre paraules i textos.

![right fit](../images%2Fone_hot.png)

### Bag of Words (BoW)

* El model **BoW** √©s un model basat en freq√º√®ncies.
* Es pot entendre com una suma dels vectors one-hot.
* Els vectors generats s√≥n **independents** de la sem√†ntica.
* El nombre del token es pot entendre com a **ordre** i en molts casos no √©s aix√≠. Aquesta discrep√†ncia pot afectar a la qualitat del model.

![right fit](../images%2Fbag-of-words.png)

### TF-IDF

* El model **TF-IDF** √©s un model basat en freq√º√®ncies i inversa de freq√º√®ncies.
* Semblant al model **BoW**, per√≤ t√© en compte la freq√º√®ncia del token en el document i en el conjunt de documents.
* El valor de cada cel¬∑la √©s el producte de la freq√º√®ncia del token en el document i la inversa de la freq√º√®ncia del token en el conjunt de documents.
* Dona m√©s import√†ncia als tokens que apareixen en pocs documents. Ra√≥: els tokens que apareixen en molts documents no solen aportar informaci√≥ rellevant.

### Word Embeddings

* Com ja hem vist, els _embeddings_ generats per Word2Vec s√≥n vectors de _n_ dimensions.
* Per a representar un text pot utilitzar-se la mitjana dels _embeddings_ de les paraules que el formen.
* Els vectors generats s√≥n **densos**, de **longitud fixa** i amb **sentit sem√†ntic**.
* Facilita calcular la similitud entre paraules i textos.

### Transformers

* Els **transformers** s√≥n models complexos basats en xarxes neuronals recurrents.
* Poden aprendre la sem√†ntica del text i generar els seus propis _embeddings_ utilitzant el mecansime d'**atenci√≥**.
* Demostren un gran rendiment en moltes tasques, com pot ser el c√†lcul de la similitud entre textos.
* S√≥n complexos i necessiten un entrenament previ amb un gran volum de dades.

## Similitud entre textos

* La similitud entre textos √©s una mesura que indica com de semblants s√≥n dos textos.
* √âs una de les funcions m√©s obvies del processament del llenguatge natural.
* El c√†lcul de la similitud entre textos, per√≤, √©s una tasca **dif√≠cil**.
* Anem a veure algunes t√®cniques de les m√©s utilitzades.

### T√©cniques per a calcular la similitud entre textos (I)

* **Basades en en regles**: Es basen en regles predefinides; f√†cils d'implementar i √∫tils per a casos senzills.
    * **Dist√†ncia de Levenshtein**: √âs el nombre m√≠nim d'operacions per a transformar una cadena en una altra.
    * **Dist√†ncia de Hamming**: √âs el nombre de posicions en les quals dues cadenes de la mateixa longitud difereixen.
    * **Recompte de paraules**: √âs el nombre com√∫ de paraules entre dos textos.
    * **Dist√†ncia de Jaccard**: √âs el nombre de paraules comunes entre dos textos dividit pel nombre total de paraules dels dos textos.

### T√©cniques per a calcular la similitud entre textos (II)

* **Basades en caracter√≠stiques sint√†ctiques**: Es basen en les caracter√≠stiques sint√†ctiques i gramaticals dels textos. Impliquen un proc√©s de **parsejat** dels textos per analitzar la seva
  estructura sint√†ctica.
* **Basades en caracter√≠stiques sem√†ntiques**: Es basen en les caracter√≠stiques sem√†ntiques dels textos. Aqu√≠ models com Word2Vec s√≥n molt √∫tils, al permetre representar el significat contextual de
  les paraules.
    * **Word Mover's Distance**: Mesura la dist√†ncia entre dos textos com la dist√†ncia entre els vectors de les paraules dels dos textos.
    * **Similitud del cosinus**: Utilitza el cosinus de l'angle entre ells.

### T√©cniques per a calcular la similitud entre textos (II)

* **Basades en l'aprenentatge autom√†tic**: Es basen en l'aprenentatge autom√†tic per a calcular la similitud entre textos.
    * **BERT i GPT**: Models de llenguatge basats en xarxes neuronals que pot ser utilitzat per a calcular la similitud entre textos.
    * **Universal Sentence Encoder**: Model espec√≠ficament entrenat per al _transfer learning_ (aprenentatge per a la transfer√®ncia; utilitzar un model entrenat per a una tasca per a una altra).

### Utilitats de la similitud entre textos

* **Correcci√≥ ortogr√†fica**: Per a corregir una paraula es busca la paraula m√©s semblant.
* **Classificaci√≥ de textos**: Per a classificar un text es busca el text m√©s semblant.
* **Agrupaci√≥ de textos**: √ötil per a agrupar textos similars en clusters.
* **B√∫squeda de resposte**: Per a trobar la resposta a una pregunta es busquen texts semblants a la pregunta.

# An√†lisi de sentiments

![](../images%2Fsentiment.png)

## An√†lisi de sentiments

* L'an√†lisi de sentiments √©s una de les tasques m√©s utilitzades en el processament del llenguatge natural.
* L'objectiu √©s determinar l'actitud d'un autor respecte a un tema o producte.
* Es basa en la **polaritat** del text, que pot ser **positiva**, **negativa** o **neutra**.
* Tamb√© poden buscar-se emocions concretes, com pot ser **alegria**, **tristesa**, **ira**, etc.

## An√†lisi subjectiva i objectiva

* L'an√†lisi de sentiments pot ser **subjectiva** o **objectiva**.
* L'an√†lisi subjectiva busca les **emocions i sentiments** de l'autor.
* L'an√†lisi objectiva es basa en **fets i dades concretes**.
* Els dos tipus d'an√†lisi s√≥n **complementaris** i poden utilitzar-se conjuntament.
* Ex: "_La pel¬∑l√≠cula t√© grans moments, per√≤ el final √©s molt trist_".
    * An√†lisi subjectiva: "_La pel¬∑l√≠cula √©s bona_".
    * An√†lisi objectiva: "_El final √©s trist_".

## Preprocessament del text

* El **tractament** de textos facilita obtindre bons resultats en NLP.
* Permet redu√Ør la **dimensionalitat** dels textos, eliminar el soroll i capturar la sem√†ntica.
* Algunes de les t√®cniques m√©s utilitzades s√≥n:
    * **Tokenitzaci√≥**: vist anteriorment.
    * **Normalitzaci√≥**: convertir el text a un format est√†ndard.
    * **Eliminaci√≥ de stopwords**: eliminar paraules que no aporten informaci√≥.
    * **Stemming** i **lematitzaci√≥**: convertir les paraules a la seva forma base.
    * **Gesti√≥ de negacions i modalitats**: convertir a un format est√†ndard.

### Preprocessament: tokenitzaci√≥

* Com ja hem vist, la tokenitzaci√≥ √©s el proc√©s de dividir un text en tokens.
* Els tokens poden ser paraules, subparaules, signes de puntuaci√≥, etc.
* Facilita un an√†lisi m√©s profund del text i l'extreure caracter√≠stiques rellevants.
* Ex: "El Bar√ßa est√† en crisi"  $$\rightarrow$$ `["El", "Bar√ßa", "est√†", "en", "crisi"]`.

### Preprocessament: normalitzaci√≥

* La **normalitzaci√≥** implica el¬∑liminar els elements que no aporten informaci√≥.
    * Nombres, signes de puntuaci√≥, etc.
* Tamb√© implica convertir el text a un format est√†ndard, passant a min√∫scules i llevant espais innecessaris, per exemple.
    * Ex: "El Bar√ßa est√† en crisi! üò°"  $$\rightarrow$$ "el bar√ßa est√† en crisi".
* La normalitzaci√≥ facilita la comparaci√≥ entre textos i la detecci√≥ de paraules clau.

### Preprocessament: eliminaci√≥ d'stopwords

* Les **stopwords** s√≥n paraules que no aporten informaci√≥ al text.
* S√≥n paraules molt comunes en un idioma, com pot ser articles, preposicions, etc.
* Els textos despr√©s de processar-se amb stopwords s√≥n m√©s f√†cils de tractar i m√©s r√†pids de processar.
* Ex: "El Bar√ßa est√† en crisi"  $$\rightarrow$$ `["Bar√ßa", "crisi"]`.

### Preprocessament: stemming i lematitzaci√≥

* El **stemming** i la **lematitzaci√≥** s√≥n t√®cniques per a convertir les paraules a la seva forma base i facilitar l'agrupaci√≥ de paraules relacionades.
* L'**stemming** √©s un proc√©s heur√≠stic basat en regles, mentre que la **lematitzaci√≥** √©s un proc√©s basat en coneixements ling√º√≠stics.
    * Ex d'stemming: "jugar", "jugar√©", "jugar√†"  $$\rightarrow$$ "jug".
    * Ex de lematitzaci√≥: "jugar", "jugar√©", "jugar√†"  $$\rightarrow$$ "jugar".
* El stemming √©s m√©s r√†pid, per√≤ la lematitzaci√≥ √©s m√©s precisa.

### Preprocessament: negacions i modalitats

* Les **negacions** i **modalitats** poden canviar el significat d'una frase.
* Ex: _"El Bar√ßa no est√† en crisi"_, _"El Bar√ßa pot estar en crisi"_, _"Deuries anar a l'estadi"_.
* Els models de NLP no poden interpretar aquestes frases sense un tractament previ.
* Necessitem t√©cniques espec√≠fiques, com la detecci√≥ de **dobles negacions** i la **reassignaci√≥ de polaritat**.

## Enfocaments per a l'an√†lisi de sentiments

* Com en totes les tasques de NLP, l'an√†lisi de sentiments pot ser abordada amb diferents enfocaments.
* Els enfocaments m√©s utilitzats s√≥n:
    * Basats en **regles**
    * Basats en l'**aprenentatge autom√†tic supervisat**
    * Basats en l'**aprenentatge autom√†tic no supervisat**

### Enfocament basat en regles

* Els enfocaments basats en regles s√≥n els m√©s senzills i r√†pids.
* Tamb√© s'anomenen **lexicon-based**.
* Es basen llistes de paraules per a determinar la polaritat del text.
* De cada paraula es busca la seva polaritat en el llistat i es fa una suma..
    * _"El Bar√ßa est√† en crisi"_. _"crisi"_ $$\rightarrow$$ **-1** - $$\rightarrow$$ **negatiu**.
    * _"Me fa il¬∑lusi√≥ anar a l'estadi"_. _"il¬∑lusi√≥"_  $$\rightarrow$$ **1**  $$\rightarrow$$ **positiu**.
    * _"El partit va acabar en empat"_. _"empat"_  $$\rightarrow$$ **0**  $$\rightarrow$$ **neutre**.

### Enfocament basat en l'AA supervisat

* Els enfocaments basats en l'aprenentatge autom√†tic supervisat s√≥n alguns dels m√©s utilitzats.
* Consisteixen en entrenar un model amb un conjunt de dades etiquetades amb la polaritat del text.
* El model identifica els patrons en el text que determinen la polaritat.
* Alguns dels models m√©s utilitzats s√≥n: **Naive Bayes**, **Support Vector Machines**, **Random Forest**, **Xarxes neuronals**

### Enfocament basat en l'AA no supervisat

* Els enfocaments basats en l'aprenentatge es basen en identificar patrons en el text sense necessitar etiquetes predefinides.
* Es poden utilitzar t√®cniques com la **clusteritzaci√≥** per a agrupar els textos en clusters segons la seva polaritat.
* Una vegada agrupats els textos, es poden etiquetar manualment els clusters.
* Aquestes t√®cniques s√≥n √∫tils per a detectar patrons en el text i per a agrupar textos semblants (segons la dist√†ncia entre textos).

## Models per a l'an√†lisi de sentiments

Alguns dels models m√©s utilitzats per a l'an√†lisi de sentiments s√≥n:

* **BOW** + clasificador: model basat en BoW i un classificador.
* **Embeddings** + clasificador: Word2Vec, FastText, etc.
* **VADER**: model basat en regles, molt utilitzat en angl√®s.
* **Transformers**: com ja hem vist, els transformers s√≥n models espec√≠fics per a NLP molt potents. Un dels models m√©s utilitzats √©s **BERT**, de q√ºal utilitzarem una implementaci√≥ en la segona
  pr√†ctica.

# Models de llenguatge

![](../images%2Flanguage-model.png)

## Models de llenguatge

* Fins ara hem vist com representar el text i com acomplir tasques com l'an√†lisi de sentiments.
* Hem comentat que els models de llenguatge s√≥n eines molt potentes que poden ser utilitzades en moltes tasques.
* En aquesta secci√≥ veurem qu√® s√≥n els models de llenguatge i com funcionen.
* Tamb√© veurem alguns dels m√©s utilitzats i les seves aplicacions.

## Aplicacions dels models de llenguatge

* Els models de llenguatge s√≥n un dels camps m√©s actius i complexos de la IA.
* S√≥n la base de moltes aplicacions de NLP, com pot ser:
    * **Traducci√≥ autom√†tica**: traduir un text d'un idioma a un altre.
    * **Reconeixement de veu**: transcriure un text a partir d'un arxiu d'√†udio.
    * **S√≠ntesi de veu**: generar un arxiu d'√†udio a partir d'un text.
    * **Generaci√≥ i resum de text**: generar textos a partir d'una seq√º√®ncia de paraules.
    * **An√†lisi de sentiments**: determinar la polaritat d'un text.
    * **Classificaci√≥ de text**: classificar un text en una categoria.
    * **Generaci√≥ de textos**: generar textos a partir d'un tema o un estil.

### Definici√≥

* Un **model de llenguatge** assigna una probabilitat a una seq√º√®ncia de paraules.
    * Per tant, permet predir la seg√ºent paraula d'una seq√º√®ncia.
    * Ex: "El bar√ßa est√† en ___" $$\rightarrow$$ `[{crisi: 0.8}, {forma: 0.1}, {casa: 0.1}]`
* Es basen en la idea que les paraules d'una seq√º√®ncia no s√≥n independents, sin√≥ que depenen de les paraules anteriors.
* Permeten calcular la "**validesa**" d'una seq√º√®ncia de paraules.
    * No √©s el mateix que la **correcci√≥** d'una seq√º√®ncia de paraules.
    * Intentem modelar el llenguatge hum√†, amb els seus **matissos i ambig√ºitats**.

## Hist√≤ria: Models basats en regles

* Els models de llenguatge s√≥n un dels camps m√©s antics del processament del llenguatge natural.
* Es basen en regles gramaticals i ling√º√≠stiques per definir les caracter√≠stiques del llenguatge.
* Les regles estan definides per experts i s√≥n dif√≠cils de modificar.
* No s√≥n flexibles i no poden adaptar-se a nous contextos.
* Ex: Gram√†tica de Chomsky, Gram√†tica de Montague, etc.

## Hist√≤ria: Models estoc√†stics

* Els models basats en regles van ser substitu√Øts per models basats en **estad√≠stiques**, m√©s flexibles i que poden modelar millor el llenguatge hum√†.
* Es basen en la idea que les paraules d'una seq√º√®ncia no s√≥n independents, sin√≥ que depenen de les paraules anteriors. Exemples:
    * **N-gram**: modela cada paraula en funci√≥ de les n paraules anteriors. (uni, bi, tri, etc).
    * **Skip-gram**: modela cada paraula en funci√≥ de les n paraules anteriors i posteriors.
    * **Syntax-based**: es basen en l'estructura sint√†ctica de les frases i no en la seva seq√º√®ncia.

## Hist√≤ria: RNN

* Els models basats en estad√≠stiques van ser substitu√Øts per models basats en **xarxes neuronals**.
* Els primers models d'aquest tipus van ser els **RNN** (xarxes neuronals recurrents).
    * A difer√®ncia de les xarxes neuronals tradicionals, les RNN tenen **mem√≤ria**.
    * L'entrada d'una neurona pot anar determinada per la sortida d'ella mateixa.
    * Permeten processar seq√º√®ncies de longitud variable.
    * Els models de llenguatge basats en RNN van ser els primers en obtenir resultats acceptables.

## Hist√≤ria: LLM

* Els models basats en RNN van ser substitu√Øts per models basats en **transformers**.
* Els transformers s√≥n models basats en xarxes neuronals que utilitzen el mecanisme d'**atenci√≥**.
    * S√≥n m√©s potents que les RNN i permeten obtenir resultats molt millors.
    * S√≥n els models m√©s utilitzats en l'actualitat.
    * Necessiten un entrenament previ amb un **gran** volum de dades (_corpus_)
    * Mostren la capacitat d'entendre el context, la sem√†ntica i la sint√†xis del text.

## Hist√≤ria: LLM (II)

* El mecanisme d'atenci√≥ √©s un mecanisme que permet a les xarxes neuronals aprendre a **centrar-se** en les parts importants de les seves entrades.
* √âs un mecanisme que imita el comportament hum√†.
* Podem entendre'l com una **capa** que s'afegeix a una xarxa neuronal.
* Els transformers s√≥n models basats en xarxes neuronals que utilitzen una variant del mecanisme d'atenci√≥ anomenada **self-attention**.

## Entrenament de models de llenguatge

* Els models de llenguatge necessiten un entrenament previ amb un **gran** volum de dades.
* Aquestes dades s'anomenen **corpus**.
* Els corpus s√≥n **molt grans** i dif√≠cils de generar.
* Poden ser **generats manualment** o **autom√†ticament**.
* Varien en contingut: not√≠cies, llibres, xats, etc.
* Poden incloure **etiquetes** per a entrenar models supervisats.

## Parts d'un LLM

* Els models de llenguatge basats en transformers s√≥n models molt complexos.
* Tenen dues parts principals:
    * **Encoder**: codifica el text d'entrada en un vector.
    * **Decoder**: decodifica el vector en un text de sortida.
    * Segons quines parts estiguen presents o no podran ser **bidireccionals** o **unidireccionals**.
    * Aix√≥ determinar√† tamb√© les tasques que poden realitzar.

## Parts d'un LLM (II)

* Encoder: codifica el text d'entrada en un vector.
* Els models `encoder-only` s√≥n **unidireccionals** i s'especialitzen en "entendre" el text d'entrada i, per tant, s√≥n √∫tils per a tasques com la classificaci√≥ de text.
* Solament necessiten el **encoder** per a realitzar la tasca perqu√© no necessiten generar un text de sortida.
* Utilitats: classificaci√≥ de text, an√†lisi de sentiments, etc.
* Ex: **BERT**, RoBERTa, ALBERT, ELECTRA, etc.

## Parts d'un LLM (III)

* Decoder: decodifica el vector en un text de sortida.
* Els models `Decoder-only` solament poden accedir a les paraules anteriors i, per tant, s√≥n √∫tils per a tasques com la generaci√≥ de text.
* Utilitats: generaci√≥ de text, escritura creativa, etc.
* Ex: **GPT**, GPT-2, GPT-3, Mixtral, etc.

## Parts d'un LLM (IV)

* Encoder + Decoder: codifica el text d'entrada en un vector i decodifica el vector en un text de sortida.
* Els models `Encoder-Decoder` poden accedir a les paraules anteriors i posteriors i, per tant, s√≥n √∫tils per a tasques com la traducci√≥ autom√†tica.
* Utilitats: traducci√≥ autom√†tica, resum de text, esquematitzaci√≥ de text, etc.
* Ex: **T5**, BART, etc.

## Utilitzaci√≥ dels LLM

* Els transformers s√≥n models molt complexos i necessiten un entrenament previ amb un gran volum de dades.
* Normalment s'utilitzen models ja entrenats i que poden ser utilitzats per a diferents tasques.
* Per a millorar el rendiment dels models entrenats es pot utilitzar el **fine-tuning**.
    * El fine-tuning consisteix en entrenar el model amb un conjunt de dades espec√≠fic per a la tasca que volem realitzar.
    * Per a aix√≤, s'utilitza un conjunt de dades m√©s petit que el corpus original.
    * En les pr√†ctiques utilitzarem un model ja entrenat i li farem fine-tuning per a millorar el rendiment en les tasques que vulgam realitzar.
