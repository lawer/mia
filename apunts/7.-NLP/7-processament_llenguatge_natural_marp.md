---
marp: true
size: 16:9
theme: lawer
class: default
_class: invert lead
paginate: true
_paginate: false
auto-scaling: true
footer: Carles Gonz√†lez
---

<style scoped>
h1, p {
  color: #FFFFFF;
  font-weight: bold;0
  text-shadow:
    0px 0px 3px #00000;
}
</style>

# 7. Processament del llenguatge natural

Models d'intel¬∑lig√®ncia artificial

![bg opacity](../images/NLP-for-Beginners-Pythons-Natural-Language-Toolkit-NLTK_Watermarked.png)

---

<style scoped>section { font-size:33px; }</style>

## Processament del llenguatge natural

- **Definici√≥:** camp de la IA que tracta de la interacci√≥ entre els ordinadors i el llenguatge hum√†.
- Se centra en la **comprensi√≥** i **generaci√≥** de llenguatge hum√†.
- Un dels camps m√©s actius i complexos de la IA.

![bg right fit](../images/periodic-table-of-nlp-tasks-high.png)

---

## Aplicacions

- Traducci√≥ autom√†tica.
- Reconeixement de veu.
- S√≠ntesi de veu.
- Generaci√≥ i resum de text.
- An√†lisi de sentiments.
- Classificaci√≥ de text.

---

## Introducci√≥ (I)

- Camp multidisciplinari que combina:
    - Ling√º√≠stica.
    - Intel¬∑lig√®ncia artificial.
    - Ci√®ncies cognitives.
    - Inform√†tica.
    - etc.

---

## Introducci√≥ (II)

- √âs un problema **dif√≠cil** perqu√®:
    - El llenguatge hum√† √©s **ambigu**.
    - El llenguatge hum√† √©s **ric**.
    - El llenguatge hum√† √©s **contextual**.
    - El llenguatge hum√† √©s **cultural**.
- Aquestes caracter√≠stiques fan que el llenguatge hum√† no sigui **formal** i, per tant, no es puga tractar amb les t√®cniques de la IA tradicional.

---

<!-- 
_class: invert lead
-->

<style scoped>
h1, p {
  color: #FFFFFF;
  font-weight: bold;
  text-shadow:
    0px 0px 3px #000000;
}
</style>


# El text com a dada

![bg opacity](../images/text.jpg)

---

## Introducci√≥

- El text √©s una font de dades **molt important**.
- Els humans **generem** i **consumim** text de forma **massiva**
- El saber com tractar el text √©s **cr√≠tic** per a moltes aplicacions.

> Anomenem **text** a una seq√º√®ncia coherent de s√≠mbols que pot ser interpretada com a un conjunt de paraules, utilitzant les regles gramaticals i sint√†ctiques d'una llengua.

---

## Significat

- Qu√© entenem per **significat**?:
    - el significat d'una paraula √©s el concepte que representa.
    - el significat d'una frase √©s el concepte que representa la combinaci√≥ de les paraules que la formen.
    - el significat d'un text √©s el concepte que representa la combinaci√≥ de les frases que el formen.

$$\text{significant}(\text{simbol}) \Leftrightarrow \text{significat}(\text{idea})$$
$$\text{arbre} \Leftrightarrow \text{\{üå≤, üå≥, üå¥, } \dots \}$$

---
<style scoped>section { font-size:32px; }</style>

## Significat en els ordinadors

- Com poden coneixer els ordinadors el significat de les paraules, frases i textos?.
- T√©cniques classiques: bases de dades de sin√≥nims i hyper√≤nims.
    - `WordNet`: base de dades l√®xica que relaciona paraules entre si.
    - Els sin√≥nims permeten relacionar paraules amb el mateix significat.
        - Ex: `ro√Øn` √©s un sin√≤nim de `dolent`, `malparit`, `miserable`, etc.
    - Els hyper√≤nims permeten relacionar paraules amb significats m√©s generals.
        - Exemple: `carnivor`, `vertebrat` serien hyper√≤nims de `gat`.

---
<style scoped>section { font-size:32px; }</style>

## Problemes de WordNet i semblants

- Molt √∫til per√≤ sense mat√≠s sem√†ntic.
    - Ex: `gat` i `fel√≠` s√≥n sin√≤nims, per√≤ no tenen el mateix significat.
- Tots els s√≠nonims no seran √∫tils en tots els contextos.
    - Ex: `cabr√≥` √©s un sin√≤nim de `ro√Øn`, per√≤ no sempre es poden intercanviar.
- Actualitzar la base de dades √©s un proc√©s **manual**, **cost√≥s** i subjectiu.
    - Ex: `the shit` √©s un sin√≤nim de `the best` en angl√®s que no apareix en WordNet.
- Les solucions modernes es basen en les representacions del text.

---
<style scoped>section { font-size:32px; }</style>

## Representacions del text (I)

* Els ordinadors necessiten representar el text com a dades num√®riques.
* Necessitarem un **vocabulari** que relacioni les unitats de text amb els n√∫meros.
* Quina unitat de text triem per a representar el text i com ho fem?.
* Aquestes decisions determinaran la **complexitat** del model, el tamany del **vocabulari** i la **precisi√≥** del model.
* Els models de representaci√≥ del text s√≥n **molt importants** en el processament del llenguatge natural.
* A continuaci√≥ veurem algunes de les t√®cniques m√©s utilitzades.

---

<style scoped>section { font-size:31.6px; }</style>

## Representaci√≥ de car√†cters

* Cada car√†cter es representa per un n√∫mero.
    * El diccionari ser√† molt petit (en el cas de l'ASCII, 128 car√†cters).
    * El model ha de ser molt complex, ja que ha d'aprendre a combinar els car√†cters per a formar paraules.
    * Exemple: "AND" es pot representar com a $[65, 78, 68]$

![bg right:38% fit](../images/ascii-character-map.png)

---

## Representaci√≥ de paraules

* Cada paraula √©s representada per un n√∫mero.
    * El diccionari ser√† molt gran, ja que cada paraula ser√† una entrada en el diccionari.
    * El model ser√† m√©s senzill, ja que les paraules s√≥n unitats sem√†ntiques.
    * Exemple: Si el nostre vocabulari √©s `["gat", "gos", "cavall", "ocell", "peix"]`, llavors "gat" es pot representar com a $1$ i "cavall" com a $3$.

---

## Representaci√≥ de subparaules

* Cada subparaula √©s representada per un n√∫mero.
    * El diccionari ser√† m√©s petit que el de paraules, ja que les subparaules s√≥n unitats sem√†ntiques.
    * El model ser√† m√©s senzill, ja que les subparaules s√≥n unitats sem√†ntiques.
        * Permet representar paraules rares i que no estan en el vocabulari.
        * √ötil per a lleng√ºes amb moltes paraules compostes i derivades.

---

<style scoped>section { font-size:31.5px; }</style>

## Tokens i tokenitzaci√≥ (I)

* Indepententment de l'enfocament, el text s'ha de **dividir** en **tokens**.
    * Ex: "New York in winter" $\rightarrow$ `["New", "York", "in", "winter"]`
* **Token**: unitat m√≠nima de text que pot ser considerada com a una unitat sem√†ntica.
    * Una paraula, subparaula, signe de puntuaci√≥, n√∫mero, etc; qualsevol unitat que es tri√Ø com a unitat m√≠nima.
* **Tokenitzaci√≥**: proc√©s de dividir un text en tokens i que facilita el tractament i comprensi√≥ del text.
    * √âs un proc√©s **no trivial**. Dep√®n de la llengua i del domini.
        * Exemple: "New York" √©s un token o dos?

---

<style scoped>section { font-size:35px; }</style>

## Tokens i tokenitzaci√≥ (II)

* Q√ºestions a tindre en compte:
    * **Puntuaci√≥**: es considera un token o no?. Pot variar la interpretaci√≥ del text.
    * **Majuscules/Min√∫scules**: es consideren tokens diferents o no?.
    * **Stopwords**: paraules que no aporten informaci√≥ al text (articles, preposicions, etc.).
    * **Idioma i domini**: el proc√©s de tokenitzaci√≥ dep√®n de l'idioma i del domini del text.

---
<style scoped>section { font-size:32.5px; }</style>

## Tokens i tokenitzaci√≥ (III)

* N-grams: seq√º√®ncies de n tokens consecutius.
* Algunes paraules tenen significat propi, per√≤ la seva combinaci√≥ amb altres paraules tamb√© t√© un significat. Ex: "New York".
* Els n-grams permeten representar aquestes combinacions de paraules, augmentant el vocabulari amb les combinacions d'n-tokens que triem.
* Bigrams: seq√º√®ncies de dos tokens consecutius, trigrams: seq√º√®ncies de tres tokens consecutius, etc.
* Problema: augmenta molt el vocabulari i la complexitat del model.

---
<style scoped>section { font-size:34px; }</style>

## Vectoritzaci√≥ (I)

* Encara que els tokens son molt √∫tils, presenten alguns problemes:
    * No s√≥n f√†cils de manipular per a les m√†quines.
    * No s√≥n f√†cils de comparar.
    * No permeten calcular la similitud entre paraules i textos.
* La **vectoritzaci√≥** √©s el proc√©s de convertir un text en un vector num√®ric.
* Els vectors s√≥n m√©s f√†cils de manipular per a les m√†quines i de comparar.

---

## Vectoritzaci√≥ (II)

* Algunes t√©cniques de vectoritzaci√≥ (embeddings):
    * **NNLM**: model basat en xarxes neuronals. El nombre de dimensions √©s fixe.
    * **Word2Vec**: cada paraula √©s un vector en un espai sem√†ntic. El nombre de dimensions √©s fixe.
    * **FastText**: creat per Facebook. Similar a Word2Vec, per√≤ permet representar paraules rares i que no estan en el vocabulari.
    * **GloVe**: model basat en NNLM i Word2Vec.

---
<style scoped>section { font-size:30px; }</style>

## Word2Vec

* Es basa en la idea que les paraules amb significats similars apareixen en contextos similars.
    * "You shall know a word by the company it keeps" (J.R. Firth, 1957).
* Quan una paraula **p** apareix en un text, les paraules properes a `p` s√≥n el seu **context**.
* Els diferents contextos de `p` defineixen el significat de `p`.

![70%](../images/Captura%20de%20pantalla%202024-01-14%20a%20las%2021.35.56.png)

---
<style scoped>section { font-size:30.6px; }</style>


## Word2Vec (II)

* Per cada paraula obtenim un vector **dens** i de **longitud fixa**.
* Cada dimensi√≥ del vector representa un **aspecte sem√†ntic** de la paraula.
* Representen la seva **posici√≥** en un espai sem√†ntic n-dimensional.
* Els vectors de paraules amb contextos semblants estaran propers en l'espai sem√†ntic.

> Facilita calcular la similitud entre paraules.

![bg right:31% fit](../images/embeddings.png)

---

<style scoped>section { font-size:31.4px; }</style>

## Word2Vec (III)

* Els vectors de parales tamb√© s'anomenen **embeddings** o **representacions de xarxa**.
* **FastText** √©s una variant de **Word2Vec** que utilitza subparaules.
    * Permet representar paraules rares i que no estan en el vocabulari.
    * √ötil per a lleng√ºes amb moltes paraules compostes i derivades.
* Els _embeddings_ generats poden ser utilitzats en una gran varietat de tasques, com pot ser la classificaci√≥ de textos, an√†lisi de sentiments, etc.
* S√≥n models que necessiten un entrenament previ amb totes les paraules del vocabulari. A continuaci√≥ veurem un exemple.

---

<!-- 
_class: invert lead
-->


<style scoped>
h1, p {
  color: #FFFFFF;
  font-weight: bold;
  text-shadow:
    0px 0px 3px #000000;
}
</style>


# Representaci√≥ de textos

![bg opacity](../images/tf-idf.png)

---

<style scoped>section { font-size:30.8px; }</style>

## Representaci√≥ de textos

* Fins ara hem vist com representar paraules, veurem com representar textos, per poder veure les relacions entre les paraules que el formen.
* Algunes de les t√®cniques utilitzades s√≥n:
    * **One-hot encoding**: cada token $\rightarrow$ una dimensi√≥; valor $\rightarrow$ 1 si el token est√† i 0 si no.
    * **Bag of Words**: model basat en freq√º√®ncies. El valor de cada cel¬∑la son les aparicions del token en el document. Simple i molt utilitzat.
    * **TF-IDF**: model basat en freq√º√®ncies i inversa de freq√º√®ncies
    * **Word Embeddings**. Vectoritzaci√≥ de paraules amb _Word2Vec_, _FastText_, etc

---

<style scoped>section { font-size:30px; }</style>

### One-hot encoding

* El model **one-hot encoding** √©s un model basat en tokens.
* Els vectors generats s√≥n **dispersos** i **grans**, ja que cada token √©s una dimensi√≥.
* Cada token √©s una dimensi√≥ i el valor de cada cel¬∑la √©s 1 si el token est√† i 0 si no.
* Els vectors generats s√≥n **independents** de la sem√†ntica.
* No facilita calcular la similitud entre paraules i textos.

![bg right:37% fit](../images/one_hot.png)

---

<style scoped>section { font-size:31.8px; }</style>

### Bag of Words (BoW)

* El model **BoW** √©s un model basat en freq√º√®ncies.
* Es pot entendre com una suma dels vectors one-hot.
* Els vectors generats s√≥n **independents** de la sem√†ntica.
* El nombre del token es pot entendre com a **ordre** i en molts casos no √©s aix√≠. Aquesta discrep√†ncia pot afectar a la qualitat del model.

![bg right:40% fit](../images/bag-of-words.png)

---

<style scoped>section { font-size:33px; }</style>

### TF-IDF

* El model **TF-IDF** √©s un model basat en freq√º√®ncies i inversa de freq√º√®ncies.
* Semblant al model **BoW**, per√≤ t√© en compte la freq√º√®ncia del token en el document i en el conjunt de documents.
* El valor de cada cel¬∑la √©s el producte de la freq√º√®ncia del token en el document i la inversa de la freq√º√®ncia del token en el conjunt de documents.
* Dona m√©s import√†ncia als tokens que apareixen en pocs documents. Ra√≥: els tokens que apareixen en molts documents no solen aportar informaci√≥ rellevant.

---

### Word Embeddings

* Els _embeddings_ generats per Word2Vec s√≥n vectors de _n_ dimensions.
* Els vectors generats s√≥n **densos**, de **longitud fixa** i amb **sentit sem√†ntic**.
* Per a representar un **text** pot utilitzar-se la mitjana dels _embeddings_ de les paraules que el formen, el m√†xim, la suma, etc.

---

#### Generaci√≥ de _embeddings_ amb Word2Vec i la llibreria Gensim

```python
from gensim.models import Word2Vec

sentences = [
    ["Gavi", "company", "Pedri"],
    ["Xavi", "entrena", "Bar√ßa"],
    ["Gavi", "juga", "Bar√ßa"],
    ["Gavi", "chuta"],
    ["Kepa", "para"],
    ["Xavi", "entrena"],
]
model = Word2Vec(sentences, min_count=1)
```

---

#### Visualitzaci√≥ dels _embeddings_

```python
gavi = model.wv["Gavi"]
print(gavi)
```

```python
[
    -0.00536227  0.00236431  0.0510335   0.09009273
                                         - 0.0930295 - 0.07116809 0.06458873  0.08972988
                                         - 0.05015428 - 0.03763372
]
```

---

#### Busquem paraules similars

```python
model.wv.most_similar("Gavi")
```

```python
[('Bar√ßa', 0.5436005592346191),
 ('Pedri', 0.3792896568775177),
 ('entrena', 0.3004249036312103),
 ('Xavi', 0.10494352877140045),
 ('juga', -0.1311161071062088),
 ('chuta', -0.1897382140159607),
 ('para', -0.22418655455112457),
 ('Kepa', -0.2726020812988281),
 ('company', -0.7287455797195435)]
```

---

<style scoped>section { font-size:32px; }</style>

#### Similitud del cosinus

* Els _embeddings_ generats per Word2Vec s√≥n vectors de _n_ dimensions.
* Per a calcular la similitud entre dos vectors s'utilitza la **similitud del cosinus**.
* El resultat √©s un valor entre -1 i 1.
    * -1: vectors oposats.
    * 0: vectors ortogonals.
    * 1: vectors iguals.
* Aquesta mesura √©s molt utilitzada en el processament del llenguatge natural.

---

<!-- 
_class: invert lead
-->


<style scoped>
h1, p {
  color: #FFFFFF;
  font-weight: bold;
  text-shadow:
    0px 0px 3px #000000;
}
</style>


# Conversi√≥ de text a veu i veu a text

![bg opacity](../images/DALL%C2%B7E-2023-11-24-17.55.16-A-vibrant-and-abstract-representation-of-the-concept-of-audio-and-speech-recognition-symbolizing-the-Whisper-speech-to-text-model.-The-image-should-f.jpg)

---

<style scoped>section { font-size:32px; }</style>

## Reconeixement de veu i transcripci√≥ autom√†tica

* La **s√≠ntesi de veu** i la **transcripci√≥ autom√†tica** s√≥n tasques de **processament del llenguatge natural**.
    * La s√≠ntesi de veu √©s el proc√©s de **convertir un arxiu de text en un arxiu d'√†udio**.
    * La transcripci√≥ autom√†tica √©s el proc√©s de **convertir un arxiu d'√†udio en un arxiu de text**.
* Ambdues s√≥n unes t√®cniques molt importants en el processament del llenguatge natural; encara que no solen estar en primer pla.
* En aquest apartat veurem com funcionen aquestes t√®cniques.

---

<style scoped>section { font-size:31px; }</style>

## S√≠ntesi de veu

* La **s√≠ntesi de veu** √©s el proc√©s de **convertir un arxiu de text en un arxiu d'√†udio**.
* Aquesta tecnologia ha millorat molt en els √∫ltims anys, gr√†cies als models de llenguatge i a les xarxes neuronals.
* Hi ha diversos enfocaments, a continuaci√≥ veurem els m√©s importants.

![bg right fit](../images/Voice-Synthesis.jpg)

---

### S√≠ntesi de veu: concatenaci√≥ de sons

* Es basa en la grabaci√≥ de **sons** i la seva **concatenaci√≥** per a formar paraules i frases.
* Es busquen els sons m√©s adaptats al context i es combinen de manera que soni el m√©s natural possible.
* Requereix una gran quantitat de dades i dificulata adapatar-se a contextos nous.

---

### S√≠ntesi de veu: s√≠ntesi basada en formants

* Els **formants** s√≥n les **frequencies** de les cordes voc√†liques.
* S'ajusten els formants per representar els diferents sons i es combinen per a formar paraules i frases.
* La s√≠ntesi de formants permet obtindre veus clares i precises.
* Requereixen menys dades que la s√≠ntesi per concatenaci√≥ de sons, tenen, per√≤ menys naturalitat i expressivitat.

---

### S√≠ntesi de veu: s√≠ntesi basada en unitats

* Es basa en la **s√≠ntesi d'unitats** m√©s petites que les paraules, com ara **fonemes** o **diftongs**.
    * Aquestes unitats s'enmagatzemen en una base de dades i es combinen de forma din√†mica segons el text.
    * La s√≠ntesi de unitats permet obtenir veus m√©s naturals i expressives.
    * Requereixen menys dades que la s√≠ntesi per concatenaci√≥ de sons.

---

<style scoped>section { font-size:32px; }</style>

### S√≠ntesi de veu: s√≠ntesi basada en xarxes neuronals (I)

* Les xarxes neuronals s√≥n capaces de sintetitzar veus a partir de text.
* Aquestes xarxes s'entrenen amb grans quantitats de dades de veu i text i s√≥n capaces de sintetitzar veus molt naturals.
* S'utiltzen Xarxes Neural Recurrents (RNN) espec√≠fiques, com ara les xarxes LSTM o GRU o models m√©s moderns com ara les xarxes Transformer.
* Aquestes xarxes s√≥n capaces de sintetitzar veus molt naturals i expressives, sempre que tinguin suficientes dades d'entrenament i suficient capacitat de proc√©s.

---

<style scoped>section { font-size:31.5px; }</style>

### S√≠ntesi de veu: s√≠ntesi basada en xarxes neuronals (II)

* Aquests models es basen en els espectrogrames de les veus (representaci√≥ de la veu en funci√≥ del temps i la freq√º√®ncia).

* Funcionen en quatre etapes:
    * **Etapa de seq√º√®ncia a seq√º√®ncia**: el text es converteix en una seq√º√®ncia de vectors.
    * **Etapa de seq√º√®ncia a espectrograma**: els vectors es converteixen en espectrogrames.
    * **Etapa de s√≠ntesi**: els espectrogrames es converteixen en veu.
    * **Etapa de postprocessament**: es millora la qualitat de la veu.

---

![bg 70%](../images/psesgmsndedform00a.jpg)

---

## Transcripci√≥ autom√†tica

* En l'actualitat el **reconeixement de veu** √©s una tasca **molt madura**.
* Els assistents virtuals com **Siri**, **Alexa** o **Google Assistant** s√≥n capa√ßos de recon√®ixer veu amb una gran precisi√≥.
* Si volem implementar un sistema de reconeixement de veu, podem utilitzar eines com **Google Cloud Speech-to-Text** o **IBM Watson Speech to Text**.
* Aquestes eines es basen en les mat√®ixes t√®cniques que hem vist per a la s√≠ntesi de veu.

---

<style scoped>section { font-size:31px; }</style>

### Models de reconeixement de veu i transcripci√≥ autom√†tica

* Els models de reconeixement de veu i transcripci√≥ autom√†tica s√≥n models de **seq√º√®ncia a seq√º√®ncia**.
* Aquests models s√≥n capa√ßos de convertir una seq√º√®ncia d'entrada en una seq√º√®ncia de sortida.
* Alguns dels models m√©s importants s√≥n:
    * **Whisper**: model de reconeixement de veu de **OpenAI**.
    * **DeepSpeech**: model de reconeixement de veu de **Mozilla**.
    * **Hugging Face Speech2Text**: model de reconeixement de veu.
    * **Bark**: Model de generaci√≥ de veu de **Suno Labs**.

---

<!-- 
_class: invert lead
-->

<style scoped>
h1, p {
  color: #FFFFFF;
  font-weight: bold;
  text-shadow:
    0px 0px 3px #000000;
}
</style>


# Similitud entre textos

![bg opacity](../images/similarity.jpg)

---

## Similitud entre textos

* La similitud entre textos √©s una mesura que indica com de semblants s√≥n dos textos.
* √âs una de les funcions m√©s obvies del processament del llenguatge natural.
* El c√†lcul de la similitud entre textos, per√≤, √©s una tasca **dif√≠cil**.
* Anem a veure algunes t√®cniques de les m√©s utilitzades.

---

<style scoped>section { font-size:32px; }</style>

### T√©cniques per a calcular la similitud entre textos (I)

* **Basades en regles**: Es basen en regles predefinides; f√†cils d'implementar i √∫tils per a casos senzills.
    * **Dist√†ncia de Levenshtein**: √âs el nombre m√≠nim d'operacions per a transformar una cadena en una altra.
    * **Dist√†ncia de Hamming**: √âs el nombre de posicions en les quals dues cadenes de la mateixa longitud difereixen.
    * **Recompte de paraules**: √âs el nombre com√∫ de paraules entre dos textos.
    * **Dist√†ncia de Jaccard**: √âs el nombre de paraules comunes entre dos textos dividit pel nombre total de paraules dels dos textos.

---

<style scoped>section { font-size:31px; }</style>

### T√©cniques per a calcular la similitud entre textos (II)

* **Basades en caracter√≠stiques sint√†ctiques**: Es basen en les caracter√≠stiques sint√†ctiques i gramaticals dels textos. Impliquen un proc√©s de **parsejat** dels textos per analitzar la seva
  estructura sint√†ctica.
* **Basades en caracter√≠stiques sem√†ntiques**: Es basen en les caracter√≠stiques sem√†ntiques dels textos. Aqu√≠ models com Word2Vec s√≥n molt √∫tils, al permetre representar el significat contextual de les paraules.
    * **Word Mover's Distance**: Mesura la dist√†ncia entre dos textos com la dist√†ncia entre els vectors de les paraules dels dos textos.
    * **Similitud del cosinus**: Utilitza el cosinus de l'angle entre ells.

---
<style scoped>section { font-size:33px; }</style>

### T√©cniques per a calcular la similitud entre textos (II)

* **Basades en l'aprenentatge autom√†tic**: Es basen en l'aprenentatge autom√†tic per a calcular la similitud entre textos.
    * **BERT i GPT**: Models de llenguatge basats en xarxes neuronals que pot ser utilitzat per a calcular la similitud entre textos.
    * **Universal Sentence Encoder**: Model espec√≠ficament entrenat per al _transfer learning_ (aprenentatge per a la transfer√®ncia; utilitzar un model entrenat per a una tasca per a una altra).

---

### Utilitats de la similitud entre textos

* **Correcci√≥ ortogr√†fica**: Per a corregir una paraula es busca la paraula m√©s semblant.
* **Classificaci√≥ de textos**: Per a classificar un text es busca el text m√©s semblant.
* **Agrupaci√≥ de textos**: √ötil per a agrupar textos similars en clusters.
* **B√∫squeda de resposte**: Per a trobar la resposta a una pregunta es busquen texts semblants a la pregunta.

---

<style scoped>section { font-size:30.5px; }</style>

### Word Embeddings

* Les t√©cniques cl√†ssiques d'NLP es basen en representacions no sem√†ntiques com BoW o TF-IDF.
* Les modernes es basen en LLMs (Language Models) i Word Embeddings.
* Com ja hem vist, els _embeddings_ generats per Word2Vec s√≥n vectors de _n_ dimensions.
* Per a representar un text pot utilitzar-se la mitjana dels _embeddings_ de les paraules que el formen.
* Els vectors generats s√≥n **densos**, de **longitud fixa** i amb **sentit sem√†ntic**.
* Facilita calcular la similitud entre paraules i textos.

---

### LLMS (Language Models)

* Els **LLMs** s√≥n models complexos basats en xarxes neuronals recurrents i l'arquitectura **Transformer**.
* Poden aprendre la sem√†ntica del text i generar els seus propis _embeddings_ utilitzant el mecansime d'**atenci√≥**.
* Demostren un gran rendiment en moltes tasques, com pot ser el c√†lcul de la similitud entre textos.
* S√≥n complexos i necessiten un entrenament previ amb un gran volum de dades.

---

<!-- 
_class: invert lead
-->


<style scoped>
h1, p {
  color: #FFFFFF;
  font-weight: bold;
  text-shadow:
    0px 0px 3px #000000;
}
</style>


# Classificaci√≥ de textos i an√†lisi de sentiments

![bg opacity](../images/sentiment.png)

---

## An√†lisi de sentiments

* L'an√†lisi de sentiments √©s un tipus de classificaci√≥ i una de les tasques m√©s utilitzades en el processament del llenguatge natural.
* L'objectiu √©s determinar l'actitud d'un autor respecte a un tema o producte.
* Es basa en la **polaritat** del text, que pot ser **positiva**, **negativa** o **neutra**.
* Tamb√© poden buscar-se emocions concretes, com pot ser **alegria**, **tristesa**, **ira**, etc.

---

<style scoped>section { font-size:34px; }</style>

## An√†lisi subjectiva i objectiva

* L'an√†lisi de sentiments pot ser **subjectiva** o **objectiva**.
* L'an√†lisi subjectiva busca les **emocions i sentiments** de l'autor.
* L'an√†lisi objectiva es basa en **fets i dades concretes**.
* Els dos tipus d'an√†lisi s√≥n **complementaris** i poden utilitzar-se conjuntament.
* Ex: "_La pel¬∑l√≠cula t√© grans moments, per√≤ el final √©s molt trist_".
    * An√†lisi subjectiva: "_La pel¬∑l√≠cula √©s bona_".
    * An√†lisi objectiva: "_El final √©s trist_".

---

<style scoped>section { font-size:30.5px; }</style>

## Preprocessament del text

* El **tractament** de textos facilita obtenir bons resultats en NLP.
* Permet redu√Ør la **dimensionalitat** dels textos, eliminar el soroll i capturar la sem√†ntica.
* Algunes de les t√®cniques m√©s utilitzades s√≥n:
    * **Tokenitzaci√≥**: vist anteriorment.
    * **Normalitzaci√≥**: convertir el text a un format est√†ndard.
    * **Eliminaci√≥ de stopwords**: eliminar paraules que no aporten informaci√≥.
    * **Stemming** i **lematitzaci√≥**: convertir les paraules a forma base.
    * **Gesti√≥ de negacions i modalitats**: convertir a un format est√†ndard.

---

### Preprocessament: tokenitzaci√≥

* Com ja hem vist, la tokenitzaci√≥ √©s el proc√©s de dividir un text en tokens.
* Els tokens poden ser paraules, subparaules, signes de puntuaci√≥, etc.
* Facilita una an√†lisi m√©s profund del text i extreure caracter√≠stiques rellevants.
* Ex: "El Bar√ßa est√† en crisi" $$\rightarrow$$ `["El", "Bar√ßa", "est√†", "en", "crisi"]`.

---

<style scoped>section { font-size:34px; }</style>

### Preprocessament: normalitzaci√≥

* La **normalitzaci√≥** implica el¬∑liminar els elements que no aporten informaci√≥.
    * Nombres, signes de puntuaci√≥, etc.
* Tamb√© implica convertir el text a un format est√†ndard, passant a min√∫scules i llevant espais innecessaris, per exemple.
    * Ex: "El Bar√ßa est√† en crisi! üò°" $$\rightarrow$$ "el Bar√ßa est√† en crisi".
* La normalitzaci√≥ facilita la comparaci√≥ entre textos i la detecci√≥ de paraules clau.

---

### Preprocessament: eliminaci√≥ d'stopwords

* Les **stopwords** s√≥n paraules que no aporten informaci√≥ al text.
* S√≥n paraules molt comunes en un idioma, com pot ser articles, preposicions, etc.
* Els textos despr√©s de processar-se amb stopwords s√≥n m√©s f√†cils de tractar i m√©s r√†pids de processar.
* Ex: "El Bar√ßa est√† en crisi" $\rightarrow$ `["Bar√ßa", "crisi"]`.

---

<style scoped>section { font-size:34px; }</style>

### Preprocessament: stemming i lematitzaci√≥

* El **stemming** i la **lematitzaci√≥** s√≥n t√®cniques per a convertir les paraules a la seva forma base i facilitar l'agrupaci√≥ de paraules relacionades.
* L'**stemming** √©s un proc√©s heur√≠stic basat en regles, mentre que la **lematitzaci√≥** √©s un proc√©s basat en coneixements ling√º√≠stics.
    * Ex d'stemming: "jugar", "jugar√©", "jugar√†"  $\rightarrow$ "jug".
    * Ex de lematitzaci√≥: "jugar", "jugar√©", "jugar√†" $\rightarrow$ "jugar".
* El stemming √©s m√©s r√†pid, per√≤ la lematitzaci√≥ √©s m√©s precisa.

---

### Preprocessament: negacions i modalitats

* Les **negacions** i **modalitats** poden canviar el significat d'una frase.
* Ex: _"El Bar√ßa no est√† en crisi"_, _"El Bar√ßa pot estar en crisi"_, _"Deuries anar a l'estadi"_.
* Els models de NLP no poden interpretar aquestes frases sense un tractament previ.
* Necessitem t√©cniques espec√≠fiques, com la detecci√≥ de **dobles negacions** i la **reassignaci√≥ de polaritat**.

---

## Enfocaments per a l'an√†lisi de sentiments

* Com en totes les tasques de NLP, l'an√†lisi de sentiments pot ser abordada amb diferents enfocaments.
* Els enfocaments m√©s utilitzats s√≥n:
    * Basats en **regles**
    * Basats en l'**aprenentatge autom√†tic supervisat**
    * Basats en l'**aprenentatge autom√†tic no supervisat**

---

### Enfocament basat en regles

* Els enfocaments basats en regles s√≥n els m√©s senzills i r√†pids.
* Tamb√© s'anomenen **lexicon-based**.
* Es basen llistes de paraules per a determinar la polaritat del text.
* De cada paraula es busca la seva polaritat en el llistat i es fa una suma..
    * _"El Bar√ßa est√† en crisi"_. _"crisi"_ $\rightarrow$ **-1** $\rightarrow$ **negatiu**.
    * _"Me fa il¬∑lusi√≥ anar a l'estadi"_. _"il¬∑lusi√≥"_  $\rightarrow$ **1**  $\rightarrow$ **positiu**.
    * _"El partit va acabar en empat"_. _"empat"_  $\rightarrow$ **0** $\rightarrow$ **neutre**.

---

### Enfocament basat en l'AA supervisat

* Els enfocaments basats en l'aprenentatge autom√†tic supervisat s√≥n alguns dels m√©s utilitzats.
* Consisteixen en entrenar un model amb un conjunt de dades etiquetades amb la polaritat del text.
* El model identifica els patrons en el text que determinen la polaritat.
* Alguns dels models m√©s utilitzats s√≥n: **Naive Bayes**, **Support Vector Machines**, **Random Forest**, **Xarxes neuronals**

---

<style scoped>section { font-size:34px; }</style>

### Enfocament basat en l'AA no supervisat

* Els enfocaments basats en l'aprenentatge es basen en identificar patrons en el text sense necessitar etiquetes predefinides.
* Es poden utilitzar t√®cniques com la **clusteritzaci√≥** per a agrupar els textos en clusters segons la seva polaritat.
* Una vegada agrupats els textos, es poden etiquetar manualment els clusters.
* Aquestes t√®cniques s√≥n √∫tils per a detectar patrons en el text i per a agrupar textos semblants (segons la dist√†ncia entre textos).

---

<style scoped>section { font-size:33px; }</style>

## Models per a l'an√†lisi de sentiments

Alguns dels models m√©s utilitzats per a l'an√†lisi de sentiments s√≥n:
* **BOW** + clasificador: model basat en BoW i un classificador.
* **Embeddings** + clasificador: Word2Vec, FastText, etc.
* **VADER**: model basat en regles, molt utilitzat en angl√®s.
* **Transformers**: com ja hem vist, els transformers s√≥n models espec√≠fics per a NLP molt potents. Un dels models m√©s utilitzats √©s **BERT**, de q√ºal utilitzarem una implementaci√≥ en la segona
  pr√†ctica.

---

## Xarxes neuronals per a l'an√†lisi de sentiments

* Les xarxes neuronals s√≥n un dels models m√©s utilitzats per a l'an√†lisi de sentiments.
* Les xarxes neuronals s√≥n capaces d'aprendre els patrons en el text i de generar els seus propis _embeddings_.

---

<!-- 
_class: invert lead
-->


<style scoped>
h1, p {
  color: #FFFFFF;
  font-weight: bold;
  text-shadow:
    0px 0px 3px #000000;
}
</style>


# Models de llenguatge

![bg opacity](../images/language-model.png)

---

## Models de llenguatge

* Fins ara hem vist com representar el text i com acomplir tasques com l'an√†lisi de sentiments.
* Hem comentat que els models de llenguatge s√≥n eines molt potentes que poden ser utilitzades en moltes tasques.
* En aquesta secci√≥ veurem qu√® s√≥n els models de llenguatge i com funcionen.
* Tamb√© veurem alguns dels m√©s utilitzats i les seves aplicacions.

---

<style scoped>section { font-size:31px; }</style>

## Aplicacions dels models de llenguatge

* S√≥n dels camps m√©s actius i complexos de la IA.
* S√≥n la base de moltes aplicacions de NLP, com pot ser:
    * **Traducci√≥ autom√†tica**: traduir un text d'un idioma a un altre.
    * **Reconeixement de veu**: transcriure un text a partir d'un arxiu d'√†udio.
    * **S√≠ntesi de veu**: generar un arxiu d'√†udio a partir d'un text.
    * **Generaci√≥ i resum de text**: generar textos a partir d'un context.
    * **An√†lisi de sentiments**: determinar la polaritat d'un text.
    * **Classificaci√≥ de text**: classificar un text en una categoria.
    * **Generaci√≥ de textos**: generar textos a partir d'un tema o un estil.

---

<style scoped>section { font-size:30px; }</style>

### Definici√≥

* Un **model de llenguatge** assigna una probabilitat a una seq√º√®ncia de paraules.
    * Per tant, permet predir la seg√ºent paraula d'una seq√º√®ncia.
    * Ex: "El bar√ßa est√† en ___" $$\rightarrow$$ `[{crisi: 0.8}, {forma: 0.1}, {casa: 0.1}]`
* Es basen en la idea que les paraules d'una seq√º√®ncia no s√≥n independents, sin√≥ que depenen de les paraules anteriors.
* Permeten calcular la "**validesa**" d'una seq√º√®ncia de paraules.
    * No √©s el mateix que la **correcci√≥** d'una seq√º√®ncia de paraules.
    * Intentem modelar el llenguatge hum√†, amb els seus **matissos i ambig√ºitats**.

---

## Hist√≤ria: Models basats en regles

* Els models de llenguatge s√≥n un dels camps m√©s antics del processament del llenguatge natural.
* Es basen en regles gramaticals i ling√º√≠stiques per definir les caracter√≠stiques del llenguatge.
* Les regles estan definides per experts i s√≥n dif√≠cils de modificar.
* No s√≥n flexibles i no poden adaptar-se a nous contextos.
* Ex: Gram√†tica de Chomsky, Gram√†tica de Montague, etc.

---

<style scoped>section { font-size:30.6px; }</style>

## Hist√≤ria: Models estoc√†stics

* Els models basats en regles van ser substitu√Øts pels basats en **estad√≠stiques**, m√©s flexibles i que modelen millor el llenguatge. Els models de Markov van ser els primers en tindre resultats acceptables.
* Es basen en la idea que les paraules d'una seq√º√®ncia no s√≥n independents, sin√≥ que depenen de les paraules anteriors. Exemples:
    * **N-gram**: modela cada paraula en funci√≥ de les n paraules anteriors. (uni, bi, tri, etc).
    * **Skip-gram**: modela cada paraula en funci√≥ de les n paraules anteriors i posteriors.
    * **Syntax-based**: es basen en l'estructura sint√†ctica de les frases i no en la seva seq√º√®ncia.

---

<style scoped>section { font-size:30px; }</style>

## Hist√≤ria: RNN

* Els models basats en estad√≠stiques van ser substitu√Øts per models basats en **xarxes neuronals**.
* Els primers models d'aquest tipus van ser els **RNN** (xarxes neuronals recurrents).
    * A difer√®ncia de les xarxes neuronals tradicionals, les RNN tenen **mem√≤ria**.
    * L'entrada d'una neurona pot anar determinada per la sortida d'ella mateixa.
    * Permeten processar seq√º√®ncies de longitud variable.
    * Els models de llenguatge basats en RNN van ser els primers en obtenir resultats acceptables.

---

<style scoped>section { font-size:30px; }</style>

## Hist√≤ria: LLM

* Els models basats en RNN van ser substitu√Øts per models basats en **transformers**.
* Els transformers s√≥n models basats en xarxes neuronals que utilitzen el mecanisme d'**atenci√≥**.
    * S√≥n m√©s potents que les RNN i permeten obtenir resultats molt millors.
    * S√≥n els models m√©s utilitzats en l'actualitat.
    * Necessiten un entrenament previ amb un **gran** volum de dades (_corpus_)
    * Mostren la capacitat d'entendre el context, la sem√†ntica i la sint√†xis del text.

---

## Hist√≤ria: LLM (II)

* El mecanisme d'atenci√≥ √©s un mecanisme que permet a les xarxes neuronals aprendre a **centrar-se** en les parts importants de les seves entrades.
* √âs un mecanisme que imita el comportament hum√†.
* Podem entendre'l com una **capa** que s'afegeix a una xarxa neuronal.
* Els transformers s√≥n models basats en xarxes neuronals que utilitzen una variant del mecanisme d'atenci√≥ anomenada **self-attention**.

---

## Entrenament de models de llenguatge

* Els models de llenguatge necessiten un entrenament previ amb un **gran** volum de dades.
* Aquestes dades s'anomenen **corpus**.
* Els corpus s√≥n **molt grans** i dif√≠cils de generar.
* Poden ser **generats manualment** o **autom√†ticament**.
* Varien en contingut: not√≠cies, llibres, xats, etc.
* Poden incloure **etiquetes** per a entrenar models supervisats.

---

## Parts d'un LLM

* Els models de llenguatge basats en transformers s√≥n models molt complexos.
* Tenen dues parts principals:
    * **Encoder**: codifica el text d'entrada en un vector.
    * **Decoder**: decodifica el vector en un text de sortida.
    * Segons quines parts estiguen presents o no podran ser **bidireccionals** o **unidireccionals**.
    * Aix√≥ determinar√† tamb√© les tasques que poden realitzar.

---

## Parts d'un LLM (II)

* Encoder: codifica el text d'entrada en un vector.
* Els models `encoder-only` s√≥n **unidireccionals** i s'especialitzen en "entendre" el text d'entrada i, per tant, s√≥n √∫tils per a tasques com la classificaci√≥ de text.
* Solament necessiten el **encoder** per a realitzar la tasca perqu√© no necessiten generar un text de sortida.
* Utilitats: classificaci√≥ de text, an√†lisi de sentiments, etc.
* Ex: **BERT**, RoBERTa, ALBERT, ELECTRA, etc.

---

## Parts d'un LLM (III)

* Decoder: decodifica el vector en un text de sortida.
* Els models `Decoder-only` solament poden accedir a les paraules anteriors i, per tant, s√≥n √∫tils per a tasques com la generaci√≥ de text.
* Utilitats: generaci√≥ de text, escritura creativa, etc.
* Ex: **GPT**, GPT-2, GPT-3, Mixtral, etc.

---

## Parts d'un LLM (IV)

* Encoder + Decoder: codifica el text d'entrada en un vector i decodifica el vector en un text de sortida.
* Els models `Encoder-Decoder` poden accedir a les paraules anteriors i posteriors i, per tant, s√≥n √∫tils per a tasques com la traducci√≥ autom√†tica.
* Utilitats: traducci√≥ autom√†tica, resum de text, esquematitzaci√≥ de text, etc.
* Ex: **T5**, BART, etc.

---

<style scoped>section { font-size:30px; }</style>

## Utilitzaci√≥ dels LLM

* Els transformers s√≥n models molt complexos i necessiten un entrenament previ amb un gran volum de dades.
* Normalment s'utilitzen models ja entrenats i que poden ser utilitzats per a diferents tasques.
* Per a millorar el rendiment dels models entrenats es pot utilitzar el **fine-tuning**.
    * El fine-tuning consisteix en entrenar el model amb un conjunt de dades espec√≠fic per a la tasca que volem realitzar.
    * S'utilitza un conjunt de dades m√©s petit que el corpus original.
    * En les pr√†ctiques utilitzarem un model ja entrenat i li farem fine-tuning per a millorar el rendiment en les tasques que realitzem.

---

<!-- 
_class: invert lead
-->

<style scoped>
h1, p {
  color: #FFFFFF;
  font-weight: bold;
  text-shadow:
    0px 0px 3px #000000;
}
</style>

# Arquitectures per a NLP

![bg opacity](../images/recursive_nn_nlp.png)

---

<style scoped>section { font-size:30px; }</style>

## Cadenes de Markov


- Els models ocults de Markov (HMM) s√≥n models estoc√†stics que permeten modelar seq√º√®ncies de paraules.

- Es basen en la idea que les paraules d‚Äôuna seq√º√®ncia no s√≥n independents, sin√≥ que depenen de les paraules anteriors.

- Els HMM s√≥n capa√ßos de modelar la probabilitat de transici√≥ entre paraules.

- El seu principal desavantatge √©s que no poden modelar depend√®ncies a llarg termini.

![bg right:35% fit](../images/mdp.png)

---

<!-- 
_class: invert lead
-->


<style scoped>
h1, p {
  color: #FFFFFF;
  font-weight: bold;
  text-shadow:
    0px 0px 3px #000000;
}
</style>


## Transformers

![bg opacity](../images/transformers.png)

---

### Introducci√≥

* Com ja hem comentat anteriorment, els **transformers** s√≥n models basats en xarxes neuronals que utilitzen el mecanisme d'**atenci√≥**.
* S√≥n els models m√©s utilitzats en l'actualitat en el processament del llenguatge natural.
* La seva arquitectura innovadora permet obtenir resultats molt millors que els models anteriors i aprofitar el **parallelisme** i les **GPU**.
* Per la seva complexitat anem a veurel's en m√©s detall.

---

### Orige

* Els transformers van ser introdu√Øts per **Vaswani et al.** en el 2017.
    * El paper original es titula "_Attention is All You Need_".
* Els transformers van ser dissenyats per a millorar el rendiment dels models de llenguatge.
* El seu primer √∫s va ser en la tasca de traducci√≥ autom√†tica.
* Va posar en primer pla el mecanisme d'atenci√≥ com a eina fonamental en el processament del llenguatge natural.

---

### Arquitectura dels transformers (I)

* Els transformers s√≥n models moderns i molt complexos.
* Per contra, si veiem les seves parts per separat, √©s m√©s f√†cil entendre'l's.
* Anem a veure punt per punt les seves parts principals i com funcionen.

![bg right:38% fit](../images/transformers_arquitectura.png)

---

<style scoped>section { font-size:33px; }</style>

### Arquitectura dels transformers (II)

* En un nivell superficial, els transformers funcionen com una caixa negra.
* Reben com a entrada un text i generen com a sortida un text.
* La seva complexitat rau en la seva arquitectura interna.
* Els transformers tenen *dos* parts principals: **encoders** i **decoders**.

![bg right:40%](../images/transformer_ml01-768x644.png)

---

<style scoped>section { font-size:33px; }</style>

### Arquitectura dels transformers (III)

* L'entrada passa per una s√®rie de capes d'encoders.
* A continuaci√≥, la sortida dels encoders passa per una s√®rie de capes de decoders.
* En el paper original: **6 capes d'encoders i 6 capes de decoders**.
* Tamb√© podem passar un "**target**" com a entrada, **per entrenar**

![bg right:40%](../images/transformer_ml02-768x644.png)

---

<style scoped>section { font-size:30px; }</style>

### Encoders i decoders

* Els **encoders** i **decoders** s√≥n les parts principals dels transformers.
* A nivell intern son semblants i comparteixen moltes caracter√≠stiques.
    * Tenen en l'entrada una (o m√©s) capa d'**atenci√≥** i com a sortida una capa **feed-forward**.
* La difer√®ncia principal √©s que els **encoders** solament tenen una capa d'atenci√≥, mentre que els **decoders** tenen dues.

![bg right:45% ](../images/transformer_ml03-768x644-1.png)

---

<style scoped>section { font-size:30px; }</style>

### Embeddings i posicions

* Els transformers utilitzen **embeddings** per a representar les paraules (de 512 dimensions en el paper original).
* A m√©s, utilitzen el **positional encoding** per a representar la posici√≥ de les paraules en la seq√º√®ncia.
    * B√°sicament una funci√≥ sinusoidal que varia segons la posici√≥ de la paraula, per lo que el mateix vector en diferents posicions ser√† un poc diferent.
    * $$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})$$
* Aquest encoding mant√© la informaci√≥ de la posici√≥ de les paraules en la seq√º√®ncia; al mateix temps que permet **enviar tots els tokens a la xarxa al mateix temps**.

---

![bg  fit](../images/transformers_arquitectura_inputs.png)

---

### Encoder

* Els **encoders** estan compostos per tres capes:
    * **Self-attention**: per a calcular la import√†ncia de cada paraula en la seq√º√®ncia. A continuaci√≥ veurem com funciona.
    * **Feed-forward**: per a processar la informaci√≥ obtinguda de l'atenci√≥.
    * **Normalization i conexions residuals**: per a evitar el desvaiment del gradient i facilitar el seu entrenament.

![bg right:40%](../images/transformer_ml04-768x644.png)

---

<style scoped>section { font-size:31px; }</style>

### Self-attention (I)

* El **self-attention** √©s el mecanisme clau dels transformers.
* Permet a la xarxa "centrar-se" en les parts importants de la seq√º√®ncia.
    * Ex: en la frase _"El gat gris va a la casa"_ sabem que $gris$ i $gat$ estan relacionats. Com ho pot saber la xarxa?
* Els transformers creen un "**Soft dictionary**" d'atencions en les paraules de la seq√º√®ncia. Aix√≠, l'atenci√≥ de $gris$ en $gat$ ser√† $1$ i en $casa$ ser√† $-1$.
* El que el diccionari siga "soft" vol dir que pot anar modificant-se.
    * En la frase "El gat va a la casa gris" l'atenci√≥ de $gris$ en $casa$ ser√† $1$.

---

<style scoped>section { font-size:34px; }</style>

### Self-attention (II)

* Per calcular el self-attention es generen tres matrius a partir de la seq√º√®ncia d'entrada: **Q** (query), **K** (key) i **V** (value)
* **Q** i **K** s√≥n matrius que representen la seq√º√®ncia d'entrada i **V** √©s la matriu que representa el valor de cada paraula.
* Per obtindre l'atenci√≥ multiplicarem **Q** per la transposada de **K**
    * Obtindrem la similitud entre les paraules.
* El resultat el multiplicarem pel valor de **V**.
    * Obtindrem la matriu d'atencions.

---

![bg fit](../images/ejemplo-self-attention-768x336.png)

---
<style scoped>section { font-size:32px; }</style>

### Self-attention (III)

* Els transformers utilitzen el **multi-head attention**.
* Aquesta t√®cnica consisteix en calcular el self-attention amb diferents grups de dimensions.
* Els transformers utilitzen **8** caps de self-attention en el paper original.
* Dels resultats es fa un promig i el resultat, segons els estudis es molt m√©s significatiu.
* √âs important ressaltar que tot el proc√±es d'atenci√≥ es paral¬∑lel i accelerat per la GPU (multiplicaci√≥ de matrius), per lo que es molt r√†pid.

---

<style scoped>section { font-size:31px; }</style>

### Altres tipus d'atenci√≥

* A m√©s del **self-attention**, els transformers utilitzen altres tipus d'atenci√≥:
    * **Cross-attention**: les entrades del decoder s√≥n les sortides de l'encoder. Aix√≤ permet al encoder condicionar el decoder, donant-li informaci√≥ sobre el context.
    * **Masked attention**: en el decoder, les paraules futures no poden ser utilitzades per a calcular l'atenci√≥. Aix√≤ evita que el model "mire al futur".

![bg right:33% fit](../images/transformers_arquitectura_attention-1.png)

---

<style scoped>section { font-size:35px; }</style>

### Normalitzaci√≥ i conexions residuals

* Les conexions residuals s√≥n una t√®cnica que permeten evitar el desvaiment del gradient.
    * Aquest problema es produeix quan les xarxes s√≥n molt profundes.
    * Les conexions residuals permeten que els valors d'entrada es mantinguen en les capes posteriors.
* La normalitzaci√≥ permet que els valors d'entrada es mantinguen en un rang determinat.
    * Aix√≤ facilita el seu entrenament i millora el seu rendiment.

---

![bg fit](../images/transformers_arquitectura_addnorm.png)

---

<style scoped>section { font-size:32px; }</style>

### Feed-forward

* La capa **feed-forward** √©s una capa de xarxa neuronal normal.
* La seva funci√≥ √©s processar la informaci√≥ obtinguda de l'atenci√≥.
* Hi haur√† dues capes de _dropout_ per a evitar l'overfitting i una funci√≥ d'activaci√≥ no lineal (ReLU en el paper original).

![bg right fit](../images/transformers_arquitectura_mlp.png)

---

<style scoped>section { font-size:33px; }</style>

### Decoder

* Els **decoders** s√≥n molt semblants als **encoders**.
* En els models normals en l'entranament utilitzem la sortida esperada per validar el resultat.
* Per contra, en els models de llenguatge, el **target** es passa com a entrada per a entrenar el model.
* Aix√≤ permet que el model aprenga a generar el text de sortida.

![bg right:40%](../images/transformer_ml05-768x644.png)

---

### Sortida final del model

* Recordem que utilitzem m√∫ltiples capes d'encoders i decoders.
* La sortida final del model √©s la sortida de l'√∫ltima capa de decoders i passa per una capa de **softmax**.
* Aquesta sortida √©s un vector de longitud igual al nombre de paraules del vocabulari.
* Aquest vector representa la probabilitat de cada paraula en el vocabulari (de 0 a 1).
* La paraula amb m√©s probabilitat ser√† la paraula de sortida.

---

<style scoped>section { font-size:30px; }</style>

### Aplicacions dels transformers

* Poden ser utilitzats en moltes tasques de NLP pero m√©s enll√† del NLP tamb√© s'utilitzen en altres tasques com les seg√ºents:
    * **Visi√≥ per computador**: s'est√† utilitzant per la classificaci√≥ d'imatges i altres. _Vision Transformer (ViT)_.
    * **Series temporals**: les seq√º√®ncies de paraules s√≥n molt semblants a les seq√º√®ncies de fets en el temps.
    * **Generatius**: s'utilitzen per a generar textos, imatges, etc.
    * **Aprenentatge per refor√ß**: s'utilitzen per a entrenar agents en entorns complexos.

* Tots aquests usos els fan una eina molt potent i que poden arribar a substituir molts dels models actuals.