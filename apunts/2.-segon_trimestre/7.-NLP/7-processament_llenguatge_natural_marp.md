---
marp: true
size: 16:9
theme: lawer
class: default
_class: invert lead
paginate: true
_paginate: false
auto-scaling: true
footer:  Carles Gonzlez - CC-BY-NC-SA
---

<style scoped>
h1, p {
  color: #FFFFFF;
  text-shadow:
    0px 0px 3px #000000;
}
</style>

# 7. Processament del llenguatge natural

Models d'intel路lig猫ncia artificial

![bg opacity](../../images/NLP-for-Beginners-Pythons-Natural-Language-Toolkit-NLTK_Watermarked.png)

---

<style scoped>section { font-size:33px; }</style>

## Processament del llenguatge natural

- **Definici贸:** camp de la IA que tracta de la interacci贸 entre els ordinadors i el llenguatge hum.
- Se centra en la **comprensi贸** i **generaci贸** de llenguatge hum.
- Un dels camps m茅s actius i complexos de la IA.

![bg right fit](../../images/periodic-table-of-nlp-tasks-high.png)

---

## Aplicacions

- Traducci贸 automtica.
- Reconeixement de veu.
- S铆ntesi de veu.
- Generaci贸 i resum de text.
- Anlisi de sentiments.
- Classificaci贸 de text.

---

## Introducci贸 (I)

- Camp multidisciplinari que combina:
  - Ling眉铆stica.
  - Intel路lig猫ncia artificial.
  - Ci猫ncies cognitives.
  - Informtica.
  - etc.

---

## Introducci贸 (II)

- s un problema **dif铆cil** perqu猫:
  - El llenguatge hum 茅s **ambigu**.
  - El llenguatge hum 茅s **ric**.
  - El llenguatge hum 茅s **contextual**.
  - El llenguatge hum 茅s **cultural**.
- Aquestes caracter铆stiques fan que el llenguatge hum no sigui **formal** i, per tant, no es puga tractar amb les t猫cniques de la IA tradicional.

---

<!--
_class: invert lead
-->

<style scoped>
h1, h2, h3, h4, h5, h6, p {
  color: #FFFFFF;
  font-weight: 800;
  text-shadow:
    0px 0px 3px #000000;
}
</style>

# El text com a dada

![bg opacity](../../images/text.jpg)

---

## Introducci贸

- El text 茅s una font de dades **molt important**.
- Els humans **generem** i **consumim** text de forma **massiva**
- El saber com tractar el text 茅s **cr铆tic** per a moltes aplicacions.

> Anomenem **text** a una seq眉猫ncia coherent de s铆mbols que pot ser interpretada com a un conjunt de paraules, utilitzant les regles gramaticals i sintctiques d'una llengua.

---

## Significat

- Qu茅 entenem per **significat**?:
  - el significat d'una paraula 茅s el concepte que representa.
  - el significat d'una frase 茅s el concepte que representa la combinaci贸 de les paraules que la formen.
  - el significat d'un text 茅s el concepte que representa la combinaci贸 de les frases que el formen.

$$\text{significant}(\text{simbol}) \Leftrightarrow \text{significat}(\text{idea})$$
$$\text{arbre} \Leftrightarrow \text{\{, , , } \dots \}$$

---

<style scoped>section { font-size:32px; }</style>

## Significat en els ordinadors

- Com poden coneixer els ordinadors el significat de les paraules, frases i textos?.
- T茅cniques classiques: bases de dades de sin贸nims i hyper貌nims.
  - `WordNet`: base de dades l猫xica que relaciona paraules entre si.
  - Els sin贸nims permeten relacionar paraules amb el mateix significat.
    - Ex: `ro茂n` 茅s un sin貌nim de `dolent`, `malparit`, `miserable`, etc.
  - Els hyper貌nims permeten relacionar paraules amb significats m茅s generals.
    - Exemple: `carnivor`, `vertebrat` serien hyper貌nims de `gat`.

---

<style scoped>section { font-size:32px; }</style>

## Problemes de WordNet i semblants

- Molt 煤til per貌 sense mat铆s semntic.
  - Ex: `gat` i `fel铆` s贸n sin貌nims, per貌 no tenen el mateix significat.
- Tots els s铆nonims no seran 煤tils en tots els contextos.
  - Ex: `cabr贸` 茅s un sin貌nim de `ro茂n`, per貌 no sempre es poden intercanviar.
- Actualitzar la base de dades 茅s un proc茅s **manual**, **cost贸s** i subjectiu.
  - Ex: `the shit` 茅s un sin貌nim de `the best` en angl猫s que no apareix en WordNet.
- Les solucions modernes es basen en les representacions del text.

---

<style scoped>section { font-size:32px; }</style>

## Representacions del text (I)

- Els ordinadors necessiten representar el text com a dades num猫riques.
- Necessitarem un **vocabulari** que relacioni les unitats de text amb els n煤meros.
- Quina unitat de text triem per a representar el text i com ho fem?.
- Aquestes decisions determinaran la **complexitat** del model, el tamany del **vocabulari** i la **precisi贸** del model.
- Els models de representaci贸 del text s贸n **molt importants** en el processament del llenguatge natural.
- A continuaci贸 veurem algunes de les t猫cniques m茅s utilitzades.

---

<style scoped>section { font-size:31.5px; }</style>

## Representaci贸 de carcters

- Cada carcter es representa per un n煤mero.
  - El diccionari ser molt petit (en el cas de l'ASCII, 128 carcters).
  - El model ha de ser molt complex, ja que ha d'aprendre a combinar els carcters per a formar paraules.
  - Exemple: "AND" es pot representar com a $[65, 78, 68]$

![bg right:38% fit](../../images/ascii-character-map.png)

---

## Representaci贸 de paraules

- Cada paraula 茅s representada per un n煤mero.
  - El diccionari ser molt gran, ja que cada paraula ser una entrada en el diccionari.
  - El model ser m茅s senzill, ja que les paraules s贸n unitats semntiques.
  - Exemple: Si el nostre vocabulari 茅s `["gat", "gos", "cavall", "ocell", "peix"]`, llavors "gat" es pot representar com a $1$ i "cavall" com a $3$.

---

## Representaci贸 de subparaules

- Cada subparaula 茅s representada per un n煤mero.
  - El diccionari ser m茅s petit que el de paraules, ja que les subparaules s贸n unitats semntiques.
  - El model ser m茅s senzill, ja que les subparaules s贸n unitats semntiques.
    - Permet representar paraules rares i que no estan en el vocabulari.
    - til per a lleng眉es amb moltes paraules compostes i derivades.

---

<style scoped>section { font-size:30.5px; }</style>

## Tokens i tokenitzaci贸 (I)

- Indepententment de l'enfocament, el text s'ha de **dividir** en **tokens**.
  - Ex: "New York in winter" $\rightarrow$ `["New", "York", "in", "winter"]`
- **Token**: unitat m铆nima de text que pot ser considerada com a una unitat semntica.
  - Una paraula, subparaula, signe de puntuaci贸, n煤mero, etc; qualsevol unitat que es tri茂 com a unitat m铆nima.
- **Tokenitzaci贸**: proc茅s de dividir un text en tokens i que facilita el tractament i comprensi贸 del text.
  - s un proc茅s **no trivial**. Dep猫n de la llengua i del domini.
    - Exemple: "New York" 茅s un token o dos?

---

<style scoped>section { font-size:35px; }</style>

## Tokens i tokenitzaci贸 (II)

- Q眉estions a tindre en compte:
  - **Puntuaci贸**: es considera un token o no?. Pot variar la interpretaci贸 del text.
  - **Majuscules/Min煤scules**: es consideren tokens diferents o no?.
  - **Stopwords**: paraules que no aporten informaci贸 al text (articles, preposicions, etc.).
  - **Idioma i domini**: el proc茅s de tokenitzaci贸 dep猫n de l'idioma i del domini del text.

---

<style scoped>section { font-size:32.5px; }</style>

## Tokens i tokenitzaci贸 (III)

- N-grams: seq眉猫ncies de n tokens consecutius.
- Algunes paraules tenen significat propi, per貌 la seva combinaci贸 amb altres paraules tamb茅 t茅 un significat. Ex: "New York".
- Els n-grams permeten representar aquestes combinacions de paraules, augmentant el vocabulari amb les combinacions d'n-tokens que triem.
- Bigrams: seq眉猫ncies de dos tokens consecutius, trigrams: seq眉猫ncies de tres tokens consecutius, etc.
- Problema: augmenta molt el vocabulari i la complexitat del model.

---

<style scoped>section { font-size:34px; }</style>

## Vectoritzaci贸 (I)

- Encara que els tokens son molt 煤tils, presenten alguns problemes:
  - No s贸n fcils de manipular per a les mquines.
  - No s贸n fcils de comparar.
  - No permeten calcular la similitud entre paraules i textos.
- La **vectoritzaci贸** 茅s el proc茅s de convertir un text en un vector num猫ric.
- Els vectors s贸n m茅s fcils de manipular per a les mquines i de comparar.

---

## Vectoritzaci贸 (II)

- Algunes t茅cniques de vectoritzaci贸 (embeddings):
  - **NNLM**: model basat en xarxes neuronals. El nombre de dimensions 茅s fixe.
  - **Word2Vec**: cada paraula 茅s un vector en un espai semntic. El nombre de dimensions 茅s fixe.
  - **FastText**: creat per Facebook. Similar a Word2Vec, per貌 permet representar paraules rares i que no estan en el vocabulari.
  - **GloVe**: model basat en NNLM i Word2Vec.

---

<style scoped>section { font-size:28px; }</style>

## Word2Vec

- Es basa en la idea que les paraules amb significats similars apareixen en contextos similars.
  - "You shall know a word by the company it keeps" (J.R. Firth, 1957).
- Quan una paraula **p** apareix en un text, les paraules properes a `p` s贸n el seu **context**.
- Els diferents contextos de `p` defineixen el significat de `p`.

![70%](../../images/Captura%20de%20pantalla%202024-01-14%20a%20las%2021.35.56.png)

---

<style scoped>section { font-size:30.6px; }</style>

## Word2Vec (II)

- Per cada paraula obtenim un vector **dens** i de **longitud fixa**.
- Cada dimensi贸 del vector representa un **aspecte semntic** de la paraula.
- Representen la seva **posici贸** en un espai semntic n-dimensional.
- Els vectors de paraules amb contextos semblants estaran propers en l'espai semntic.

> Facilita calcular la similitud entre paraules.

![bg right:31% fit](../../images/embeddings.png)

---

<style scoped>section { font-size:31px; }</style>

## Word2Vec (III)

- Els vectors de parales tamb茅 s'anomenen **embeddings** o **representacions de xarxa**.
- **FastText** 茅s una variant de **Word2Vec** que utilitza subparaules.
  - Permet representar paraules rares i que no estan en el vocabulari.
  - til per a lleng眉es amb moltes paraules compostes i derivades.
- Els _embeddings_ generats poden ser utilitzats en una gran varietat de tasques, com pot ser la classificaci贸 de textos, anlisi de sentiments, etc.
- S贸n models que necessiten un entrenament previ amb totes les paraules del vocabulari. A continuaci贸 veurem un exemple.

---

<!--
_class: invert lead
-->

<style scoped>
h1, p {
  color: #FFFFFF;
  text-shadow:
    0px 0px 3px #000000;
}
</style>

# Representaci贸 de textos

![bg opacity](../../images/tf-idf.png)

---

<style scoped>section { font-size:30.6px; }</style>

## Representaci贸 de textos

- Fins ara hem vist com representar paraules, veurem com representar textos, per poder veure les relacions entre les paraules que el formen.
- Algunes de les t猫cniques utilitzades s贸n:
  - **One-hot encoding**: cada token $\rightarrow$ una dimensi贸; valor $\rightarrow$ 1 si el token est i 0 si no.
  - **Bag of Words**: model basat en freq眉猫ncies. El valor de cada cel路la son les aparicions del token en el document. Simple i molt utilitzat.
  - **TF-IDF**: model basat en freq眉猫ncies i inversa de freq眉猫ncies
  - **Word Embeddings**. Vectoritzaci贸 de paraules amb _Word2Vec_, _FastText_, etc

---

<style scoped>section { font-size:30px; }</style>

### One-hot encoding

- El model **one-hot encoding** 茅s un model basat en tokens.
- Els vectors generats s贸n **dispersos** i **grans**, ja que cada token 茅s una dimensi贸.
- Cada token 茅s una dimensi贸 i el valor de cada cel路la 茅s 1 si el token est i 0 si no.
- Els vectors generats s贸n **independents** de la semntica.
- No facilita calcular la similitud entre paraules i textos.

![bg right:37% fit](../../images/one_hot.png)

---

<style scoped>section { font-size:31.5px; }</style>

### Bag of Words (BoW)

- El model **BoW** 茅s un model basat en freq眉猫ncies.
- Es pot entendre com una suma dels vectors one-hot.
- Els vectors generats s贸n **independents** de la semntica.
- El nombre del token es pot entendre com a **ordre** i en molts casos no 茅s aix铆. Aquesta discrepncia pot afectar a la qualitat del model.

![bg right:40% fit](../../images/bag-of-words.png)

---

<style scoped>section { font-size:33px; }</style>

### TF-IDF

- El model **TF-IDF** 茅s un model basat en freq眉猫ncies i inversa de freq眉猫ncies.
- Semblant al model **BoW**, per貌 t茅 en compte la freq眉猫ncia del token en el document i en el conjunt de documents.
- El valor de cada cel路la 茅s el producte de la freq眉猫ncia del token en el document i la inversa de la freq眉猫ncia del token en el conjunt de documents.
- Dona m茅s importncia als tokens que apareixen en pocs documents. Ra贸: els tokens que apareixen en molts documents no solen aportar informaci贸 rellevant.

---

### Word Embeddings

- Els _embeddings_ generats per Word2Vec s贸n vectors de _n_ dimensions.
- Els vectors generats s贸n **densos**, de **longitud fixa** i amb **sentit semntic**.
- Per a representar un **text** pot utilitzar-se la mitjana dels _embeddings_ de les paraules que el formen, el mxim, la suma, etc.

---

#### Generaci贸 de _embeddings_ amb Word2Vec i la llibreria Gensim

```python
from gensim.models import Word2Vec

sentences = [
    ["Gavi", "company", "Pedri"],
    ["Xavi", "entrena", "Bar莽a"],
    ["Gavi", "juga", "Bar莽a"],
    ["Gavi", "chuta"],
    ["Kepa", "para"],
    ["Xavi", "entrena"],
]
model = Word2Vec(sentences, min_count=1)
```

---

#### Visualitzaci贸 dels _embeddings_

```python
gavi = model.wv["Gavi"]
print(gavi)
```

```python
[
    -0.00536227  0.00236431  0.0510335   0.09009273
                                         - 0.0930295 - 0.07116809 0.06458873  0.08972988
                                         - 0.05015428 - 0.03763372
]
```

---

#### Busquem paraules similars

```python
model.wv.most_similar("Gavi")
```

```python
[('Bar莽a', 0.5436005592346191),
 ('Pedri', 0.3792896568775177),
 ('entrena', 0.3004249036312103),
 ('Xavi', 0.10494352877140045),
 ('juga', -0.1311161071062088),
 ('chuta', -0.1897382140159607),
 ('para', -0.22418655455112457),
 ('Kepa', -0.2726020812988281),
 ('company', -0.7287455797195435)]
```

---

<style scoped>section { font-size:32px; }</style>

#### Similitud del cosinus

- Els _embeddings_ generats per Word2Vec s贸n vectors de _n_ dimensions.
- Per a calcular la similitud entre dos vectors s'utilitza la **similitud del cosinus**.
- El resultat 茅s un valor entre -1 i 1.
  - -1: vectors oposats.
  - 0: vectors ortogonals.
  - 1: vectors iguals.
- Aquesta mesura 茅s molt utilitzada en el processament del llenguatge natural.

---

<!--
_class: invert lead
-->

<style scoped>
h1, h2, h3, h4, h5, h6, p {
  color: #FFFFFF;
  font-weight: 800;
  text-shadow:
    0px 0px 3px #000000;
}
</style>

# Conversi贸 de text a veu i veu a text

![bg opacity](../../images/DALL%C2%B7E-2023-11-24-17.55.16-A-vibrant-and-abstract-representation-of-the-concept-of-audio-and-speech-recognition-symbolizing-the-Whisper-speech-to-text-model.-The-image-should-f.jpg)

---

<style scoped>section { font-size:31.5px; }</style>

## Reconeixement de veu i transcripci贸 automtica

- La **s铆ntesi de veu** i la **transcripci贸 automtica** s贸n tasques de **processament del llenguatge natural**.
  - La s铆ntesi de veu 茅s el proc茅s de **convertir un arxiu de text en un arxiu d'udio**.
  - La transcripci贸 automtica 茅s el proc茅s de **convertir un arxiu d'udio en un arxiu de text**.
- Ambdues s贸n unes t猫cniques molt importants en el processament del llenguatge natural; encara que no solen estar en primer pla.
- En aquest apartat veurem com funcionen aquestes t猫cniques.

---

<style scoped>section { font-size:31px; }</style>

## S铆ntesi de veu

- La **s铆ntesi de veu** 茅s el proc茅s de **convertir un arxiu de text en un arxiu d'udio**.
- Aquesta tecnologia ha millorat molt en els 煤ltims anys, grcies als models de llenguatge i a les xarxes neuronals.
- Hi ha diversos enfocaments, a continuaci贸 veurem els m茅s importants.

![bg right fit](../../images/Voice-Synthesis.jpg)

---

### S铆ntesi de veu: concatenaci贸 de sons

- Es basa en la grabaci贸 de **sons** i la seva **concatenaci贸** per a formar paraules i frases.
- Es busquen els sons m茅s adaptats al context i es combinen de manera que soni el m茅s natural possible.
- Requereix una gran quantitat de dades i dificulata adapatar-se a contextos nous.

---

### S铆ntesi de veu: s铆ntesi basada en formants

- Els **formants** s贸n les **frequencies** de les cordes vocliques.
- S'ajusten els formants per representar els diferents sons i es combinen per a formar paraules i frases.
- La s铆ntesi de formants permet obtindre veus clares i precises.
- Requereixen menys dades que la s铆ntesi per concatenaci贸 de sons, tenen, per貌 menys naturalitat i expressivitat.

---

### S铆ntesi de veu: s铆ntesi basada en unitats

- Es basa en la **s铆ntesi d'unitats** m茅s petites que les paraules, com ara **fonemes** o **diftongs**.
  - Aquestes unitats s'enmagatzemen en una base de dades i es combinen de forma dinmica segons el text.
  - La s铆ntesi de unitats permet obtenir veus m茅s naturals i expressives.
  - Requereixen menys dades que la s铆ntesi per concatenaci贸 de sons.

---

<style scoped>section { font-size:32px; }</style>

### S铆ntesi de veu: basada en xarxes neuronals (I)

- Les xarxes neuronals s贸n capaces de sintetitzar veus a partir de text.
- Aquestes xarxes s'entrenen amb grans quantitats de dades de veu i text i s贸n capaces de sintetitzar veus molt naturals.
- S'utiltzen Xarxes Neural Recurrents (RNN) espec铆fiques, com ara les xarxes LSTM o GRU o models m茅s moderns com ara les xarxes Transformer.
- Aquestes xarxes s贸n capaces de sintetitzar veus molt naturals i expressives, sempre que tinguin suficientes dades d'entrenament i suficient capacitat de proc茅s.

---

<style scoped>section { font-size:31.5px; }</style>

### S铆ntesi de veu: basada en xarxes neuronals (II)

- Aquests models es basen en els espectrogrames de les veus (representaci贸 de la veu en funci贸 del temps i la freq眉猫ncia).

- Funcionen en quatre etapes:
  - **Etapa de seq眉猫ncia a seq眉猫ncia**: el text es converteix en una seq眉猫ncia de vectors.
  - **Etapa de seq眉猫ncia a espectrograma**: els vectors es converteixen en espectrogrames.
  - **Etapa de s铆ntesi**: els espectrogrames es converteixen en veu.
  - **Etapa de postprocessament**: es millora la qualitat de la veu.

---

![bg 70%](../../images/psesgmsndedform00a.jpg)

---

## Transcripci贸 automtica

- En l'actualitat el **reconeixement de veu** 茅s una tasca **molt madura**.
- Els assistents virtuals com **Siri**, **Alexa** o **Google Assistant** s贸n capa莽os de recon猫ixer veu amb una gran precisi贸.
- Si volem implementar un sistema de reconeixement de veu, podem utilitzar eines com **Google Cloud Speech-to-Text** o **IBM Watson Speech to Text**.
- Aquestes eines es basen en les mat猫ixes t猫cniques que hem vist per a la s铆ntesi de veu.

---

<style scoped>section { font-size:31px; }</style>

### Models de reconeixement de veu i transcripci贸 automtica

- Els models de reconeixement de veu i transcripci贸 automtica s贸n models de **seq眉猫ncia a seq眉猫ncia**.
- Aquests models s贸n capa莽os de convertir una seq眉猫ncia d'entrada en una seq眉猫ncia de sortida.
- Alguns dels models m茅s importants s贸n:
  - **Whisper**: model de reconeixement de veu de **OpenAI**.
  - **DeepSpeech**: model de reconeixement de veu de **Mozilla**.
  - **Hugging Face Speech2Text**: model de reconeixement de veu.
  - **Bark**: Model de generaci贸 de veu de **Suno Labs**.

---

<!--
_class: invert lead
-->

<style scoped>
h1, p {
  color: #FFFFFF;
  text-shadow:
    0px 0px 3px #000000;
}
</style>

# Similitud entre textos

![bg opacity](../../images/similarity.jpg)

---

## Similitud entre textos

- La similitud entre textos 茅s una mesura que indica com de semblants s贸n dos textos.
- s una de les funcions m茅s obvies del processament del llenguatge natural.
- El clcul de la similitud entre textos, per貌, 茅s una tasca **dif铆cil**.
- Anem a veure algunes t猫cniques de les m茅s utilitzades.

---

<style scoped>section { font-size:31px; }</style>

### T茅cniques per a calcular la similitud entre textos (I)

- **Basades en regles**: Es basen en regles predefinides; fcils d'implementar i 煤tils per a casos senzills.
  - **Distncia de Levenshtein**: s el nombre m铆nim d'operacions per a transformar una cadena en una altra.
  - **Distncia de Hamming**: s el nombre de posicions en les quals dues cadenes de la mateixa longitud difereixen.
  - **Recompte de paraules**: s el nombre com煤 de paraules entre dos textos.
  - **Distncia de Jaccard**: s el nombre de paraules comunes entre dos textos dividit pel nombre total de paraules dels dos textos.

---

<style scoped>section { font-size:31px; }</style>

### T茅cniques per a calcular la similitud entre textos (II)

- **Basades en caracter铆stiques sintctiques**: Es basen en les caracter铆stiques sintctiques i gramaticals dels textos. Impliquen un proc茅s de **parsejat** dels textos per analitzar la seva
  estructura sintctica.
- **Basades en caracter铆stiques semntiques**: Es basen en les caracter铆stiques semntiques dels textos. Aqu铆 models com Word2Vec s贸n molt 煤tils, al permetre representar el significat contextual de les paraules.
  - **Word Mover's Distance**: Mesura la distncia entre dos textos com la distncia entre els vectors de les paraules dels dos textos.
  - **Similitud del cosinus**: Utilitza el cosinus de l'angle entre ells.

---

<style scoped>section { font-size:33px; }</style>

### T茅cniques per a calcular la similitud entre textos (II)

- **Basades en l'aprenentatge automtic**: Es basen en l'aprenentatge automtic per a calcular la similitud entre textos.
  - **BERT i GPT**: Models de llenguatge basats en xarxes neuronals que pot ser utilitzat per a calcular la similitud entre textos.
  - **Universal Sentence Encoder**: Model espec铆ficament entrenat per al _transfer learning_ (aprenentatge per a la transfer猫ncia; utilitzar un model entrenat per a una tasca per a una altra).

---

### Utilitats de la similitud entre textos

- **Correcci贸 ortogrfica**: Per a corregir una paraula es busca la paraula m茅s semblant.
- **Classificaci贸 de textos**: Per a classificar un text es busca el text m茅s semblant.
- **Agrupaci贸 de textos**: til per a agrupar textos similars en clusters.
- **B煤squeda de resposte**: Per a trobar la resposta a una pregunta es busquen texts semblants a la pregunta.

---

<style scoped>section { font-size:30.5px; }</style>

### Word Embeddings

- Les t茅cniques clssiques d'NLP es basen en representacions no semntiques com BoW o TF-IDF.
- Les modernes es basen en LLMs (Language Models) i Word Embeddings.
- Com ja hem vist, els _embeddings_ generats per Word2Vec s贸n vectors de _n_ dimensions.
- Per a representar un text pot utilitzar-se la mitjana dels _embeddings_ de les paraules que el formen.
- Els vectors generats s贸n **densos**, de **longitud fixa** i amb **sentit semntic**.
- Facilita calcular la similitud entre paraules i textos.

---

### LLMS (Language Models)

- Els **LLMs** s贸n models complexos basats en xarxes neuronals recurrents i l'arquitectura **Transformer**.
- Poden aprendre la semntica del text i generar els seus propis _embeddings_ utilitzant el mecansime d'**atenci贸**.
- Demostren un gran rendiment en moltes tasques, com pot ser el clcul de la similitud entre textos.
- S贸n complexos i necessiten un entrenament previ amb un gran volum de dades.

---

<!--
_class: invert lead
-->

<style scoped>
h1, p {
  color: #FFFFFF;
  text-shadow:
    0px 0px 3px #000000;
}
</style>

# Classificaci贸 de textos i anlisi de sentiments

![bg opacity](../../images/sentiment.png)

---

## Anlisi de sentiments

- L'anlisi de sentiments 茅s un tipus de classificaci贸 i una de les tasques m茅s utilitzades en el processament del llenguatge natural.
- L'objectiu 茅s determinar l'actitud d'un autor respecte a un tema o producte.
- Es basa en la **polaritat** del text, que pot ser **positiva**, **negativa** o **neutra**.
- Tamb茅 poden buscar-se emocions concretes, com pot ser **alegria**, **tristesa**, **ira**, etc.

---

<style scoped>section { font-size:34px; }</style>

## Anlisi subjectiva i objectiva

- L'anlisi de sentiments pot ser **subjectiva** o **objectiva**.
- L'anlisi subjectiva busca les **emocions i sentiments** de l'autor.
- L'anlisi objectiva es basa en **fets i dades concretes**.
- Els dos tipus d'anlisi s贸n **complementaris** i poden utilitzar-se conjuntament.
- Ex: "_La pel路l铆cula t茅 grans moments, per貌 el final 茅s molt trist_".
  - Anlisi subjectiva: "_La pel路l铆cula 茅s bona_".
  - Anlisi objectiva: "_El final 茅s trist_".

---

<style scoped>section { font-size:30.5px; }</style>

## Preprocessament del text

- El **tractament** de textos facilita obtenir bons resultats en NLP.
- Permet redu茂r la **dimensionalitat** dels textos, eliminar el soroll i capturar la semntica.
- Algunes de les t猫cniques m茅s utilitzades s贸n:
  - **Tokenitzaci贸**: vist anteriorment.
  - **Normalitzaci贸**: convertir el text a un format estndard.
  - **Eliminaci贸 de stopwords**: eliminar paraules que no aporten informaci贸.
  - **Stemming** i **lematitzaci贸**: convertir les paraules a forma base.
  - **Gesti贸 de negacions i modalitats**: convertir a un format estndard.

---

### Preprocessament: tokenitzaci贸

- Com ja hem vist, la tokenitzaci贸 茅s el proc茅s de dividir un text en tokens.
- Els tokens poden ser paraules, subparaules, signes de puntuaci贸, etc.
- Facilita una anlisi m茅s profund del text i extreure caracter铆stiques rellevants.
- Ex: "El Bar莽a est en crisi" $\rightarrow$ `["El", "Bar莽a", "est", "en", "crisi"]`.

---

<style scoped>section { font-size:34px; }</style>

### Preprocessament: normalitzaci贸

- La **normalitzaci贸** implica el路liminar els elements que no aporten informaci贸.
  - Nombres, signes de puntuaci贸, etc.
- Tamb茅 implica convertir el text a un format estndard, passant a min煤scules i llevant espais innecessaris, per exemple.
  - Ex: "El Bar莽a est en crisi! " $\rightarrow$ "el Bar莽a est en crisi".
- La normalitzaci贸 facilita la comparaci贸 entre textos i la detecci贸 de paraules clau.

---

### Preprocessament: eliminaci贸 d'stopwords

- Les **stopwords** s贸n paraules que no aporten informaci贸 al text.
- S贸n paraules molt comunes en un idioma, com pot ser articles, preposicions, etc.
- Els textos despr茅s de processar-se amb stopwords s贸n m茅s fcils de tractar i m茅s rpids de processar.
- Ex: "El Bar莽a est en crisi" $\rightarrow$ `["Bar莽a", "crisi"]`.

---

<style scoped>section { font-size:34px; }</style>

### Preprocessament: stemming i lematitzaci贸

- El **stemming** i la **lematitzaci贸** s贸n t猫cniques per a convertir les paraules a la seva forma base i facilitar l'agrupaci贸 de paraules relacionades.
- L'**stemming** 茅s un proc茅s heur铆stic basat en regles, mentre que la **lematitzaci贸** 茅s un proc茅s basat en coneixements ling眉铆stics.
  - Ex d'stemming: "jugar", "jugar茅", "jugar" $\rightarrow$ "jug".
  - Ex de lematitzaci贸: "jugar", "jugar茅", "jugar" $\rightarrow$ "jugar".
- El stemming 茅s m茅s rpid, per貌 la lematitzaci贸 茅s m茅s precisa.

---

### Preprocessament: negacions i modalitats

- Les **negacions** i **modalitats** poden canviar el significat d'una frase.
- Ex: _"El Bar莽a no est en crisi"_, _"El Bar莽a pot estar en crisi"_, _"Deuries anar a l'estadi"_.
- Els models de NLP no poden interpretar aquestes frases sense un tractament previ.
- Necessitem t茅cniques espec铆fiques, com la detecci贸 de **dobles negacions** i la **reassignaci贸 de polaritat**.

---

## Enfocaments per a l'anlisi de sentiments

- Com en totes les tasques de NLP, l'anlisi de sentiments pot ser abordada amb diferents enfocaments.
- Els enfocaments m茅s utilitzats s贸n:
  - Basats en **regles**
  - Basats en l'**aprenentatge automtic supervisat**
  - Basats en l'**aprenentatge automtic no supervisat**

---

### Enfocament basat en regles

- Els enfocaments basats en regles s贸n els m茅s senzills i rpids.
- Tamb茅 s'anomenen **lexicon-based**.
- Es basen llistes de paraules per a determinar la polaritat del text.
- De cada paraula es busca la seva polaritat en el llistat i es fa una suma..
  - _"El Bar莽a est en crisi"_. _"crisi"_ $\rightarrow$ **-1** $\rightarrow$ **negatiu**.
  - _"Me fa il路lusi贸 anar a l'estadi"_. _"il路lusi贸"_ $\rightarrow$ **1** $\rightarrow$ **positiu**.
  - _"El partit va acabar en empat"_. _"empat"_ $\rightarrow$ **0** $\rightarrow$ **neutre**.

---

### Enfocament basat en l'AA supervisat

- Els enfocaments basats en l'aprenentatge automtic supervisat s贸n alguns dels m茅s utilitzats.
- Consisteixen en entrenar un model amb un conjunt de dades etiquetades amb la polaritat del text.
- El model identifica els patrons en el text que determinen la polaritat.
- Alguns dels models m茅s utilitzats s贸n: **Naive Bayes**, **Support Vector Machines**, **Random Forest**, **Xarxes neuronals**

---

<style scoped>section { font-size:34px; }</style>

### Enfocament basat en l'AA no supervisat

- Els enfocaments basats en l'aprenentatge es basen en identificar patrons en el text sense necessitar etiquetes predefinides.
- Es poden utilitzar t猫cniques com la **clusteritzaci贸** per a agrupar els textos en clusters segons la seva polaritat.
- Una vegada agrupats els textos, es poden etiquetar manualment els clusters.
- Aquestes t猫cniques s贸n 煤tils per a detectar patrons en el text i per a agrupar textos semblants (segons la distncia entre textos).

---

<style scoped>section { font-size:33px; }</style>

## Models per a l'anlisi de sentiments

Alguns dels models m茅s utilitzats per a l'anlisi de sentiments s贸n:

- **BOW** + clasificador: model basat en BoW i un classificador.
- **Embeddings** + clasificador: Word2Vec, FastText, etc.
- **VADER**: model basat en regles, molt utilitzat en angl猫s.
- **Transformers**: com ja hem vist, els transformers s贸n models espec铆fics per a NLP molt potents. Un dels models m茅s utilitzats 茅s **BERT**, de q眉al utilitzarem una implementaci贸 en la segona
  prctica.

---

## Xarxes neuronals per a l'anlisi de sentiments

- Les xarxes neuronals s贸n un dels models m茅s utilitzats per a l'anlisi de sentiments.
- Les xarxes neuronals s贸n capaces d'aprendre els patrons en el text i de generar els seus propis _embeddings_.

---

<!--
_class: invert lead
-->

<style scoped>
h1, p {
  color: #FFFFFF;
  text-shadow:
    0px 0px 3px #000000;
}
</style>

# Models de llenguatge

![bg opacity](../../images/language-model.png)

---

## Models de llenguatge

- Fins ara hem vist com representar el text i com acomplir tasques com l'anlisi de sentiments.
- Hem comentat que els models de llenguatge s贸n eines molt potentes que poden ser utilitzades en moltes tasques.
- En aquesta secci贸 veurem qu猫 s贸n els models de llenguatge i com funcionen.
- Tamb茅 veurem alguns dels m茅s utilitzats i les seves aplicacions.

---

<style scoped>section { font-size:31px; }</style>

## Aplicacions dels models de llenguatge

- S贸n dels camps m茅s actius i complexos de la IA.
- S贸n la base de moltes aplicacions de NLP, com pot ser:
  - **Traducci贸 automtica**: traduir un text d'un idioma a un altre.
  - **Reconeixement de veu**: transcriure un text a partir d'un arxiu d'udio.
  - **S铆ntesi de veu**: generar un arxiu d'udio a partir d'un text.
  - **Generaci贸 i resum de text**: generar textos a partir d'un context.
  - **Anlisi de sentiments**: determinar la polaritat d'un text.
  - **Classificaci贸 de text**: classificar un text en una categoria.
  - **Generaci贸 de textos**: generar textos a partir d'un tema o un estil.

---

<style scoped>section { font-size:30px; }</style>

### Definici贸

- Un **model de llenguatge** assigna una probabilitat a una seq眉猫ncia de paraules.
  - Per tant, permet predir la seg眉ent paraula d'una seq眉猫ncia.
  - Ex: "El bar莽a est en \_\_\_" $\rightarrow$ `[{crisi: 0.8}, {forma: 0.1}, {casa: 0.1}]`
- Es basen en la idea que les paraules d'una seq眉猫ncia no s贸n independents, sin贸 que depenen de les paraules anteriors.
- Permeten calcular la "**validesa**" d'una seq眉猫ncia de paraules.
  - No 茅s el mateix que la **correcci贸** d'una seq眉猫ncia de paraules.
  - Intentem modelar el llenguatge hum, amb els seus **matissos i ambig眉itats**.

---

## Hist貌ria: Models basats en regles

- Els models de llenguatge s贸n un dels camps m茅s antics del processament del llenguatge natural.
- Es basen en regles gramaticals i ling眉铆stiques per definir les caracter铆stiques del llenguatge.
- Les regles estan definides per experts i s贸n dif铆cils de modificar.
- No s贸n flexibles i no poden adaptar-se a nous contextos.
- Ex: Gramtica de Chomsky, Gramtica de Montague, etc.

---

<style scoped>section { font-size:30.6px; }</style>

## Hist貌ria: Models estocstics

- Els models basats en regles van ser substitu茂ts pels basats en **estad铆stiques**, m茅s flexibles i que modelen millor el llenguatge. Els models de Markov van ser els primers en tindre resultats acceptables.
- Es basen en la idea que les paraules d'una seq眉猫ncia no s贸n independents, sin贸 que depenen de les paraules anteriors. Exemples:
  - **N-gram**: modela cada paraula en funci贸 de les n paraules anteriors. (uni, bi, tri, etc).
  - **Skip-gram**: modela cada paraula en funci贸 de les n paraules anteriors i posteriors.
  - **Syntax-based**: es basen en l'estructura sintctica de les frases i no en la seva seq眉猫ncia.

---

<style scoped>section { font-size:30px; }</style>

## Hist貌ria: RNN

- Els models basats en estad铆stiques van ser substitu茂ts per models basats en **xarxes neuronals**.
- Els primers models d'aquest tipus van ser els **RNN** (xarxes neuronals recurrents).
  - A difer猫ncia de les xarxes neuronals tradicionals, les RNN tenen **mem貌ria**.
  - L'entrada d'una neurona pot anar determinada per la sortida d'ella mateixa.
  - Permeten processar seq眉猫ncies de longitud variable.
  - Els models de llenguatge basats en RNN van ser els primers en obtenir resultats acceptables.

---

<style scoped>section { font-size:30px; }</style>

## Hist貌ria: LLM

- Els models basats en RNN van ser substitu茂ts per models basats en **transformers**.
- Els transformers s贸n models basats en xarxes neuronals que utilitzen el mecanisme d'**atenci贸**.
  - S贸n m茅s potents que les RNN i permeten obtenir resultats molt millors.
  - S贸n els models m茅s utilitzats en l'actualitat.
  - Necessiten un entrenament previ amb un **gran** volum de dades (_corpus_)
  - Mostren la capacitat d'entendre el context, la semntica i la sintxis del text.

---

## Hist貌ria: LLM (II)

- El mecanisme d'atenci贸 茅s un mecanisme que permet a les xarxes neuronals aprendre a **centrar-se** en les parts importants de les seves entrades.
- s un mecanisme que imita el comportament hum.
- Podem entendre'l com una **capa** que s'afegeix a una xarxa neuronal.
- Els transformers s贸n models basats en xarxes neuronals que utilitzen una variant del mecanisme d'atenci贸 anomenada **self-attention**.

---

<!--
_class: invert lead
-->

<style scoped>
h1, p {
  color: #FFFFFF;
  text-shadow:
    0px 0px 3px #000000;
}
</style>

# Arquitectures per a NLP

![bg opacity](../../images/recursive_nn_nlp.png)

---

<style scoped>section { font-size:30px; }</style>

## Cadenes de Markov

- Els **models ocults de Markov** (HMM) s贸n models estocstics que permeten modelar seq眉猫ncies de paraules.

- Es basen en la idea que les paraules duna seq眉猫ncia no s贸n independents, sin贸 que depenen de les paraules anteriors.

- Els HMM s贸n capa莽os de modelar la probabilitat de transici贸 entre paraules.

- El seu principal desavantatge 茅s que no poden modelar depend猫ncies a llarg termini.

![bg right:35% fit](../../images/mdp.png)

---

## Xarxes neuronals recurrents (RNN)

- Com ja hem parlat, les xarxes neuronals recurrents (RNN) s贸n xarxes neuronals que poden processar seq眉猫ncies de longitud variable de forma eficient.

- En aquesta secci贸 veuren en m茅s detall com funcionen les RNN i com s贸n utilitzades en NLP.

![bg right:40% fit](../../images/rnn.png)

---

### Mem貌ria

- La principal caracter铆stica de les RNN 茅s que tenen **mem貌ria**.
- Com s'aconsegueix aquesta mem貌ria?
  - La sortida d'una neurona pot anar determinada per la **sortida d'ella mateixa**.
  - Aix贸 permet que la informaci贸 puga processar-se un una mateixa capa, sense necessitat de capa addicional.
  - Apareix un nou problema: el **desvaiment del gradient**. Quina informaci贸 es mant茅 i quina no?
  - Per aix贸 han anat diferents aquitectures al llarg del temps.

---

### Xarxes Recurrents Tradicionals (I)

- La sortida d'una neurona pot anar determinada per la sortida d'ella mateixa.
- Cada neurona t茅 dues entrades: el **valor actual** i el **valor anterior**.
- Aquesta mem貌ria 茅s inherenment de curta durada.
- Per facilitar la comprensi贸 es solen mostrar de forma **desplegada**, com veurem a continuaci贸.
- S贸n una millora respecte als models ocults de Markov, per貌 molt vulnerables als problemes de desvaiment del gradient.

---

### Xarxes Recurrents Tradicionals (II)

- En el cas de les RNN, la mem貌ria es mant茅 en la capa oculta.
- En el seg眉ent esquema podem veure com es representa una RNN sense desplegar.

![bg right:50% fit](../../images/rnn_comprimida.png)

---

<style scoped>section { font-size:26px; }</style>

<!-- _footer: "" -->

### Xarxes Recurrents Tradicionals (III)

- En l'esquema es representen tres passos de temps.
- Solament tenim un node d'entrada, pero al necessitar tres passos de temps, necessitem passar tres valors d'entrada.
- Els valors a entrenar en les neurones recurrents s贸n tres: pes de la entrada, pes de la entrada recurrent i el biaix.

## ![40%](../../images/rnn_desplegada.png)

---

### LSTM (Long Short-Term Memory) (I)

- Les **LSTM** s贸n una millora de les RNN tradicionals implementant una **mem貌ria a llarg termini**.
- Tenen una **mem貌ria interna** que pot ser mantinguda, **modificada** o **eliminada** segons les necessitats.
- **Millora el desvaiment** del gradient **a costa de ser m茅s complexa i costosa** de processar.
- Cada cel路la LSTM equival a quatre capes en una RNN tradicional.

---

### LSTM (Long Short-Term Memory) (II)

- L'estat intern de la cel路la es gestiona utilitzant les **portes**.
- Les LSTM tenen tres portes:
  - **Porta d'oblit**: Si s'activa la cel路la oblidar la informaci贸 anterior.
  - **Porta d'entrada**: Si s'activa la cel路la afegir nova informaci贸.
  - **Porta de sortida**: Si s'activa la cel路la generar la sortida.
- Aquestes portes permeten a la LSTM mantenir la informaci贸 important i descartar la que no ho 茅s.

---

### LSTM (Long Short-Term Memory) (III)

![bg fit 90%](../../images/lstm_hor.png)

---

### GRU (Gated Recurrent Unit)

- Les GRU s贸n una altra millora de les RNN tradicionals implementant una mem貌ria a llarg termini.
- No tenen una mem貌ria interna com les LSTM, per貌 s贸n m茅s senzilles i m茅s rpides de processar.
- Hi ha dues portes: porta d'actualitzaci贸 i porta de reinici. En conjunt determinen la quantitat d'informaci贸 que es mant茅 i la que es descarta.
- Millora el desvaiment del gradient sense ser tan complexa com les LSTM.

---

### GRU (Gated Recurrent Unit) (II)

- Les GRU tenen dues portes: **porta d'actualitzaci贸** i **porta de reinici**.
- Si s'activa la porta de reinici la cel路la oblidar la informaci贸 anterior.
- Si s'activa la porta d'actualitzaci贸 la sortida incorporar nova informaci贸.
  - Aquesta informaci贸 pot ser la mateixa que la anterior o una combinaci贸 de la nova i la anterior. El poder retenir la informaci贸 anterior 茅s el que fa que les GRU no tinguin una mem貌ria interna com les LSTM.
- L'efecte 茅s semblant al de les LSTM, per貌 amb menys complexitat.

---

### GRU (Gated Recurrent Unit) (III)

![bg fit 90%](../../images/gru_hor.png)

---

<!--
_class: invert lead
-->

<style scoped>
h1, h2, h3, h4, h5, h6, p {
  color: #FFFFFF;
  font-weight: 800;
  text-shadow:
    0px 0px 3px #000000;
}
</style>

## Transformers

![bg opacity](../../images/transformers.png)

---

### Introducci贸

- Com ja hem comentat anteriorment, els **transformers** s贸n models basats en xarxes neuronals que utilitzen el mecanisme d'**atenci贸**.
- S贸n els models m茅s utilitzats en l'actualitat en el processament del llenguatge natural.
- La seva arquitectura innovadora permet obtenir resultats molt millors que els models anteriors i aprofitar el **parallelisme** i les **GPU**.
- Per la seva complexitat anem a veurel's en m茅s detall.

---

### Orige

- Els transformers van ser introdu茂ts per **Vaswani et al.** en el 2017.
  - El paper original es titula "_Attention is All You Need_".
- Els transformers van ser dissenyats per a millorar el rendiment dels models de llenguatge.
- El seu primer 煤s va ser en la tasca de traducci贸 automtica.
- Va posar en primer pla el mecanisme d'atenci贸 com a eina fonamental en el processament del llenguatge natural.

---

### Arquitectura dels transformers (I)

- Els transformers s贸n models moderns i molt complexos.
- Per contra, si veiem les seves parts per separat, 茅s m茅s fcil entendre'l's.
- Anem a veure punt per punt les seves parts principals i com funcionen.

![bg right:38% fit](../../images/transformers_arquitectura.png)

---

<style scoped>section { font-size:33px; }</style>

### Arquitectura dels transformers (II)

- En un nivell superficial, els transformers funcionen com una caixa negra.
- Reben com a entrada un text i generen com a sortida un text.
- La seva complexitat rau en la seva arquitectura interna.
- Els transformers tenen _dos_ parts principals: **encoders** i **decoders**.

![bg right:40%](../../images/transformer_ml01-768x644.png)

---

<style scoped>section { font-size:33px; }</style>

### Arquitectura dels transformers (III)

- L'entrada passa per una s猫rie de capes d'encoders.
- A continuaci贸, la sortida dels encoders passa per una s猫rie de capes de decoders.
- En el paper original: **6 capes d'encoders i 6 capes de decoders**.
- Tamb茅 podem passar un "**target**" com a entrada, **per entrenar**

![bg right:40%](../../images/transformer_ml02-768x644.png)

---

<style scoped>section { font-size:30px; }</style>

### Encoders i decoders

- Els **encoders** i **decoders** s贸n les parts principals dels transformers.
- A nivell intern son semblants i comparteixen moltes caracter铆stiques.
  - Tenen en l'entrada una (o m茅s) capa d'**atenci贸** i com a sortida una capa **feed-forward**.
- La difer猫ncia principal 茅s que els **encoders** solament tenen una capa d'atenci贸, mentre que els **decoders** tenen dues.

![bg right:45% ](../../images/transformer_ml03-768x644-1.png)

---

<style scoped>section { font-size:30px; }</style>

### Embeddings i posicions

- Els transformers utilitzen **embeddings** per a representar les paraules (de 512 dimensions en el paper original).
- A m茅s, utilitzen el **positional encoding** per a representar la posici贸 de les paraules en la seq眉猫ncia.
  - B谩sicament una funci贸 sinusoidal que varia segons la posici贸 de la paraula, per lo que el mateix vector en diferents posicions ser un poc diferent.
  - $$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})$$
- Aquest encoding mant茅 la informaci贸 de la posici贸 de les paraules en la seq眉猫ncia; al mateix temps que permet **enviar tots els tokens a la xarxa al mateix temps**.

---

![bg  fit](../../images/transformers_arquitectura_inputs.png)

---

<style scoped>section { font-size:30px; }</style>

### Encoder

- Els **encoders** estan compostos per tres capes:
  - **Self-attention**: per a calcular la importncia de cada paraula en la seq眉猫ncia. A continuaci贸 veurem com funciona.
  - **Feed-forward**: per a processar la informaci贸 obtinguda de l'atenci贸.
  - **Normalization i conexions residuals**: per a evitar el desvaiment del gradient i facilitar el seu entrenament.

![bg right:40%](../../images/transformer_ml04-768x644.png)

---

<style scoped>section { font-size:31px; }</style>

### Self-attention (I)

- El **self-attention** 茅s el mecanisme clau dels transformers.
- Permet a la xarxa "centrar-se" en les parts importants de la seq眉猫ncia.
  - Ex: en la frase _"El gat gris va a la casa"_ sabem que $gris$ i $gat$ estan relacionats. Com ho pot saber la xarxa?
- Els transformers creen un "**Soft dictionary**" d'atencions en les paraules de la seq眉猫ncia. Aix铆, l'atenci贸 de $gris$ en $gat$ ser $1$ i en $casa$ ser $-1$.
- El que el diccionari siga "soft" vol dir que pot anar modificant-se.
  - En la frase "El gat va a la casa gris" l'atenci贸 de $gris$ en $casa$ ser $1$.

---

<style scoped>section { font-size:34px; }</style>

### Self-attention (II)

- Per calcular el self-attention es generen tres matrius a partir de la seq眉猫ncia d'entrada: **Q** (query), **K** (key) i **V** (value)
- **Q** i **K** s贸n matrius que representen la seq眉猫ncia d'entrada i **V** 茅s la matriu que representa el valor de cada paraula.
- Per obtindre l'atenci贸 multiplicarem **Q** per la transposada de **K**
  - Obtindrem la similitud entre les paraules.
- El resultat el multiplicarem pel valor de **V**.
  - Obtindrem la matriu d'atencions.

---

![bg fit](../../images/ejemplo-self-attention-768x336.png)

---

<style scoped>section { font-size:32px; }</style>

### Self-attention (III)

- Els transformers utilitzen el **multi-head attention**.
- Aquesta t猫cnica consisteix en calcular el self-attention amb diferents grups de dimensions.
- Els transformers utilitzen **8** caps de self-attention en el paper original.
- Dels resultats es fa un promig i el resultat, segons els estudis es molt m茅s significatiu.
- s important ressaltar que tot el proc帽es d'atenci贸 es paral路lel i accelerat per la GPU (multiplicaci贸 de matrius), per lo que es molt rpid.

---

<style scoped>section { font-size:31px; }</style>

### Altres tipus d'atenci贸

- A m茅s del **self-attention**, els transformers utilitzen altres tipus d'atenci贸:
  - **Cross-attention**: les entrades del decoder s贸n les sortides de l'encoder. Aix貌 permet al encoder condicionar el decoder, donant-li informaci贸 sobre el context.
  - **Masked attention**: en el decoder, les paraules futures no poden ser utilitzades per a calcular l'atenci贸. Aix貌 evita que el model "mire al futur".

![bg right:33% fit](../../images/transformers_arquitectura_attention-1.png)

---

<style scoped>section { font-size:35px; }</style>

### Normalitzaci贸 i conexions residuals

- Les conexions residuals s贸n una t猫cnica que permeten evitar el desvaiment del gradient.
  - Aquest problema es produeix quan les xarxes s贸n molt profundes.
  - Les conexions residuals permeten que els valors d'entrada es mantinguen en les capes posteriors.
- La normalitzaci贸 permet que els valors d'entrada es mantinguen en un rang determinat.
  - Aix貌 facilita el seu entrenament i millora el seu rendiment.

---

![bg fit](../../images/transformers_arquitectura_addnorm.png)

---

<style scoped>section { font-size:32px; }</style>

### Feed-forward

- La capa **feed-forward** 茅s una capa de xarxa neuronal normal.
- La seva funci贸 茅s processar la informaci贸 obtinguda de l'atenci贸.
- Hi haur dues capes de _dropout_ per a evitar l'overfitting i una funci贸 d'activaci贸 no lineal (ReLU en el paper original).

![bg right fit](../../images/transformers_arquitectura_mlp.png)

---

<style scoped>section { font-size:33px; }</style>

### Decoder

- Els **decoders** s贸n molt semblants als **encoders**.
- En els models normals en l'entranament utilitzem la sortida esperada per validar el resultat.
- Per contra, en els models de llenguatge, el **target** es passa com a entrada per a entrenar el model.
- Aix貌 permet que el model aprenga a generar el text de sortida.

![bg right:40%](../../images/transformer_ml05-768x644.png)

---

### Sortida final del model

- Recordem que utilitzem m煤ltiples capes d'encoders i decoders.
- La sortida final del model 茅s la sortida de l'煤ltima capa de decoders i passa per una capa de **softmax**.
- Aquesta sortida 茅s un vector de longitud igual al nombre de paraules del vocabulari.
- Aquest vector representa la probabilitat de cada paraula en el vocabulari (de 0 a 1).
- La paraula amb m茅s probabilitat ser la paraula de sortida.

---

<style scoped>section { font-size:30px; }</style>

### Aplicacions dels transformers

- Poden ser utilitzats en moltes tasques de NLP pero m茅s enll del NLP tamb茅 s'utilitzen en altres tasques com les seg眉ents:

  - **Visi贸 per computador**: s'est utilitzant per la classificaci贸 d'imatges i altres. _Vision Transformer (ViT)_.
  - **Series temporals**: les seq眉猫ncies de paraules s贸n molt semblants a les seq眉猫ncies de fets en el temps.
  - **Generatius**: s'utilitzen per a generar textos, imatges, etc.
  - **Aprenentatge per refor莽**: s'utilitzen per a entrenar agents en entorns complexos.

- Tots aquests usos els fan una eina molt potent i que poden arribar a substituir molts dels models actuals.

---

<style scoped>section { font-size:29px; }</style>

### Classificaci贸 dels LLM

- **Encoder-only**: No necessiten generar un text de sortida, processen el text d'entrada.
  - Utilitats: classificaci贸 de text, anlisi de sentiments, etc. Ex: BERT, RoBERTa, ALBERT, ELECTRA, etc.
- **Decoder-only**: No transformen el text d'entrada, solament generen un text de sortida.
  - Utilitats: generaci贸 de text, escritura creativa, etc. Ex: GPT-X, Mixtral, Aguila...
- **Encoder + Decoder**: Processen el text d'entrada i generen un text de sortida
  - Utilitats: traducci贸 automtica, resum, esquematitzaci贸, etc. Ex: T5, BART, etc.

---

<style scoped>section { font-size:30px; }</style>

## Utilitzaci贸 dels LLM

- Els transformers s贸n models molt complexos i necessiten un entrenament previ amb un gran volum de dades.
- Normalment s'utilitzen models ja entrenats i que poden ser utilitzats per a diferents tasques.
- Per a millorar el rendiment dels models entrenats es pot utilitzar el **fine-tuning**.
  - El fine-tuning consisteix en entrenar el model amb un conjunt de dades espec铆fic per a la tasca que volem realitzar.
  - S'utilitza un conjunt de dades m茅s petit que el corpus original.
  - En les prctiques utilitzarem un model ja entrenat i li farem fine-tuning per a millorar el rendiment en les tasques que realitzem.

---
