{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f22c112528f9104",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Sistema de processament de text per una tasca específica\n",
    "\n",
    "En aquesta pràctica implementarem un sistema de processament de text per a una tasca específica. En aquest cas, el sistema serà el controlador per veu per un robot.\n",
    "\n",
    "## Tokenització\n",
    "\n",
    "La tokenització és el procés de dividir una cadena de caràcters en unitats més petites. Aquestes unitats poden ser paraules, caràcters, frases, etc.\n",
    "\n",
    "Aquesta es una operació bàsica i necessària per poder aplicar qualsevol tècnica de processament de llenguatge natural.  En aquesta pràctica utilitzarem la tokenització de paraules, és a dir, dividirem el text en paraules. \n",
    "\n",
    "Per fer-ho utilitzarem la llibreria [TextBlob](https://textblob.readthedocs.io/en/dev/). Aquesta llibreria ens permetrà tokenitzar text de forma senzilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa463042a366512",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:38.226841Z",
     "start_time": "2024-01-24T12:01:06.507611Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from nltk>=3.9->textblob) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from nltk>=3.9->textblob) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from nltk>=3.9->textblob) (4.66.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from gensim) (1.26.4)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wrapt in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n",
      "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m387.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m751.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:06\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: smart-open, scipy, gensim\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.14.1\n",
      "    Uninstalling scipy-1.14.1:\n",
      "      Successfully uninstalled scipy-1.14.1\n",
      "Successfully installed gensim-4.3.3 scipy-1.13.1 smart-open-7.1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install textblob==0.19.0\n",
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58c2aae9a15103d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Per tokenitzar un text amb textblob abans hem de baixar els `corpora`. Els `corpora` són conjunts de dades que ens permeten utilitzar les funcionalitats de la llibreria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3075d22fccd656d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:43.614935Z",
     "start_time": "2024-01-24T12:01:38.225263Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/carles/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/carles/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/carles/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/carles/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to /home/carles/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/carles/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75355725643ddfd7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tokenització de paraules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b84aac281b92f2a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Un cop hem baixat el model, podem tokenitzar el text. Per fer-ho, primer de tot hem d'importar la llibreria.\n",
    "\n",
    "A continuació, crearem un objecte `TextBlob` amb el text que volem tokenitzar. Aquest objecte ens permetrà accedir a les funcionalitats de la llibreria.\n",
    "\n",
    "Finalment, utilitzarem la funció `words` per tokenitzar el text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d21a73a1faf29a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:43.861694Z",
     "start_time": "2024-01-24T12:01:43.623536Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['El', 'Barça', 'és', 'el', 'millor', 'equip', 'del', 'món', 'De', 'vegades'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importem la llibreria\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Creem un objecte TextBlob amb el text que volem tokenitzar\n",
    "text = \"El Barça és el millor equip del món. De vegades.\"\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Tokenitzem el text\n",
    "tokens = blob.words\n",
    "\n",
    "# Mostrem els tokens\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973a6d15145f4f69",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Una de les funcions més interessants de TextBlob és que ens permet obtenir la categoria gramatical de cada paraula. Per fer-ho, utilitzarem l'atribut `tags` de l'objecte `TextBlob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc4cc46ccc4b1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:44.128781Z",
     "start_time": "2024-01-24T12:01:43.651475Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('El', 'NNP'),\n",
       " ('Barça', 'NNP'),\n",
       " ('és', 'NNP'),\n",
       " ('el', 'VBZ'),\n",
       " ('millor', 'NN'),\n",
       " ('equip', 'NN'),\n",
       " ('del', 'NN'),\n",
       " ('món', 'NN'),\n",
       " ('De', 'NNP'),\n",
       " ('vegades', 'NNS')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtenim la categoria gramatical de cada paraula\n",
    "# Les categories gramaticals es poden consultar a: https://digiasset.org/html/mbsp-tags.html\n",
    "\n",
    "tags = blob.tags\n",
    "\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257af34438f5bb17",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "També podem fer el `noun phrase extraction`. Aquesta funció ens permet obtenir els grups de paraules que formen un nom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ecb674665f481ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:44.500952Z",
     "start_time": "2024-01-24T12:01:43.670643Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['el barça', 'és el millor equip del món', 'de'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtenim els noun phrases\n",
    "\n",
    "nps = blob.noun_phrases\n",
    "\n",
    "nps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71daa861f8f2f398",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tokenització de frases\n",
    "\n",
    "En alguns casos, també és necessari tokenitzar el text en frases. Per fer-ho, utilitzarem la funció `sentences` de l'objecte `TextBlob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a35f363ff99bedf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:44.503925Z",
     "start_time": "2024-01-24T12:01:43.683607Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"El Barça és el millor equip del món.\"), Sentence(\"De vegades.\")]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenitzem el text en frases\n",
    "\n",
    "text = \"El Barça és el millor equip del món. De vegades.\"\n",
    "blob = TextBlob(text)\n",
    "sentences = blob.sentences\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc5c0978ac88feb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stopwords i signes de puntuació\n",
    "\n",
    "Les *stopwords* són paraules que no aporten informació rellevant per a la tasca que estem realitzant. Per exemple, en la tasca de classificació de text, les *stopwords* no aporten informació per a la classificació.\n",
    "\n",
    "Per tant, en molts casos, és recomanable eliminar les *stopwords* del text abans d'aplicar qualsevol tècnica de processament de llenguatge natural.\n",
    "\n",
    "TextBlob ens permet eliminar les *stopwords* del text. Per fer-ho, utilitzarem la llista de *stopwords* que té NLTK (llibreria en la que es basa TextBlob)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c81ba5087a3c67d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:44.650908Z",
     "start_time": "2024-01-24T12:01:43.696Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/carles/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'abans',\n",
       " 'ací',\n",
       " 'ah',\n",
       " 'així',\n",
       " 'això',\n",
       " 'al',\n",
       " 'aleshores',\n",
       " 'algun',\n",
       " 'alguna',\n",
       " 'algunes',\n",
       " 'alguns',\n",
       " 'alhora',\n",
       " 'allà',\n",
       " 'allí',\n",
       " 'allò',\n",
       " 'als',\n",
       " 'altra',\n",
       " 'altre',\n",
       " 'altres',\n",
       " 'amb',\n",
       " 'ambdues',\n",
       " 'ambdós',\n",
       " 'anar',\n",
       " 'ans',\n",
       " 'apa',\n",
       " 'aquell',\n",
       " 'aquella',\n",
       " 'aquelles',\n",
       " 'aquells',\n",
       " 'aquest',\n",
       " 'aquesta',\n",
       " 'aquestes',\n",
       " 'aquests',\n",
       " 'aquí',\n",
       " 'baix',\n",
       " 'bastant',\n",
       " 'bé',\n",
       " 'cada',\n",
       " 'cadascuna',\n",
       " 'cadascunes',\n",
       " 'cadascuns',\n",
       " 'cadascú',\n",
       " 'com',\n",
       " 'consegueixo',\n",
       " 'conseguim',\n",
       " 'conseguir',\n",
       " 'consigueix',\n",
       " 'consigueixen',\n",
       " 'consigueixes',\n",
       " 'contra',\n",
       " \"d'un\",\n",
       " \"d'una\",\n",
       " \"d'unes\",\n",
       " \"d'uns\",\n",
       " 'dalt',\n",
       " 'de',\n",
       " 'del',\n",
       " 'dels',\n",
       " 'des',\n",
       " 'des de',\n",
       " 'després',\n",
       " 'dins',\n",
       " 'dintre',\n",
       " 'donat',\n",
       " 'doncs',\n",
       " 'durant',\n",
       " 'e',\n",
       " 'eh',\n",
       " 'el',\n",
       " 'elles',\n",
       " 'ells',\n",
       " 'els',\n",
       " 'em',\n",
       " 'en',\n",
       " 'encara',\n",
       " 'ens',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'erem',\n",
       " 'eren',\n",
       " 'eres',\n",
       " 'es',\n",
       " 'esta',\n",
       " 'estan',\n",
       " 'estat',\n",
       " 'estava',\n",
       " 'estaven',\n",
       " 'estem',\n",
       " 'esteu',\n",
       " 'estic',\n",
       " 'està',\n",
       " 'estàvem',\n",
       " 'estàveu',\n",
       " 'et',\n",
       " 'etc',\n",
       " 'ets',\n",
       " 'fa',\n",
       " 'faig',\n",
       " 'fan',\n",
       " 'fas',\n",
       " 'fem',\n",
       " 'fer',\n",
       " 'feu',\n",
       " 'fi',\n",
       " 'fins',\n",
       " 'fora',\n",
       " 'gairebé',\n",
       " 'ha',\n",
       " 'han',\n",
       " 'has',\n",
       " 'haver',\n",
       " 'havia',\n",
       " 'he',\n",
       " 'hem',\n",
       " 'heu',\n",
       " 'hi',\n",
       " 'ho',\n",
       " 'i',\n",
       " 'igual',\n",
       " 'iguals',\n",
       " 'inclòs',\n",
       " 'ja',\n",
       " 'jo',\n",
       " \"l'hi\",\n",
       " 'la',\n",
       " 'les',\n",
       " 'li',\n",
       " \"li'n\",\n",
       " 'llarg',\n",
       " 'llavors',\n",
       " \"m'he\",\n",
       " 'ma',\n",
       " 'mal',\n",
       " 'malgrat',\n",
       " 'mateix',\n",
       " 'mateixa',\n",
       " 'mateixes',\n",
       " 'mateixos',\n",
       " 'me',\n",
       " 'mentre',\n",
       " 'meu',\n",
       " 'meus',\n",
       " 'meva',\n",
       " 'meves',\n",
       " 'mode',\n",
       " 'molt',\n",
       " 'molta',\n",
       " 'moltes',\n",
       " 'molts',\n",
       " 'mon',\n",
       " 'mons',\n",
       " 'més',\n",
       " \"n'he\",\n",
       " \"n'hi\",\n",
       " 'ne',\n",
       " 'ni',\n",
       " 'no',\n",
       " 'nogensmenys',\n",
       " 'només',\n",
       " 'nosaltres',\n",
       " 'nostra',\n",
       " 'nostre',\n",
       " 'nostres',\n",
       " 'o',\n",
       " 'oh',\n",
       " 'oi',\n",
       " 'on',\n",
       " 'pas',\n",
       " 'pel',\n",
       " 'pels',\n",
       " 'per',\n",
       " 'per que',\n",
       " 'perquè',\n",
       " 'però',\n",
       " 'poc',\n",
       " 'poca',\n",
       " 'pocs',\n",
       " 'podem',\n",
       " 'poden',\n",
       " 'poder',\n",
       " 'podeu',\n",
       " 'poques',\n",
       " 'potser',\n",
       " 'primer',\n",
       " 'propi',\n",
       " 'puc',\n",
       " 'qual',\n",
       " 'quals',\n",
       " 'quan',\n",
       " 'quant',\n",
       " 'que',\n",
       " 'quelcom',\n",
       " 'qui',\n",
       " 'quin',\n",
       " 'quina',\n",
       " 'quines',\n",
       " 'quins',\n",
       " 'què',\n",
       " \"s'ha\",\n",
       " \"s'han\",\n",
       " 'sa',\n",
       " 'sabem',\n",
       " 'saben',\n",
       " 'saber',\n",
       " 'sabeu',\n",
       " 'sap',\n",
       " 'saps',\n",
       " 'semblant',\n",
       " 'semblants',\n",
       " 'sense',\n",
       " 'ser',\n",
       " 'ses',\n",
       " 'seu',\n",
       " 'seus',\n",
       " 'seva',\n",
       " 'seves',\n",
       " 'si',\n",
       " 'sobre',\n",
       " 'sobretot',\n",
       " 'soc',\n",
       " 'solament',\n",
       " 'sols',\n",
       " 'som',\n",
       " 'son',\n",
       " 'sons',\n",
       " 'sota',\n",
       " 'sou',\n",
       " 'sóc',\n",
       " 'són',\n",
       " \"t'ha\",\n",
       " \"t'han\",\n",
       " \"t'he\",\n",
       " 'ta',\n",
       " 'tal',\n",
       " 'també',\n",
       " 'tampoc',\n",
       " 'tan',\n",
       " 'tant',\n",
       " 'tanta',\n",
       " 'tantes',\n",
       " 'te',\n",
       " 'tene',\n",
       " 'tenim',\n",
       " 'tenir',\n",
       " 'teniu',\n",
       " 'teu',\n",
       " 'teus',\n",
       " 'teva',\n",
       " 'teves',\n",
       " 'tinc',\n",
       " 'ton',\n",
       " 'tons',\n",
       " 'tot',\n",
       " 'tota',\n",
       " 'totes',\n",
       " 'tots',\n",
       " 'un',\n",
       " 'una',\n",
       " 'unes',\n",
       " 'uns',\n",
       " 'us',\n",
       " 'va',\n",
       " 'vaig',\n",
       " 'vam',\n",
       " 'van',\n",
       " 'vas',\n",
       " 'veu',\n",
       " 'vosaltres',\n",
       " 'vostra',\n",
       " 'vostre',\n",
       " 'vostres',\n",
       " 'érem',\n",
       " 'éreu',\n",
       " 'és',\n",
       " 'éssent',\n",
       " 'últim',\n",
       " 'ús']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtenim les stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('catalan')\n",
    "\n",
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476130c1178cb27a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A continuació, veurem un exemple de com eliminar les *stopwords* d'un text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73809b0f6613d867",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:44.738575Z",
     "start_time": "2024-01-24T12:01:43.729312Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['El', 'Barça', 'millor', 'equip', 'món', 'De', 'vegades']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminem les stopwords d'un text\n",
    "\n",
    "text = \"El Barça és el millor equip del món. De vegades.\"\n",
    "blob = TextBlob(text)\n",
    "tokens = [token for token in blob.words if token not in stop]\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d009063a838fcd0a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "També podem veure com, junt als stopwords, també s'eliminen els signes de puntuació."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bea4cdedd34fef6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Lematització i stemming\n",
    "\n",
    "La lematització és el procés de convertir una paraula a la seva forma base. Per exemple, la paraula *està* es converteix en *estar*. L'stemming, en canvi, consisteix en eliminar els afixos de les paraules. Per exemple, la paraula *està* es converteix en *est*.\n",
    "\n",
    "Ambdues tècniques ens permeten reduir el vocabulari del text i, per tant, reduir la dimensionalitat dels vectors de paraules; cosa que ens pot ajudar a millorar el rendiment dels nostres models.\n",
    "\n",
    "Vejam un exemple de com aplicar la lematització i el stemming amb TextBlob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb4548e61f0f9167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:44.773350Z",
     "start_time": "2024-01-24T12:01:43.754840Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Series', 'un', 'bon', 'cantant', 'si', 'no', 'forum', 'per', 'la', 'veu'],\n",
       " ['seri', 'un', 'bon', 'cantant', 'si', 'no', 'fora', 'per', 'la', 'veu'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lematitzem i stemitzem un text\n",
    "\n",
    "text = \"Series un bon cantant, si no fora per la veu.\"\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Lematitzem\n",
    "lemmas = [token.lemmatize() for token in blob.words]\n",
    "\n",
    "# Stemitzem\n",
    "\n",
    "stemes = [token.stem() for token in blob.words]\n",
    "\n",
    "lemmas, stemes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eea0e5658d55b4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## N-grams\n",
    "\n",
    "Els n-grams són seqüències de n paraules consecutives. Per exemple, si tenim el següent text: *'El Barça és el millor equip del món'*, els n-grams de 2 paraules serien: *'El Barça', 'Barça és', 'és el', 'el millor', 'millor equip', 'equip del', 'del món'*.\n",
    "\n",
    "Els n-grams ens permeten tenir en compte la relació entre paraules consecutives. Per exemple, en el text anterior, si només utilitzem unigrammes, no sabrem que *'millor'* i *'equip'* estan relacionades. En canvi, si utilitzem bigrames, sí que sabrem que *'millor equip'* estan relacionades.\n",
    "\n",
    "Per crear els n-grams en TextBlob, utilitzarem la funció `ngrams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef9376a1122fc5fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:44.777081Z",
     "start_time": "2024-01-24T12:01:43.799597Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['El', 'Barça']),\n",
       " WordList(['Barça', 'és']),\n",
       " WordList(['és', 'el']),\n",
       " WordList(['el', 'millor']),\n",
       " WordList(['millor', 'equip']),\n",
       " WordList(['equip', 'del']),\n",
       " WordList(['del', 'món'])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creem bigrames d'un text\n",
    "\n",
    "text = \"El Barça és el millor equip del món.\"\n",
    "blob = TextBlob(text)\n",
    "bigrams = blob.ngrams(n=2)\n",
    "\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f60d714bf9586f2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Representació del text\n",
    "\n",
    "Un cop hem pre-processat el text, hem de representar-lo en un format que pugui ser entès per l'ordinador. \n",
    "\n",
    "Veurem algunes de les representacions més utilitzades en processament de llenguatge natural. En aquesta pràctica, ens centrarem en algunes de les representacions més utilitzades en tasques de classificació de text.\n",
    " \n",
    "### One-hot encoding\n",
    "\n",
    "La representació *one-hot encoding* consisteix en crear un vector per a cada document. Aquest vector té tantes dimensions com paraules diferents hi ha en el vocabulari. Cada dimensió del vector representa una paraula del vocabulari i el valor de la dimensió és 1 si la paraula apareix en el document i 0 si no apareix.\n",
    "\n",
    "Per exemple, si tenim el següent vocabulari: *'casa', 'cotxe', 'avio'* i el següent document: *'casa cotxe casa'*, el vector resultant seria: *[1, 1, 0]*.\n",
    "\n",
    "Utilitzarem la classe `OneHotEncoder` de la llibreria `sklearn` per crear la representació *one-hot encoding*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19dcca3bebde5a6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:44.817981Z",
     "start_time": "2024-01-24T12:01:43.816680Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Creem la representació one-hot encoding d'un text\n",
    "\n",
    "text = \"El barça és el millor equip del món. De vegades.\"\n",
    "blob = TextBlob(text)\n",
    "tokens = numpy.array([token for token in blob.words if token not in stop])\n",
    "\n",
    "encoder = preprocessing.OneHotEncoder()\n",
    "\n",
    "encoded = encoder.fit_transform(tokens.reshape(-1, 1)).toarray()\n",
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad667c94d047037",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Bag of Words\n",
    "\n",
    "La representació *Bag of Words* consisteix en crear un vector per a cada document. Aquest vector té tantes dimensions com paraules diferents hi ha en el vocabulari. Cada dimensió del vector representa una paraula del vocabulari i el valor de la dimensió és el nombre d'aparicions d'aquesta paraula en el document.\n",
    "\n",
    "Per exemple, si tenim el següent vocabulari: *'casa', 'cotxe', 'avio'* i el següent document: *'casa cotxe casa'*, el vector resultant seria: *[2, 1, 0]*.\n",
    "\n",
    "Per crear la representació *Bag of Words* utilitzarem la classe `CountVectorizer` de la llibreria `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4673ccc11709bc65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:44.826140Z",
     "start_time": "2024-01-24T12:01:43.935113Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['casa' 'em' 'passo' 'treball' 'vida']\n",
      "[[2 1 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "# Creem la representació Bag of Words d'un text\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = \"Em passo la vida del treball a casa i de casa al treball.\"\n",
    "blob = TextBlob(text)\n",
    "tokens = [token for token in blob.words if token not in stop]\n",
    "text = ' '.join(tokens)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform([text])\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(bow.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff61ee98abadd2a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### TF-IDF\n",
    "\n",
    "La representació TF-IDF (*Term Frequency - Inverse Document Frequency*) consisteix en crear un vector per a cada document. Aquest vector té tantes dimensions com paraules diferents hi ha en el vocabulari. Cada dimensió del vector representa una paraula del vocabulari i el valor de la dimensió és el producte de la freqüència de la paraula en el document (TF) i la freqüència inversa de la paraula en el conjunt de documents (IDF).\n",
    "\n",
    "Per exemple, si tenim el següent vocabulari: *'casa', 'cotxe', 'avio'* i el següent document: *'casa cotxe casa'*, el vector resultant seria: *[2/3, 1/3, 0]*.\n",
    "\n",
    "Per crear la representació TF-IDF utilitzarem la classe `TfidfVectorizer` de la llibreria `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5745e2e4c168600e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:44.855983Z",
     "start_time": "2024-01-24T12:01:43.980211Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['casa' 'em' 'passo' 'treball' 'vida']\n",
      "[[0.60302269 0.30151134 0.30151134 0.60302269 0.30151134]]\n"
     ]
    }
   ],
   "source": [
    "# Creem la representació TF-IDF d'un text\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text = \"Em passo la vida del treball a casa i de casa al treball.\"\n",
    "blob = TextBlob(text)\n",
    "tokens = [token for token in blob.words if token not in stop]\n",
    "text = ' '.join(tokens)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform([text])\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7984789df920d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Word2Vec\n",
    "\n",
    "Word2Vec és un algorisme que ens permet representar les paraules en un espai vectorial. Aquesta representació ens permet tenir en compte la semàntica de les paraules. Per exemple, si representem les paraules *'rei'* i *'reina'* en un espai vectorial, veurem que la distància entre els vectors és menor que la distància entre els vectors de *'rei'* i *'cotxe'*.\n",
    "\n",
    "Per crear la representació Word2Vec utilitzarem la classe `Word2Vec` de la llibreria `gensim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9506855ed1f3fe1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:44.872657Z",
     "start_time": "2024-01-24T12:01:44.025925Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.3622725e-04,  2.3643136e-04,  5.1033497e-03,  9.0092728e-03,\n",
       "       -9.3029495e-03, -7.1168090e-03,  6.4588725e-03,  8.9729885e-03,\n",
       "       -5.0154282e-03, -3.7633716e-03,  7.3805046e-03, -1.5334714e-03,\n",
       "       -4.5366134e-03,  6.5540518e-03, -4.8601604e-03, -1.8160177e-03,\n",
       "        2.8765798e-03,  9.9187379e-04, -8.2852151e-03, -9.4488179e-03,\n",
       "        7.3117660e-03,  5.0702621e-03,  6.7576934e-03,  7.6286553e-04,\n",
       "        6.3508903e-03, -3.4053659e-03, -9.4640139e-04,  5.7685734e-03,\n",
       "       -7.5216377e-03, -3.9361035e-03, -7.5115822e-03, -9.3004224e-04,\n",
       "        9.5381187e-03, -7.3191668e-03, -2.3337686e-03, -1.9377411e-03,\n",
       "        8.0774371e-03, -5.9308959e-03,  4.5162440e-05, -4.7537340e-03,\n",
       "       -9.6035507e-03,  5.0072931e-03, -8.7595852e-03, -4.3918253e-03,\n",
       "       -3.5099984e-05, -2.9618145e-04, -7.6612402e-03,  9.6147433e-03,\n",
       "        4.9820580e-03,  9.2331432e-03, -8.1579173e-03,  4.4957981e-03,\n",
       "       -4.1370760e-03,  8.2453608e-04,  8.4986202e-03, -4.4621765e-03,\n",
       "        4.5175003e-03, -6.7869602e-03, -3.5484887e-03,  9.3985079e-03,\n",
       "       -1.5776526e-03,  3.2137157e-04, -4.1406299e-03, -7.6826881e-03,\n",
       "       -1.5080082e-03,  2.4697948e-03, -8.8802696e-04,  5.5336617e-03,\n",
       "       -2.7429771e-03,  2.2600652e-03,  5.4557943e-03,  8.3459532e-03,\n",
       "       -1.4537406e-03, -9.2081428e-03,  4.3705525e-03,  5.7178497e-04,\n",
       "        7.4419081e-03, -8.1328274e-04, -2.6384138e-03, -8.7530091e-03,\n",
       "       -8.5655687e-04,  2.8265631e-03,  5.4014288e-03,  7.0526563e-03,\n",
       "       -5.7031214e-03,  1.8588197e-03,  6.0888636e-03, -4.7980510e-03,\n",
       "       -3.1072604e-03,  6.7976294e-03,  1.6314756e-03,  1.8991709e-04,\n",
       "        3.4736372e-03,  2.1777749e-04,  9.6188262e-03,  5.0606038e-03,\n",
       "       -8.9173904e-03, -7.0415605e-03,  9.0145587e-04,  6.3925339e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creem la representació Word2Vec d'un text\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "text = \"Em passo la vida del treball a casa i de casa al treball.\"\n",
    "blob = TextBlob(text)\n",
    "tokens = [token for token in blob.words if token not in stop]\n",
    "\n",
    "model = Word2Vec([tokens], min_count=1)\n",
    "model.wv['casa']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6924355f3a8c8081",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Altres funcionalitats de TextBlob\n",
    "\n",
    "TextBlob ens permet realitzar altres tasques de processament de llenguatge natural. A continuació, veurem algunes d'aquestes funcionalitats.\n",
    "\n",
    "### Anàlisi de sentiments\n",
    "\n",
    "TextBlob ens permet analitzar els sentiments d'un text. Per fer-ho, utilitzarem l'atribut `sentiment` de l'objecte `TextBlob`. Aquest atribut ens retorna un objecte `Sentiment` amb dos atributs: `polarity` i `subjectivity`. La polaritat és un valor entre -1 i 1 que indica si el text és positiu o negatiu. La subjectivitat és un valor entre 0 i 1 que indica si el text és objectiu o subjectiu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b61aaad91e8e76c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:44.886046Z",
     "start_time": "2024-01-24T12:01:44.081959Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.4, subjectivity=0.55)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analitzem els sentiments d'un text\n",
    "\n",
    "text = \"This song is the shit. It's the best song I've ever heard!\"\n",
    "blob = TextBlob(text)\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f55cf26922583c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Classificació\n",
    "\n",
    "TextBlob ens permet classificar text utilitzant diferents classificadors. Per fer-ho, primer de tot hem d'entrenar el classificador.\n",
    "\n",
    "En aquesta pràctica, utilitzarem el classificador *Naive Bayes* per classificar text. Per entrenar el classificador, utilitzarem la classe `NaiveBayesClassifier` de la llibreria `textblob.classifiers`.\n",
    "\n",
    "Un cop entrenat el classificador, podem classificar text utilitzant la funció `classify` de l'objecte `NaiveBayesClassifier`.\n",
    "\n",
    "A continuació, veurem un exemple de com classificar text amb TextBlob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e18a286e19e6d06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:45.801479Z",
     "start_time": "2024-01-24T12:01:44.136838Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bug'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# En primer lloc crearem les dades d'entrenament i testeig\n",
    "\n",
    "# Training data\n",
    "train = [\n",
    "    ('The application crashes when I try to open it.', 'Bug'),\n",
    "    ('I would like to request a new feature.', 'Feature Request'),\n",
    "    ('How do I reset my password?', 'Question'),\n",
    "    ('There is a typo on the main page.', 'Bug'),\n",
    "    ('Could you add support for multiple languages?', 'Feature Request'),\n",
    "    ('Where can I find the user manual?', 'Question'),\n",
    "]\n",
    "\n",
    "# Testing data\n",
    "test = [\n",
    "    ('The app is not responding.', 'Bug'),\n",
    "    ('I think it would be great if you could add a dark mode.', 'Feature Request'),\n",
    "    ('What is the maximum file size I can upload?', 'Question'),\n",
    "]\n",
    "\n",
    "# Entrenem el classificador\n",
    "\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier(train)\n",
    "\n",
    "# Classifiquem text\n",
    "\n",
    "classifier.classify('The app crashes when I try to upload a file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fb538dfdb84eb1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:45.806246Z",
     "start_time": "2024-01-24T12:01:44.231080Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.accuracy(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed611a019c31f00c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "En aquesta pràctica hem vist com pre-processar text i com representar-lo en un format que pugui ser entès per l'ordinador. A més, hem vist com utilitzar algunes de les funcionalitats de TextBlob i llibreries relacionades.\n",
    "\n",
    "TextBlob, però, està basat en NLTK, una llibreria que es fonamenta en regles i, com hem vist a teoria, aquestes llibreries no sempre funcionen bé. Per tant, en la següent pràctica veurem com utilitzar eines basades en xarxes neuronals per pre-processar text i classificar-lo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
