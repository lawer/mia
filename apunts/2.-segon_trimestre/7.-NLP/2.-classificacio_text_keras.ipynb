{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "188b48ed5b49e64b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Classificador de notícies\n",
    "\n",
    "En aquesta pràctica, crearem un classificador de notícies utilitzant les tècniques de processament de llenguatge natural que hem vist a classe, centrades en la representació del text.\n",
    "\n",
    "Utilitzarem el `dataset` [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset) que conté 1.000.000 de notícies de 4 categories diferents.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "Per carregar el dataset, utilitzarem la llibreria `tensorflow_datasets` que ens permetrà carregar-lo de forma senzilla. Aquesta llibreria també ens permetrà fer la separació del dataset en conjunt d'entrenament i de test.\n",
    "\n",
    "Per carregar el dataset, utilitzarem la funció `load('ag_news_subset')` de la llibreria `tensorflow_datasets`. Aquesta funció retorna un `tf.data.Dataset` que conté els exemples del dataset. Aquest objecte és similar a un `numpy.array` però conté més informació sobre el dataset. En concret, conté els exemples del dataset i la seva etiqueta. Per accedir als exemples i les etiquetes, utilitzarem els atributs `data` i `label` de l'objecte `tf.data.Dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a731df9a4a91fd19",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preparació del dataset\n",
    "\n",
    "Per instal·lar la llibreria `tensorflow_datasets`, executarem la següent cel·la. Aquesta llibreria conté molts datasets que poden ser útils per a la pràctica de l'assignatura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9987ceaeb8f560be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T11:56:50.943374Z",
     "start_time": "2024-01-24T11:56:23.465806Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (2.15.0)\n",
      "Collecting keras\n",
      "  Using cached keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: absl-py in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: numpy in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from keras) (13.8.0)\n",
      "Requirement already satisfied: namex in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from keras) (0.12.1)\n",
      "Requirement already satisfied: ml-dtypes in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from keras) (0.3.2)\n",
      "Requirement already satisfied: packaging in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from keras) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from optree->keras) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Using cached keras-3.8.0-py3-none-any.whl (1.3 MB)\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.15.0\n",
      "    Uninstalling keras-2.15.0:\n",
      "      Successfully uninstalled keras-2.15.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.15.1 requires keras<2.16,>=2.15.0, but you have keras 3.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed keras-3.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: tensorflow in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (2.15.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow) (4.25.4)\n",
      "Requirement already satisfied: setuptools in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow) (1.66.1)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow) (2.15.0)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow)\n",
      "  Using cached keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.34.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
      "Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.8.0\n",
      "    Uninstalling keras-3.8.0:\n",
      "      Successfully uninstalled keras-3.8.0\n",
      "Successfully installed keras-2.15.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: tensorflow_datasets in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (4.9.7)\n",
      "Requirement already satisfied: absl-py in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow_datasets) (2.1.0)\n",
      "Requirement already satisfied: click in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow_datasets) (8.1.8)\n",
      "Requirement already satisfied: dm-tree in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow_datasets) (0.1.9)\n",
      "Requirement already satisfied: immutabledict in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow_datasets) (4.2.1)\n",
      "Requirement already satisfied: numpy in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow_datasets) (1.26.4)\n",
      "Requirement already satisfied: promise in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow_datasets) (4.25.4)\n",
      "Requirement already satisfied: psutil in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow_datasets) (6.0.0)\n",
      "Requirement already satisfied: pyarrow in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow_datasets) (17.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow_datasets) (2.32.3)\n",
      "Requirement already satisfied: simple-parsing in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow_datasets) (0.1.7)\n",
      "Requirement already satisfied: tensorflow-metadata in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow_datasets) (1.16.1)\n",
      "Requirement already satisfied: termcolor in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow_datasets) (2.4.0)\n",
      "Requirement already satisfied: toml in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow_datasets) (4.66.5)\n",
      "Requirement already satisfied: wrapt in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow_datasets) (1.14.1)\n",
      "Requirement already satisfied: array-record>=0.5.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow_datasets) (0.6.0)\n",
      "Requirement already satisfied: etils>=1.9.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (1.11.0)\n",
      "Requirement already satisfied: fsspec in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (2024.6.1)\n",
      "Requirement already satisfied: importlib_resources in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (6.5.2)\n",
      "Requirement already satisfied: typing_extensions in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (4.12.2)\n",
      "Requirement already satisfied: zipp in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (3.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (2024.8.30)\n",
      "Requirement already satisfied: attrs>=18.2.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from dm-tree->tensorflow_datasets) (24.2.0)\n",
      "Requirement already satisfied: six in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from simple-parsing->tensorflow_datasets) (0.16)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from tensorflow-metadata->tensorflow_datasets) (1.66.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Instalem les llibreries necessàries\n",
    "\n",
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6b60a6618dbd12a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T11:57:04.218018Z",
     "start_time": "2024-01-24T11:56:50.954479Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# En aquest tutorial, entrenarem molts models. Per utilitzar la memòria de la GPU de forma eficient,\n",
    "# configurarem tensorflow perquè creixi la memòria de la GPU quan sigui necessari.\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# Carreguem el dataset. Es descarregarà automàticament i es guardarà a la carpeta\n",
    "# /home/USUARI/tensorflow_datasets/ag_news_subset/1.0.0\n",
    "dataset = tfds.load('ag_news_subset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba463cc1c41c1298",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Automáticament, la funció `load` ha dividit el dataset en dos conjunts: un de train i un de test. Per accedir a aquests conjunts, utilitzarem els atributs `train` i `test` de l'objecte `dataset`. Aquests atributs són objectes `tf.data.Dataset` que contenen els exemples i les etiquetes del conjunt d'entrenament i de test. Per accedir als exemples i les etiquetes, utilitzarem els atributs `data` i `label` de l'objecte `tf.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ca34052cf84fbd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T11:57:04.237928Z",
     "start_time": "2024-01-24T11:57:04.224379Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'exemples de train: 120000\n",
      "Nombre d'exemples de test: 7600\n"
     ]
    }
   ],
   "source": [
    "# Separem el dataset en conjunt d'entrenament i de test\n",
    "ds_train = dataset['train']\n",
    "ds_test = dataset['test']\n",
    "# Vejam quants exemples hi ha en cada conjunt\n",
    "print('Nombre d\\'exemples de train:', len(ds_train))\n",
    "print('Nombre d\\'exemples de test:', len(ds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3645de832fbdacf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Imprimim els primers 5 exemples del conjunt d'entrenament. Com podem veure, cada exemple és una notícia i la seva etiqueta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60e7bededaa593f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T11:57:04.513469Z",
     "start_time": "2024-01-24T11:57:04.242231Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 (Sci/Tech) -> b'AMD Debuts Dual-Core Opteron Processor' b'AMD #39;s new dual-core Opteron chip is designed mainly for corporate computing applications, including databases, Web services, and financial transactions.'\n",
      "1 (Sports) -> b\"Wood's Suspension Upheld (Reuters)\" b'Reuters - Major League Baseball\\\\Monday announced a decision on the appeal filed by Chicago Cubs\\\\pitcher Kerry Wood regarding a suspension stemming from an\\\\incident earlier this season.'\n",
      "2 (Business) -> b'Bush reform may have blue states seeing red' b'President Bush #39;s  quot;revenue-neutral quot; tax reform needs losers to balance its winners, and people claiming the federal deduction for state and local taxes may be in administration planners #39; sights, news reports say.'\n",
      "3 (Sci/Tech) -> b\"'Halt science decline in schools'\" b'Britain will run out of leading scientists unless science education is improved, says Professor Colin Pillinger.'\n",
      "1 (Sports) -> b'Gerrard leaves practice' b'London, England (Sports Network) - England midfielder Steven Gerrard injured his groin late in Thursday #39;s training session, but is hopeful he will be ready for Saturday #39;s World Cup qualifier against Austria.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 21:30:59.025943: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\n",
    "# Imprimim els primers 5 exemples del conjunt d'entrenament\n",
    "for w in ds_train.take(5):\n",
    "    print(f\"{w['label']} ({classes[w['label']]}) -> {w['title']} {w['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8990e6d999d35898",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Representació del text\n",
    "\n",
    "Per poder entrenar un model de xarxes neuronals, necessitem representar el text com a vectors de nombres. En aquesta pràctica, utilitzarem la representació Bag-of-Words (BoW) que consisteix en representar cada paraula com un nombre. Aquesta representació és molt senzilla i no té en compte l'ordre de les paraules ni la seva semàntica. Però és una representació que funciona prou bé en molts casos.\n",
    "\n",
    "### Limitem el vocabulari\n",
    "\n",
    "En el dataset, hi ha moltes paraules que apareixen poques vegades. Aquestes paraules no són útils per entrenar el model i només augmenten la dimensionalitat de la representació. Per això, limitarem el vocabulari a les 50.000 paraules més freqüents. \n",
    "\n",
    "Per fer aquest procés utilitzarem la capa `TextVectorization` de `keras`. Aquesta capa ens permetrà convertir el text en seqüències de nombres. A més, ens permetrà limitar el vocabulari i convertir les paraules a minúscules. Per fer-ho, utilitzarem els paràmetres `max_tokens` i `standardize` de la capa `TextVectorization`.\n",
    "\n",
    "> Per construïr el vocabulari, utilitzarem solament 500 notícies del conjunt d'entrenament. Això és perquè el procés de construcció del vocabulari és molt lent i volem reduïr el temps d'execució del tutorial. En un cas real, utilitzaríem totes les notícies del conjunt d'entrenament. Asumim el risc de que alguna paraula del conjunt de test no estigui en el vocabulari i baixe l'accuracy del model; no deuria ser un problema greu de tota manera.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5ff0c95e3a816f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T11:57:05.627139Z",
     "start_time": "2024-01-24T11:57:04.519034Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 21:30:59.364616: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# Limitem el vocabulari a les 50.000 paraules més freqüents\n",
    "tamany_vocabulari = 50000\n",
    "\n",
    "# Creem la capa TextVectorization\n",
    "# max_tokens: nombre màxim de paraules que tindrà el vocabulari\n",
    "# standardize: funció que s'aplicarà a cada paraula per convertir-la a minúscules\n",
    "# output_mode: 'int' perquè la capa retorni seqüències de nombres (per defecte)\n",
    "vectorize_layer = keras.layers.TextVectorization(\n",
    "    max_tokens=tamany_vocabulari,\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    output_mode='int'\n",
    ")\n",
    "\n",
    "# Actualitzem el vocabulari de la capa TextVectorization amb les paraules del conjunt d'entrenament\n",
    "# Per fer-ho, primer creem un dataset amb les notícies del conjunt d'entrenament. Per reduïr el temps d'execució solament treballarem en 500 notícies\n",
    "ds_text = ds_train.take(500).map(lambda article: article['title'] + ' ' + article['description'])\n",
    "\n",
    "# Després, apliquem la capa TextVectorization a aquest dataset per construir el vocabulari\n",
    "vectorize_layer.adapt(ds_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cc57f6e7b54c64",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Ara ja podem accedir al vocabulari de la capa `TextVectorization` utilitzant l'atribut `get_vocabulary`. Aquest atribut retorna una llista amb les paraules del vocabulari. Com podem veure, la capa `TextVectorization` ha convertit les paraules a minúscules i ha eliminat els caràcters de puntuació."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60192d946334d282",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T11:57:05.688769Z",
     "start_time": "2024-01-24T11:57:05.619866Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'to', 'a', 'in', 'of', 'and', 'on', 'for']\n",
      "Tamany del vocabulari: 5335\n"
     ]
    }
   ],
   "source": [
    "# Imprimim les 10 primeres paraules del vocabulari\n",
    "print(vectorize_layer.get_vocabulary()[:10])\n",
    "\n",
    "# Imprimim el tamany del vocabulari\n",
    "tamany_vocabulari = len(vectorize_layer.get_vocabulary())\n",
    "print('Tamany del vocabulari:', tamany_vocabulari)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a2e337bfb751b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Utilitzant el vectoritzador, podem convertir un text en una seqüència de nombres. Vejam un exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d76842f2477c0f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T11:57:05.782036Z",
     "start_time": "2024-01-24T11:57:05.665156Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([ 35,  17,   4, 675,   1])>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer('this is a test sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89259e6a26a7e3ac",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Representació Bag-of-Words\n",
    "\n",
    "Encara que el significat de les paraules no és fàcil de deduir sense poder accedir al context, en alguns casos, la representació Bag-of-Words pot ser útil. Per exemple, en el text d'una notícia, la paraula `covid` pot ser un bon indicador que la notícia parla sobre la pandèmia de la COVID-19 i la paraula `snow` pot ser un bon indicador que la notícia parla sobre el temps atmosfèric.\n",
    "\n",
    "De les tècniques clàssiques de vectorització de text, la més senzilla és la representació Bag-of-Words (BoW). En aquesta representació, cada paraula es representa com un nombre. Per convertir un text en una representació BoW, primer creem un vector amb tants zeros com paraules hi ha en el vocabulari. Després, per cada paraula del text, incrementem en 1 el valor de la posició corresponent al vector. Per exemple, si el text és `this sentence is a test sentence`, el vector resultant seria `[1, 2, 1, 1, 0, 0, 0, 0, 0, 0, ...]`.\n",
    "\n",
    "Si recordem la representació one-hot, veurem que la representació BoW és molt similar. La diferència és que la representació one-hot serà una sèrie de vectors amb un sol 1 i la resta de valors a 0. En canvi, la representació BoW serà un vector amb tants 1 com vegades apareixi cada paraula. Podem considerar que la representació BoW seria la suma de vectors one-hot.\n",
    "\n",
    "Per exemple, si el text és `this sentence is a test sentence`, el vector one-hot de la primera paraula seria `[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]` i el vector one-hot de la segona paraula seria `[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...]`. La representació BoW seria la suma d'aquests dos vectors: `[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...]`.\n",
    "\n",
    "Per generar una representació BoW, utilitzarem aquesta tècnica per convertir cada paraula en un vector one-hot i després sumarem tots els vectors. Per fer-ho, utilitzarem la funció `to_bow` que crearem a continuació. Aquesta funció rep un text i retorna un vector amb la representació BoW del text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d57f10bba32951f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T11:57:05.850037Z",
     "start_time": "2024-01-24T11:57:05.757995Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 5., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_bow(text):\n",
    "    return tf.reduce_sum(tf.one_hot(vectorize_layer(text), tamany_vocabulari), axis=0)\n",
    "\n",
    "\n",
    "to_bow('My dog likes hot dogs on a hot day.').numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83bb2a026a54a15",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Entrenament del model de classificació BoW\n",
    "\n",
    "Per entrenar el model de classificació, utilitzarem la capa `TextVectorization` per convertir el text en una representació BoW. Després, utilitzarem una capa `Dense` per classificar el text. Aquesta capa és una xarxa neuronal totalment connectada amb una capa d'entrada i una capa de sortida. La capa d'entrada tindrà tants neurones com paraules hi hagi en el vocabulari i la capa de sortida tindrà tantes neurones com classes hi hagi en el problema. En aquest cas, la capa de sortida tindrà 4 neurones perquè el problema té 4 classes.\n",
    "\n",
    "### Representació BoW\n",
    "\n",
    "Convertim el text en la representació BoW utilitzant la funció `to_bow` que hem creat abans. Aquesta funció rep un text i retorna un vector amb la representació BoW del text.\n",
    "\n",
    "Utilitzarem la funció `map` de `tf.data.Dataset` per aplicar la funció `to_bow` a cada exemple del dataset. Aquesta funció retorna un nou dataset amb els exemples transformats. Després, utilitzarem la funció `batch` per agrupar els exemples en lots de 128 exemples i optimitzar el procés d'entrenament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "675f6b3ef8620ec0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T11:57:06.192009Z",
     "start_time": "2024-01-24T11:57:05.800531Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "ds_train_bow = ds_train.map(lambda x: (to_bow(x['title'] + ' ' + x['description']), x['label'])).batch(batch_size)\n",
    "ds_test_bow = ds_test.map(lambda x: (to_bow(x['title'] + ' ' + x['description']), x['label'])).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e85d6ad8ccb956f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Model de classificació\n",
    "\n",
    "Per crear el model de classificació, utilitzarem la classe `Sequential` de `keras`. Aquesta classe ens permet crear models seqüencials de forma senzilla. Per crear el model, li passarem una llista amb les capes que volem afegir al model. \n",
    "\n",
    "En aquest cas, el model tindrà una capa `Dense` amb 4 neurones perquè el problema té 4 classes i tantes neurones d'entrada com paraules hi hagi en el vocabulari (`input_shape=(tamany_vocabulari,)`). Aquesta capa tindrà una funció d'activació `softmax` perquè volem que la sortida del model sigui una distribució de probabilitats sobre les classes. \n",
    "\n",
    "Per entrenar el model, utilitzarem l'optimitzador `Adam` (ja que és un dels optimitzadors més utilitzats) i la funció de cost `SparseCategoricalCrossentropy` (ja que tenim un problema de classificació amb més de dues classes). Per compilar el model, utilitzarem la funció `compile` i li passarem l'optimitzador i la funció de cost. A més, li passarem la llista de mètriques que volem calcular durant l'entrenament. En aquest cas, volem calcular l'accuracy.\n",
    "\n",
    "Per últim, entrenarem el model utilitzant la funció `fit` i li passarem el dataset d'entrenament i el dataset de validació. Aquesta funció entrenarà el model i calcularà l'accuracy del model en el dataset de validació a cada època. Així, podrem veure com evoluciona l'accuracy del model durant l'entrenament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b26ef926987a1a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T11:58:47.961159Z",
     "start_time": "2024-01-24T11:57:06.211669Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 18s 19ms/step - loss: 0.5912 - accuracy: 0.8502 - val_loss: 0.4162 - val_accuracy: 0.8774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ff90ff5fe90>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(4, input_shape=(tamany_vocabulari,), activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(ds_train_bow, validation_data=ds_test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3230bbc2532904",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "El model ha aconseguit una accuracy de més de 0.86 en el conjunt d'entrenament; un nombre prou acceptable tenint en compte que hem simplificat el problema per reduïr el temps d'execució del tutorial. En un cas real, utilitzaríem totes les notícies del conjunt d'entrenament i el model seria més precís."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70afd32c0e143f09",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Clasificador com a xarxa neuronal\n",
    "\n",
    "Podem aprofitar que hem definit el vectoritzador com una capa de `keras` per crear un model de classificació com una xarxa neuronal completa. Aixó ens premetrà entrenar el model de classificació i el vectoritzador alhora; així com guardar i carregar el model de classificació i el vectoritzador en un sol fitxer.\n",
    "\n",
    "Crearem un model amb les següents capes:\n",
    "\n",
    "* La capa d'entrada agafrà un string amb el text de la notícia\n",
    "* La capa `TextVectorization` convertirà el text en array d'enters\n",
    "* La capa `one_hot` convertirà l'array d'enters en un array de vectors `one-hot`\n",
    "* La capa `reduce_sum` sumarà els vectors one-hot per obtenir la representació BoW\n",
    "* La capa `Dense` classificarà el text en una de les 4 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5401e6060eda70d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T11:58:48.099701Z",
     "start_time": "2024-01-24T11:58:47.970833Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_3 (Text  (None, None)              0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " tf.one_hot_1 (TFOpLambda)   (None, None, 5335)        0         \n",
      "                                                                 \n",
      " tf.math.reduce_sum_1 (TFOp  (None, 5335)              0         \n",
      " Lambda)                                                         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 4)                 21344     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21344 (83.38 KB)\n",
      "Trainable params: 21344 (83.38 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def extrau_text(x):\n",
    "    # Concatenem el títol i la descripció de la notícia\n",
    "    return x['title'] + x['description']\n",
    "\n",
    "\n",
    "def dict_a_tupla(x):\n",
    "    # Convertim el diccionari en una tupla amb el text i l'etiqueta\n",
    "    return (extrau_text(x), x['label'])\n",
    "\n",
    "\n",
    "# Creem el model\n",
    "# La capa d'entrada serà un string amb el text de la notícia\n",
    "inp = keras.Input(shape=(1,), dtype=tf.string)\n",
    "\n",
    "# Vectorize_layer retorna un tensor amb un enter per cada paraula del text\n",
    "enters = vectorize_layer(inp)\n",
    "\n",
    "# Convertim el tensor d'enters en un tensor d'arrays one-hot\n",
    "one_hots = tf.one_hot(enters, tamany_vocabulari)\n",
    "\n",
    "# Sumem els vectors one-hot per obtenir la representació BoW\n",
    "bow = tf.reduce_sum(one_hots, axis=1)\n",
    "\n",
    "# Creem la capa de sortida amb 4 neurones i funció d'activació softmax\n",
    "out = keras.layers.Dense(4, activation='softmax')(bow)\n",
    "\n",
    "# Creem el model\n",
    "model = keras.models.Model(inp, out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addc3f64b873cdb4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Entrenament del model\n",
    "\n",
    "Per entrenar el model, utilitzarem la funció `map` de `tf.data.Dataset` per aplicar la funció `dict_a_tupla` a cada exemple del dataset. Aquesta funció retorna un nou dataset amb els exemples transformats. Després, utilitzarem la funció `batch` per agrupar els exemples en lots de 128 exemples i optimitzar el procés d'entrenament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b79834c5b21cacf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:22.694455Z",
     "start_time": "2024-01-24T11:58:48.096499Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 19s 20ms/step - loss: 0.6325 - acc: 0.8314 - val_loss: 0.4442 - val_acc: 0.8671\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ff9c8641bd0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(ds_train.map(dict_a_tupla).batch(batch_size), validation_data=ds_test.map(dict_a_tupla).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83f187870687fed",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Generar els Vectors BoW automàticament\n",
    "\n",
    "En el model anterior, hem creat manualment la representació BoW. Això ens ha permès entendre com funciona la representació BoW i com es pot implementar en `keras`. Però, en la pràctica, no cal que creem manualment la representació BoW perquè `keras` ja té una opcio per fer-ho: el paràmetre `output_mode=\"count\"` de la capa `TextVectorization`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12faff952ab9ac92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:36.942065Z",
     "start_time": "2024-01-24T12:01:22.706456Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 3s 3ms/step - loss: 0.6143 - acc: 0.8423 - val_loss: 0.4370 - val_acc: 0.8717\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_4 (Text  (None, 5335)              0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 4)                 21344     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21344 (83.38 KB)\n",
      "Trainable params: 21344 (83.38 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creem la xarxa amb la capa TextVectorization\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=tamany_vocabulari, output_mode='count'),\n",
    "    keras.layers.Dense(4, input_shape=(tamany_vocabulari,), activation='softmax')\n",
    "])\n",
    "\n",
    "# Actualitzem el vocabulari de la capa TextVectorization amb les paraules del conjunt d'entrenament\n",
    "model.layers[0].adapt(ds_train.take(500).map(extrau_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(ds_train.map(dict_a_tupla).batch(batch_size), validation_data=ds_test.map(dict_a_tupla).batch(batch_size))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8488dac34f70654a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### N-grames\n",
    "\n",
    "La representació BoW és molt senzilla i no té en compte l'ordre de les paraules ni la seva semàntica. Per això, en alguns casos, la representació BoW pot ser millorada utilitzant n-grames. Un n-grames és una seqüència de n paraules consecutives. Per exemple, en el text `this sentence is a test sentence`, els 2-grames serien `this sentence`, `sentence is`, `is a`, `a test` i `test sentence`.\n",
    "\n",
    "Per generar una representació BoW utilitzant n-grames, veurem una altra forma de generar la representació BoW: `CountVectorizer`. Aquesta funció rep una llista de textos i retorna una matriu on cada fila és un text i cada columna és una paraula del vocabulari. El valor de cada cèl·lula és el nombre de vegades que apareix la paraula en el text. Per exemple, si el text és `this sentence is a test sentence`, el vector BoW seria `[1, 2, 1, 1, 0, 0, 0, 0, 0, 0, ...]` i la matriu BoW seria `[[1, 2, 1, 1, 0, 0, 0, 0, 0, 0, ...]]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9924e6aece2a7232",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:37.792256Z",
     "start_time": "2024-01-24T12:01:36.948562Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " {'i': 7, 'like': 11, 'hot': 4, 'dogs': 2, 'i like': 8, 'like hot': 12, 'hot dogs': 5, 'the': 16, 'dog': 0, 'ran': 14, 'fast': 3, 'the dog': 17, 'dog ran': 1, 'ran fast': 15, 'its': 9, 'outside': 13, 'its hot': 10, 'hot outside': 6}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "corpus = [\n",
    "    'I like hot dogs.',\n",
    "    'The dog ran fast.',\n",
    "    'Its hot outside.',\n",
    "]\n",
    "bigram_vectorizer.fit_transform(corpus)\n",
    "print(\"Vocabulary:\\n\", bigram_vectorizer.vocabulary_)\n",
    "bigram_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad89bd81db01fe1a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "El major problema dels `n-grames` és que el nombre de n-grames creix molt ràpidament amb n. Per exemple, si tenim un vocabulari de 50.000 paraules, el nombre de 2-grames serà de 2.500.000.000. Per això, en la pràctica, s'han de combinar o substituir per altres tècniques de reducció de dimensionalitat com ara `Word2Vec` o `GloVe`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829b25a07a249c81",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Representació TF-IDF\n",
    "\n",
    "Una altra representació molt utilitzada en el processament de llenguatge natural és la representació TF-IDF. Aquesta representació té en compte la freqüència de les paraules en el text i en el conjunt de documents. Això permet donar més importància a les paraules que defineixen un text.\n",
    "\n",
    "Dit d'una altra manera, si una paraula apareix moltes vegades en un text però també apareix en altres textos, aquesta paraula no aporta molta informació sobre el text (`and`). En canvi, si una paraula apareix moltes vegades en un text i no apareix tant en altres textos (`covid`), aquesta paraula aporta molta informació sobre el text.\n",
    "\n",
    "Per utilitzar la representació TF-IDF, utilitzarem el paràmetre `output_mode=\"tf-idf\"` de la capa `TextVectorization`. Fer la prova per veure sl'accuracy del model millora un poc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "959d9fbd51681493",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:02:01.217902Z",
     "start_time": "2024-01-24T12:01:37.810188Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 3s 3ms/step - loss: 0.4297 - acc: 0.8637 - val_loss: 0.3547 - val_acc: 0.8828\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_5 (Text  (None, 5335)              1         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 4)                 21344     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21345 (83.38 KB)\n",
      "Trainable params: 21344 (83.38 KB)\n",
      "Non-trainable params: 1 (8.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creem la xarxa amb la capa TextVectorization\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=tamany_vocabulari, output_mode='tf-idf'),\n",
    "    keras.layers.Dense(4, input_shape=(tamany_vocabulari,), activation='softmax')\n",
    "])\n",
    "\n",
    "# Actualitzem el vocabulari de la capa TextVectorization amb les paraules del conjunt d'entrenament\n",
    "model.layers[0].adapt(ds_train.take(500).map(extrau_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(ds_train.map(dict_a_tupla).batch(batch_size), validation_data=ds_test.map(dict_a_tupla).batch(batch_size))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fe580202e44097",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Representació Word2Vec\n",
    "\n",
    "La representació Word2Vec és una representació molt utilitzada en el processament de llenguatge natural. Aquesta representació té en compte el context de les paraules i permet fer operacions amb les paraules. Per exemple, si restem el vector de la paraula `king` i sumem el vector de la paraula `woman`, obtindrem un vector que serà molt similar al vector de la paraula `queen`.\n",
    "\n",
    "Per generar la representació Word2Vec, utilitzarem la llibreria `gensim`. Aquesta llibreria conté molts models de representació de paraules. En aquest cas, utilitzarem el model `word2vec-google-news-300` que conté la representació Word2Vec de 3 milions de paraules i frases. \n",
    "\n",
    "> La primera vegada que s'executi aquesta cel·la, la funció `load` descarregarà el model d'1.5GB. Això pot trigar uns minuts. Un cop descarregat, el model es guardarà a la carpeta `/home/USUARI/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz` i no caldrà descarregar-lo de nou.\n",
    "> Aquesta funció retorna un objecte `KeyedVectors` que conté la representació Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b9fa1231614bce",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Ara ja podem accedir a la representació Word2Vec de cada paraula. Per exemple, per accedir a la representació de la paraula `king`, utilitzarem la funció `get_vector` de l'objecte `KeyedVectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fee703eb5efc219a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.25976562e-01,  2.97851562e-02,  8.60595703e-03,  1.39648438e-01,\n",
       "       -2.56347656e-02, -3.61328125e-02,  1.11816406e-01, -1.98242188e-01,\n",
       "        5.12695312e-02,  3.63281250e-01, -2.42187500e-01, -3.02734375e-01,\n",
       "       -1.77734375e-01, -2.49023438e-02, -1.67968750e-01, -1.69921875e-01,\n",
       "        3.46679688e-02,  5.21850586e-03,  4.63867188e-02,  1.28906250e-01,\n",
       "        1.36718750e-01,  1.12792969e-01,  5.95703125e-02,  1.36718750e-01,\n",
       "        1.01074219e-01, -1.76757812e-01, -2.51953125e-01,  5.98144531e-02,\n",
       "        3.41796875e-01, -3.11279297e-02,  1.04492188e-01,  6.17675781e-02,\n",
       "        1.24511719e-01,  4.00390625e-01, -3.22265625e-01,  8.39843750e-02,\n",
       "        3.90625000e-02,  5.85937500e-03,  7.03125000e-02,  1.72851562e-01,\n",
       "        1.38671875e-01, -2.31445312e-01,  2.83203125e-01,  1.42578125e-01,\n",
       "        3.41796875e-01, -2.39257812e-02, -1.09863281e-01,  3.32031250e-02,\n",
       "       -5.46875000e-02,  1.53198242e-02, -1.62109375e-01,  1.58203125e-01,\n",
       "       -2.59765625e-01,  2.01416016e-02, -1.63085938e-01,  1.35803223e-03,\n",
       "       -1.44531250e-01, -5.68847656e-02,  4.29687500e-02, -2.46582031e-02,\n",
       "        1.85546875e-01,  4.47265625e-01,  9.58251953e-03,  1.31835938e-01,\n",
       "        9.86328125e-02, -1.85546875e-01, -1.00097656e-01, -1.33789062e-01,\n",
       "       -1.25000000e-01,  2.83203125e-01,  1.23046875e-01,  5.32226562e-02,\n",
       "       -1.77734375e-01,  8.59375000e-02, -2.18505859e-02,  2.05078125e-02,\n",
       "       -1.39648438e-01,  2.51464844e-02,  1.38671875e-01, -1.05468750e-01,\n",
       "        1.38671875e-01,  8.88671875e-02, -7.51953125e-02, -2.13623047e-02,\n",
       "        1.72851562e-01,  4.63867188e-02, -2.65625000e-01,  8.91113281e-03,\n",
       "        1.49414062e-01,  3.78417969e-02,  2.38281250e-01, -1.24511719e-01,\n",
       "       -2.17773438e-01, -1.81640625e-01,  2.97851562e-02,  5.71289062e-02,\n",
       "       -2.89306641e-02,  1.24511719e-02,  9.66796875e-02, -2.31445312e-01,\n",
       "        5.81054688e-02,  6.68945312e-02,  7.08007812e-02, -3.08593750e-01,\n",
       "       -2.14843750e-01,  1.45507812e-01, -4.27734375e-01, -9.39941406e-03,\n",
       "        1.54296875e-01, -7.66601562e-02,  2.89062500e-01,  2.77343750e-01,\n",
       "       -4.86373901e-04, -1.36718750e-01,  3.24218750e-01, -2.46093750e-01,\n",
       "       -3.03649902e-03, -2.11914062e-01,  1.25000000e-01,  2.69531250e-01,\n",
       "        2.04101562e-01,  8.25195312e-02, -2.01171875e-01, -1.60156250e-01,\n",
       "       -3.78417969e-02, -1.20117188e-01,  1.15234375e-01, -4.10156250e-02,\n",
       "       -3.95507812e-02, -8.98437500e-02,  6.34765625e-03,  2.03125000e-01,\n",
       "        1.86523438e-01,  2.73437500e-01,  6.29882812e-02,  1.41601562e-01,\n",
       "       -9.81445312e-02,  1.38671875e-01,  1.82617188e-01,  1.73828125e-01,\n",
       "        1.73828125e-01, -2.37304688e-01,  1.78710938e-01,  6.34765625e-02,\n",
       "        2.36328125e-01, -2.08984375e-01,  8.74023438e-02, -1.66015625e-01,\n",
       "       -7.91015625e-02,  2.43164062e-01, -8.88671875e-02,  1.26953125e-01,\n",
       "       -2.16796875e-01, -1.73828125e-01, -3.59375000e-01, -8.25195312e-02,\n",
       "       -6.49414062e-02,  5.07812500e-02,  1.35742188e-01, -7.47070312e-02,\n",
       "       -1.64062500e-01,  1.15356445e-02,  4.45312500e-01, -2.15820312e-01,\n",
       "       -1.11328125e-01, -1.92382812e-01,  1.70898438e-01, -1.25000000e-01,\n",
       "        2.65502930e-03,  1.92382812e-01, -1.74804688e-01,  1.39648438e-01,\n",
       "        2.92968750e-01,  1.13281250e-01,  5.95703125e-02, -6.39648438e-02,\n",
       "        9.96093750e-02, -2.72216797e-02,  1.96533203e-02,  4.27246094e-02,\n",
       "       -2.46093750e-01,  6.39648438e-02, -2.25585938e-01, -1.68945312e-01,\n",
       "        2.89916992e-03,  8.20312500e-02,  3.41796875e-01,  4.32128906e-02,\n",
       "        1.32812500e-01,  1.42578125e-01,  7.61718750e-02,  5.98144531e-02,\n",
       "       -1.19140625e-01,  2.74658203e-03, -6.29882812e-02, -2.72216797e-02,\n",
       "       -4.82177734e-03, -8.20312500e-02, -2.49023438e-02, -4.00390625e-01,\n",
       "       -1.06933594e-01,  4.24804688e-02,  7.76367188e-02, -1.16699219e-01,\n",
       "        7.37304688e-02, -9.22851562e-02,  1.07910156e-01,  1.58203125e-01,\n",
       "        4.24804688e-02,  1.26953125e-01,  3.61328125e-02,  2.67578125e-01,\n",
       "       -1.01074219e-01, -3.02734375e-01, -5.76171875e-02,  5.05371094e-02,\n",
       "        5.26428223e-04, -2.07031250e-01, -1.38671875e-01, -8.97216797e-03,\n",
       "       -2.78320312e-02, -1.41601562e-01,  2.07031250e-01, -1.58203125e-01,\n",
       "        1.27929688e-01,  1.49414062e-01, -2.24609375e-02, -8.44726562e-02,\n",
       "        1.22558594e-01,  2.15820312e-01, -2.13867188e-01, -3.12500000e-01,\n",
       "       -3.73046875e-01,  4.08935547e-03,  1.07421875e-01,  1.06933594e-01,\n",
       "        7.32421875e-02,  8.97216797e-03, -3.88183594e-02, -1.29882812e-01,\n",
       "        1.49414062e-01, -2.14843750e-01, -1.83868408e-03,  9.91210938e-02,\n",
       "        1.57226562e-01, -1.14257812e-01, -2.05078125e-01,  9.91210938e-02,\n",
       "        3.69140625e-01, -1.97265625e-01,  3.54003906e-02,  1.09375000e-01,\n",
       "        1.31835938e-01,  1.66992188e-01,  2.35351562e-01,  1.04980469e-01,\n",
       "       -4.96093750e-01, -1.64062500e-01, -1.56250000e-01, -5.22460938e-02,\n",
       "        1.03027344e-01,  2.43164062e-01, -1.88476562e-01,  5.07812500e-02,\n",
       "       -9.37500000e-02, -6.68945312e-02,  2.27050781e-02,  7.61718750e-02,\n",
       "        2.89062500e-01,  3.10546875e-01, -5.37109375e-02,  2.28515625e-01,\n",
       "        2.51464844e-02,  6.78710938e-02, -1.21093750e-01, -2.15820312e-01,\n",
       "       -2.73437500e-01, -3.07617188e-02, -3.37890625e-01,  1.53320312e-01,\n",
       "        2.33398438e-01, -2.08007812e-01,  3.73046875e-01,  8.20312500e-02,\n",
       "        2.51953125e-01, -7.61718750e-02, -4.66308594e-02, -2.23388672e-02,\n",
       "        2.99072266e-02, -5.93261719e-02, -4.66918945e-03, -2.44140625e-01,\n",
       "       -2.09960938e-01, -2.87109375e-01, -4.54101562e-02, -1.77734375e-01,\n",
       "       -2.79296875e-01, -8.59375000e-02,  9.13085938e-02,  2.51953125e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.get_vector('king')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade1217873948a0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "També podem accedir a les paraules més similars a una paraula. Per exemple, per accedir a les paraules més similars a la paraula `king`, utilitzarem la funció `most_similar` de l'objecte `KeyedVectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bffdc3f5aef651db",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kings -> 0.7138045430183411\n",
      "queen -> 0.6510956883430481\n",
      "monarch -> 0.6413194537162781\n",
      "crown_prince -> 0.6204220056533813\n",
      "prince -> 0.6159993410110474\n",
      "sultan -> 0.5864824056625366\n",
      "ruler -> 0.5797567367553711\n",
      "princes -> 0.5646552443504333\n",
      "Prince_Paras -> 0.5432944297790527\n",
      "throne -> 0.5422105193138123\n"
     ]
    }
   ],
   "source": [
    "for w, p in w2v.most_similar('king'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fa38ee95af3d11",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "El més interessant de la representació Word2Vec és que els vectors tenen una estructura matemàtica que permet fer operacions amb les paraules. Per exemple, si restem el vector de la paraula `king` i sumem el vector de la paraula `woman`, obtindrem un vector que serà molt similar al vector de la paraula `queen`.\n",
    "\n",
    "$$ KING - MAN + WOMAN = QUEEN $$\n",
    "\n",
    "Per fer aquesta operació, utilitzarem la funció `most_similar` de l'objecte `KeyedVectors` i li passarem els vectors de les paraules `king`, `woman` i `man`. Aquesta funció retornarà una llista amb les paraules més similars al vector resultant. Com podem veure, la paraula més similar és `queen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f79029055681603",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118193507194519)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king', 'woman'], negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a73014166e65daf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.src.layers import Embedding\n",
    "\n",
    "\n",
    "def gensim_to_keras_embedding(model, train_embeddings=False):\n",
    "    \"\"\"Get a Keras 'Embedding' layer with weights set from Word2Vec model's learned word embeddings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_embeddings : bool\n",
    "        If False, the returned weights are frozen and stopped from being updated.\n",
    "        If True, the weights can / will be further updated in Keras.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    `keras.layers.Embedding`\n",
    "        Embedding layer, to be used as input to deeper network layers.\n",
    "\n",
    "    \"\"\"\n",
    "    keyed_vectors = model  # structure holding the result of training\n",
    "    weights = keyed_vectors.vectors  # vectors themselves, a 2D numpy array    \n",
    "    index_to_key = keyed_vectors.index_to_key  # which row in `weights` corresponds to which word?\n",
    "\n",
    "    layer = Embedding(\n",
    "        input_dim=weights.shape[0],\n",
    "        output_dim=weights.shape[1],\n",
    "        weights=[weights],\n",
    "        trainable=train_embeddings,\n",
    "    )\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c073aa85513cc244",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 19s 7ms/step - loss: 1.3542 - acc: 0.4296 - val_loss: 1.3274 - val_acc: 0.4925\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 1.3023 - acc: 0.4952 - val_loss: 1.2841 - val_acc: 0.5149\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 1.2629 - acc: 0.5154 - val_loss: 1.2497 - val_acc: 0.5322\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 1.2308 - acc: 0.5313 - val_loss: 1.2209 - val_acc: 0.5459\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 1.2037 - acc: 0.5444 - val_loss: 1.1963 - val_acc: 0.5539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ff94e7cf050>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorize_layer,\n",
    "    gensim_to_keras_embedding(w2v, train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(dict_a_tupla).batch(128),validation_data=ds_test.map(dict_a_tupla).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7754f4cd2c374fd4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "El resultat no es molt bo. Això és perquè el model Word2Vec que hem utilitzat no té les paraules que apareixen en el dataset. Per exemple, si busquem la paraula `covid`, veurem que no apareix en el model.\n",
    "\n",
    "Per solucionar aquest problema hauriem d'utilitzar un model Word2Vec entrenat amb les paraules del dataset. Però això és molt lent i no ho farem en aquest tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
