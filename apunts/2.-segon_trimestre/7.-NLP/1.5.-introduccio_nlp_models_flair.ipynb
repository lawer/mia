{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f22c112528f9104",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Processament de llenguatge natural amb models pre-entrenats\n",
    "\n",
    "En l'exemple anterior hem vist com utilitzar llibreries com Nltk i TextBlob per processar text. Encara que son eficients i fàcils d'utilitzar, no sempre són les millors opcions per processar text en llenguatges diferents a l'anglès.\n",
    "\n",
    "Els models pre-entrenats són models que han estat entrenats amb grans quantitats de dades i que poden ser utilitzats per processar text en diferents idiomes. Aquests models requereixen més dades i temps de computació per ser entrenats, però una vegada entrenats si son eficients.\n",
    "\n",
    "El seu entrenament també és menys manual i facilita la seva utilització en diferents idiomes.\n",
    "\n",
    "En aquesta pràctica veurem com utilitzar models pre-entrenats per processar text en diferents idiomes.\n",
    "\n",
    "Utilitzarem els models pre-entrenats de la llibreria `flair` pero podriem utilitzar altres coleccions de models com `spaCy`.\n",
    "\n",
    "## Inicialització de l'entorn\n",
    "\n",
    "En primer lloc haurem d'instal·lar la llibreria `flair`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8406a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting flair\n",
      "  Downloading flair-0.15.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting boto3>=1.20.27 (from flair)\n",
      "  Downloading boto3-1.36.22-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting conllu<5.0.0,>=4.0 (from flair)\n",
      "  Downloading conllu-4.5.3-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Collecting deprecated>=1.2.13 (from flair)\n",
      "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting ftfy>=6.1.0 (from flair)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting gdown>=4.4.0 (from flair)\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.10.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from flair) (0.24.6)\n",
      "Collecting langdetect>=1.0.9 (from flair)\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting lxml>=4.8.0 (from flair)\n",
      "  Downloading lxml-5.3.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from flair) (3.10.0)\n",
      "Requirement already satisfied: more-itertools>=8.13.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from flair) (10.5.0)\n",
      "Collecting mpld3>=0.3 (from flair)\n",
      "  Downloading mpld3-0.5.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pptree>=3.1 (from flair)\n",
      "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from flair) (2.9.0.post0)\n",
      "Collecting pytorch-revgrad>=0.2.0 (from flair)\n",
      "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from flair) (2024.7.24)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from flair) (1.6.1)\n",
      "Collecting segtok>=1.5.11 (from flair)\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting sqlitedict>=2.0.0 (from flair)\n",
      "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tabulate>=0.8.10 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from flair) (0.9.0)\n",
      "Requirement already satisfied: torch>=1.13.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from flair) (2.6.0)\n",
      "Requirement already satisfied: tqdm>=4.63.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from flair) (4.66.5)\n",
      "Collecting transformer-smaller-training-vocab>=0.2.3 (from flair)\n",
      "  Downloading transformer_smaller_training_vocab-0.4.0-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.25.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (4.48.3)\n",
      "Collecting wikipedia-api>=0.5.7 (from flair)\n",
      "  Downloading wikipedia_api-0.8.1.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting bioc<3.0.0,>=2.0.0 (from flair)\n",
      "  Downloading bioc-2.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting jsonlines>=1.2.0 (from bioc<3.0.0,>=2.0.0->flair)\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting intervaltree (from bioc<3.0.0,>=2.0.0->flair)\n",
      "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting docopt (from bioc<3.0.0,>=2.0.0->flair)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting botocore<1.37.0,>=1.36.22 (from boto3>=1.20.27->flair)\n",
      "  Downloading botocore-1.36.22-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.20.27->flair)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3>=1.20.27->flair)\n",
      "  Downloading s3transfer-0.11.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from deprecated>=1.2.13->flair) (1.14.1)\n",
      "Requirement already satisfied: wcwidth in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from ftfy>=6.1.0->flair) (0.2.13)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from gdown>=4.4.0->flair) (4.12.3)\n",
      "Requirement already satisfied: filelock in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from gdown>=4.4.0->flair) (3.16.0)\n",
      "Requirement already satisfied: requests[socks] in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from gdown>=4.4.0->flair) (2.32.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from huggingface-hub>=0.10.0->flair) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from huggingface-hub>=0.10.0->flair) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from huggingface-hub>=0.10.0->flair) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from huggingface-hub>=0.10.0->flair) (4.12.2)\n",
      "Requirement already satisfied: six in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from langdetect>=1.0.9->flair) (1.16.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from matplotlib>=2.2.3->flair) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from matplotlib>=2.2.3->flair) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from matplotlib>=2.2.3->flair) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from matplotlib>=2.2.3->flair) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from matplotlib>=2.2.3->flair) (1.26.4)\n",
      "Requirement already satisfied: pillow>=8 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from matplotlib>=2.2.3->flair) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from matplotlib>=2.2.3->flair) (3.1.4)\n",
      "Requirement already satisfied: jinja2 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from mpld3>=0.3->flair) (3.1.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from scikit-learn>=1.0.2->flair) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from scikit-learn>=1.0.2->flair) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from scikit-learn>=1.0.2->flair) (3.5.0)\n",
      "Requirement already satisfied: networkx in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch>=1.13.1->flair) (3.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch>=1.13.1->flair) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch>=1.13.1->flair) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch>=1.13.1->flair) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch>=1.13.1->flair) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch>=1.13.1->flair) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch>=1.13.1->flair) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch>=1.13.1->flair) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch>=1.13.1->flair) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch>=1.13.1->flair) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch>=1.13.1->flair) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch>=1.13.1->flair) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch>=1.13.1->flair) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch>=1.13.1->flair) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch>=1.13.1->flair) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch>=1.13.1->flair) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from sympy==1.13.1->torch>=1.13.1->flair) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from transformers<5.0.0,>=4.25.0->transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from transformers<5.0.0,>=4.25.0->transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.4.5)\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair)\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: protobuf in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (4.25.4)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from botocore<1.37.0,>=1.36.22->boto3>=1.20.27->flair) (2.2.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from jsonlines>=1.2.0->bioc<3.0.0,>=2.0.0->flair) (24.2.0)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair) (1.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from beautifulsoup4->gdown>=4.4.0->flair) (2.6)\n",
      "Collecting sortedcontainers<3.0,>=2.0 (from intervaltree->bioc<3.0.0,>=2.0.0->flair)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from jinja2->mpld3>=0.3->flair) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from requests[socks]->gdown>=4.4.0->flair) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from requests[socks]->gdown>=4.4.0->flair) (3.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from requests[socks]->gdown>=4.4.0->flair) (2024.8.30)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown>=4.4.0->flair)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: psutil in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from accelerate>=0.26.0->transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair) (6.0.0)\n",
      "Downloading flair-0.15.1-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bioc-2.1-py3-none-any.whl (33 kB)\n",
      "Downloading boto3-1.36.22-py3-none-any.whl (139 kB)\n",
      "Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
      "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Downloading lxml-5.3.1-cp311-cp311-manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpld3-0.5.10-py3-none-any.whl (202 kB)\n",
      "Downloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
      "Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Downloading transformer_smaller_training_vocab-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading botocore-1.36.22-py3-none-any.whl (13.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading s3transfer-0.11.2-py3-none-any.whl (84 kB)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Building wheels for collected packages: langdetect, pptree, sqlitedict, wikipedia-api, docopt, intervaltree\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=6cf8e98f643d99c407c4e0a815b6c10055845867384fd10347d6e55b038797ef\n",
      "  Stored in directory: /home/carles/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "  Building wheel for pptree (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4608 sha256=1d73c5911c30a806e523560414eb1119dbc8be352c94002ac2f6b45c3e998007\n",
      "  Stored in directory: /home/carles/.cache/pip/wheels/68/8a/eb/d683aa6d09dc68ebfde2f37566ddc8807837c4415b4fd2b04c\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=31fd60aa76d799936422fcd3a6f59ad0b902a874aa083379c9cce75df2e3e780\n",
      "  Stored in directory: /home/carles/.cache/pip/wheels/73/63/89/7210274f9b7fb033b8f22671f64c0e0b55083d30c3c046a3ff\n",
      "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia-api: filename=Wikipedia_API-0.8.1-py3-none-any.whl size=15383 sha256=62390d623d5323fb91acc2cf97448f3a95b9ab5962ece114890a9447f4ea0276\n",
      "  Stored in directory: /home/carles/.cache/pip/wheels/0b/0f/39/e8214ec038ccd5aeb8c82b957289f2f3ab2251febeae5c2860\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=9f44d7ca1658759a2747ab0b2cdbe910ef95cab82bd857b1c2717b930012b279\n",
      "  Stored in directory: /home/carles/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
      "  Building wheel for intervaltree (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26094 sha256=55f344d69aadfac08af13776b3f0c33ff35795c99ab06c7a56704aa5264bf444\n",
      "  Stored in directory: /home/carles/.cache/pip/wheels/31/d7/d9/eec6891f78cac19a693bd40ecb8365d2f4613318c145ec9816\n",
      "Successfully built langdetect pptree sqlitedict wikipedia-api docopt intervaltree\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: sqlitedict, sortedcontainers, sentencepiece, pptree, docopt, segtok, PySocks, lxml, langdetect, jsonlines, jmespath, intervaltree, ftfy, deprecated, conllu, wikipedia-api, botocore, bioc, s3transfer, mpld3, gdown, pytorch-revgrad, boto3, transformer-smaller-training-vocab, flair\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/carles/Documentos/notebooks/.venv/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed PySocks-1.7.1 bioc-2.1 boto3-1.36.22 botocore-1.36.22 conllu-4.5.3 deprecated-1.2.18 docopt-0.6.2 flair-0.15.1 ftfy-6.3.1 gdown-5.2.0 intervaltree-3.1.0 jmespath-1.0.1 jsonlines-4.0.0 langdetect-1.0.9 lxml-5.3.1 mpld3-0.5.10 pptree-3.1 pytorch-revgrad-0.2.0 s3transfer-0.11.2 segtok-1.5.11 sentencepiece-0.2.0 sortedcontainers-2.4.0 sqlitedict-2.1.0 transformer-smaller-training-vocab-0.4.0 wikipedia-api-0.8.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install flair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e075ae45",
   "metadata": {},
   "source": [
    "## Tokenització\n",
    "\n",
    "Per tokenitzar amb spacy simplement creem una `Sentence` amb el text que volem processar. Aquesta `sentence` la farem servir per gran part de les operacions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "daa463042a366512",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:38.226841Z",
     "start_time": "2024-01-24T12:01:06.507611Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence[9]: \"Els dilluns a la tarda són molt llargs.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "\n",
    "sentence = Sentence('Els dilluns a la tarda són molt llargs.')\n",
    "\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade7583d",
   "metadata": {},
   "source": [
    "La sortida ens indica que hi ha 9 tokens a la frase.\n",
    "\n",
    "Per accedir als tokens podem recorrer l'objecte `Sentence` com si fos una llista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f2e41f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token[0]: \"Els\"\n",
      "Token[1]: \"dilluns\"\n",
      "Token[2]: \"a\"\n",
      "Token[3]: \"la\"\n",
      "Token[4]: \"tarda\"\n",
      "Token[5]: \"són\"\n",
      "Token[6]: \"molt\"\n",
      "Token[7]: \"llargs\"\n",
      "Token[8]: \".\"\n"
     ]
    }
   ],
   "source": [
    "for token in sentence:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2a507d",
   "metadata": {},
   "source": [
    "Si solament volem els tokens podem utilitzar el métode `text` de cada token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a09dd108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Els\n",
      "dilluns\n",
      "a\n",
      "la\n",
      "tarda\n",
      "són\n",
      "molt\n",
      "llargs\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in sentence:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973a6d15145f4f69",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Una de les funcions més interessants d'Spacy és que ens permet obtenir la categoria gramatical de cada paraula. Per fer-ho, crearem un classificador de text amb el model pre-entrenat `pos-multi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9a3d949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-17 23:45:54,740 SequenceTagger predicts: Dictionary with 17 tags: NOUN, PUNCT, ADP, VERB, ADJ, DET, PROPN, ADV, PRON, AUX, CCONJ, NUM, SCONJ, PART, X, SYM, INTJ\n",
      "Token[0]: \"Els\" → DET (0.9979)\n",
      "Token[1]: \"dilluns\" → NOUN (0.9716)\n",
      "Token[2]: \"a\" → ADP (0.9995)\n",
      "Token[3]: \"la\" → DET (0.9998)\n",
      "Token[4]: \"tarda\" → NOUN (0.9897)\n",
      "Token[5]: \"són\" → AUX (0.4685)\n",
      "Token[6]: \"molt\" → ADV (0.6738)\n",
      "Token[7]: \"llargs\" → ADJ (0.5771)\n",
      "Token[8]: \".\" → PUNCT (1.0000)\n"
     ]
    }
   ],
   "source": [
    "from flair.nn import Classifier\n",
    "\n",
    "tagger = Classifier.load('pos-multi')\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# Mostrem els tags de cada paraula\n",
    "for token in sentence:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad51527",
   "metadata": {},
   "source": [
    "Les categories gramaticals es poden consultar a: https://huggingface.co/flair/upos-multi/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71daa861f8f2f398",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tokenització de frases\n",
    "\n",
    "En alguns casos, també és necessari tokenitzar el text en frases. Per fer-ho, crearem un `Splitter` de tipus `SegtokSentenceSplitter` i utilitzarem el mètode `split` per obtenir les frases.\n",
    "\n",
    "Flair pot utilitzar molts altres splitters; utilitzem `SegtokSentenceSplitter` per ser simple i eficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a35f363ff99bedf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:44.503925Z",
     "start_time": "2024-01-24T12:01:43.683607Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence[9]: \"El Barça és el millor equip del món.\"\n",
      "Sentence[3]: \"De vegades.\"\n"
     ]
    }
   ],
   "source": [
    "from flair.splitter import SegtokSentenceSplitter\n",
    "\n",
    "# Tokenitzem el text en frases\n",
    "\n",
    "splitter = SegtokSentenceSplitter()\n",
    "\n",
    "text = \"El Barça és el millor equip del món. De vegades.\"\n",
    "sentences = splitter.split(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc5c0978ac88feb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stopwords i signes de puntuació\n",
    "\n",
    "Les *stopwords* són paraules que no aporten informació rellevant per a la tasca que estem realitzant. Per exemple, en la tasca de classificació de text, les *stopwords* no aporten informació per a la classificació.\n",
    "\n",
    "Per tant, en molts casos, és recomanable eliminar les *stopwords* del text abans d'aplicar qualsevol tècnica de processament de llenguatge natural.\n",
    "\n",
    " Per eliminar les *stopwords* del text, utilitzarem la llista de *stopwords* que té NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10095a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/carles/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('catalan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "73809b0f6613d867",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:44.738575Z",
     "start_time": "2024-01-24T12:01:43.729312Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence[12]: \"El Barça és el millor equip del món. De vegades.\" → [\"El\"/DET, \"Barça\"/PROPN, \"és\"/AUX, \"el\"/DET, \"millor\"/NOUN, \"equip\"/NOUN, \"del\"/X, \"món\"/NOUN, \".\"/PUNCT, \"De\"/ADP, \"vegades\"/PROPN, \".\"/PUNCT]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text = \"El Barça és el millor equip del món. De vegades.\"\n",
    "sentence = Sentence(text)\n",
    "tagger.predict(sentence)\n",
    "\n",
    "tokens = [token.text for token in sentence if token.text not in stop]\n",
    "tokens\n",
    "\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d009063a838fcd0a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "També podem veure com, junt als stopwords, també podem llevar els signes de puntuació."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b8b90f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['El', 'Barça', 'millor', 'equip', 'món', 'De', 'vegades']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [token.text for token in sentence if token.text not in stop and not token.tag == 'PUNCT']\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bea4cdedd34fef6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Lematització i stemming\n",
    "\n",
    "La lematització és el procés de convertir una paraula a la seva forma base. Per exemple, la paraula *està* es converteix en *estar*. L'stemming, en canvi, consisteix en eliminar els afixos de les paraules. Per exemple, la paraula *està* es converteix en *est*.\n",
    "\n",
    "Ambdues tècniques ens permeten reduir el vocabulari del text i, per tant, reduir la dimensionalitat dels vectors de paraules; cosa que ens pot ajudar a millorar el rendiment dels nostres models.\n",
    "\n",
    "Spacy no inclou un mètode per fer stemming, però si que inclou un mètode per fer lematització. Per fer-ho, accedim a l'atribut `lemma_` de cada paraula.\n",
    "\n",
    "La raó per la qual Spacy no inclou un mètode per fer stemming és perquè la lematització és més precisa que el stemming i spacy està dissenyat per a tasques de producció on la precisió és més important que la velocitat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb4548e61f0f9167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:44.773350Z",
     "start_time": "2024-01-24T12:01:43.754840Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seria',\n",
       " 'un',\n",
       " 'bo',\n",
       " 'cantant',\n",
       " ',',\n",
       " 'si',\n",
       " 'no',\n",
       " 'fora',\n",
       " 'per',\n",
       " 'el',\n",
       " 'veu',\n",
       " '.']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lematitzem i stemitzem un text\n",
    "\n",
    "text = \"Series un bon cantant, si no fora per la veu.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Lematitzem\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f60d714bf9586f2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Representació del text\n",
    "\n",
    "Un cop hem pre-processat el text, hem de representar-lo en un format que pugui ser entès per l'ordinador. \n",
    "\n",
    "Com ja hem comentat Spacy està centrat en tasques de producció i, per tant inclou els métodes més comuns actualment; especialment els basats en vectors de característiques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7984789df920d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Embeddings\n",
    "\n",
    "Spacy inclou un mètode per obtenir els embeddings del document i de la paraula. Aquests embeddings són vectors de característiques que representen el document o la paraula.\n",
    "\n",
    "Els embeddings són útils per a tasques de classificació de text, agrupació de text, etc.\n",
    "\n",
    "Com estem utilitzant el model petit (`ca_core_news_sm`), els embeddings son menys precisos i solament a nivell de document i no a nivell de paraula. Així i tot, ens poden ser útils per a tasques senzilles.\n",
    "\n",
    "Si necessitem embeddings més precisos, podem utilitzar els models mitjà (`ca_core_news_md`) o gran (`ca_core_news_lg`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "68cc01ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.7305475 , -0.12840228, -0.5396843 , -0.83717424, -1.3100742 ,\n",
       "        0.17650956, -0.1620681 ,  0.48682687,  0.06764957,  0.18184917,\n",
       "        0.8215378 , -0.40531054,  0.25735107,  0.93934184, -0.06517167,\n",
       "        0.04939266, -0.4045429 ,  0.24763483,  1.0905582 ,  0.6970573 ,\n",
       "       -0.31526443,  1.4649282 , -0.23816238, -0.4865251 , -0.09747415,\n",
       "       -0.28767216,  0.5814107 , -0.7125825 ,  0.49832496,  0.8188111 ,\n",
       "       -0.15080123,  0.1994148 ,  0.2723398 ,  0.8346703 , -0.7402689 ,\n",
       "       -1.4915727 , -0.9905634 ,  0.42298186,  0.6075589 ,  0.29053253,\n",
       "       -0.00331459, -1.3474356 , -0.58441734, -0.7741163 ,  0.05676066,\n",
       "       -0.17020352,  0.40141544,  0.6308312 , -0.40938064,  0.6958816 ,\n",
       "        0.3174107 ,  1.168798  , -0.04763202,  0.19581653,  0.08472364,\n",
       "       -0.01598583,  0.00706673, -0.8146202 , -0.65326816, -0.6593003 ,\n",
       "       -1.0186236 , -1.1242762 ,  0.1646251 ,  0.07394195, -0.7488492 ,\n",
       "       -0.5852267 , -0.8377841 , -0.47709122,  0.6825135 ,  0.5533623 ,\n",
       "        0.12207075,  0.6433471 ,  0.27980176,  0.4344176 , -0.07335876,\n",
       "        0.32564497,  0.19506216, -1.3851962 , -0.0534953 ,  0.3016157 ,\n",
       "       -0.01600329, -0.34237376, -0.9469802 , -0.3793352 ,  0.29250276,\n",
       "        0.30036998, -0.2898674 ,  0.8703751 ,  0.16369058,  0.8993411 ,\n",
       "       -0.5003162 ,  0.6861995 , -0.28650212,  1.036498  ,  0.281137  ,\n",
       "        0.81204104], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de516317",
   "metadata": {},
   "source": [
    "## Similaritat de text\n",
    "\n",
    "Aprofitant els embeddings, Spacy ens permet calcular la similitud entre dos documents o dos paraules. Aquesta similitud es calcula utilitzant la similitud del cosinus entre els vectors de característiques.\n",
    "\n",
    "Podem aprofiar aquesta funcionalitat per a buscar sinònims, per a recomanadors, etc.\n",
    "\n",
    "Ens caldrà carregar el model mitjà (`ca_core_news_md`) o gran (`ca_core_news_lg`) per a poder calcular el vector de característiques de les paraules i per buscar similituds entre vectors.\n",
    "\n",
    "### Exemple de sinònims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "76fb1341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ca-core-news-md==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/ca_core_news_md-3.8.0/ca_core_news_md-3.8.0-py3-none-any.whl (49.2 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/carles/Documentos/notebooks/.venv/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ca_core_news_md')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "# Carreguem un model de paraules\n",
    "\n",
    "spacy.cli.download(\"ca_core_news_md\")\n",
    "nlp = spacy.load(\"ca_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8d81f6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Els (['2.-Els', 'Fels', 'Algúns'], array([[1.    , 0.7737, 0.7637]], dtype=float32))\n",
      "dilluns (['dillun', '-dimarts', 'dimartts'], array([[1.    , 0.9662, 0.9593]], dtype=float32))\n",
      "a (['a', 'al', 'podrar'], array([[1.    , 0.343 , 0.3332]], dtype=float32))\n",
      "la (['deLa', 'PLa', 'ceva'], array([[1.    , 0.708 , 0.6945]], dtype=float32))\n",
      "tarda (['latarda', 'vespri', 'matí-migdia'], array([[1.    , 0.8403, 0.8243]], dtype=float32))\n",
      "són (['són.', 'Sóu', 'Arien'], array([[1.    , 0.8283, 0.7558]], dtype=float32))\n",
      "molt (['skolt', '-bastant', 'cagaire'], array([[1.    , 0.879 , 0.8005]], dtype=float32))\n",
      "llargs (['llargs-', 'hurts', 'camesllargues'], array([[1.    , 0.7822, 0.7382]], dtype=float32))\n",
      ". (['.', '</s>', 'i'], array([[1.    , 0.9258, 0.5367]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "def most_similar(word, topn=5):\n",
    "    ms = nlp.vocab.vectors.most_similar(\n",
    "        nlp(word).vector.reshape(1, nlp(word).vector.shape[0]), n=topn\n",
    "    )\n",
    "    words = [nlp.vocab.strings[w] for w in ms[0][0]]\n",
    "    distances = ms[2]\n",
    "    return words, distances\n",
    "\n",
    "\n",
    "doc = nlp(\"Els dilluns a la tarda són molt llargs.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, most_similar(token.text, topn=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed611a019c31f00c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "En aquesta pràctica hem vist com pre-processar text i com representar-lo en un format que pugui ser entès per l'ordinador. A més, hem vist com utilitzar algunes de les funcionalitats de TextBlob i llibreries relacionades.\n",
    "\n",
    "TextBlob, però, està basat en NLTK, una llibreria que es fonamenta en regles i, com hem vist a teoria, aquestes llibreries no sempre funcionen bé. Per tant, en la següent pràctica veurem com utilitzar eines basades en xarxes neuronals per pre-processar text i classificar-lo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
