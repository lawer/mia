{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f22c112528f9104",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Processament de llenguatge natural amb models pre-entrenats\n",
    "\n",
    "En l'exemple anterior hem vist com utilitzar llibreries com Nltk i TextBlob per processar text. Encara que son eficients i fàcils d'utilitzar, no sempre són les millors opcions per processar text en llenguatges diferents a l'anglès.\n",
    "\n",
    "Els models pre-entrenats són models que han estat entrenats amb grans quantitats de dades i que poden ser utilitzats per processar text en diferents idiomes. Aquests models requereixen més dades i temps de computació per ser entrenats, però una vegada entrenats si son eficients.\n",
    "\n",
    "El seu entrenament també és menys manual i facilita la seva utilització en diferents idiomes.\n",
    "\n",
    "En aquesta pràctica veurem com utilitzar models pre-entrenats per processar text en diferents idiomes.\n",
    "\n",
    "Utilitzarem els models pre-entrenats de la llibreria `spaCy` pero podriem utilitzar altres coleccions de models com `flair`.\n",
    "\n",
    "## Inicialització de l'entorn\n",
    "\n",
    "En primer lloc haurem d'instal·lar la llibreria `spaCy` i els models pre-entrenats en diferents idiomes; en aquest cas instal·larem el model de llenguatge en català `ca_core_news_sm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b8406a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: spacy in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.14.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/carles/Documentos/notebooks/.venv/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2d3f688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ca-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ca_core_news_sm-3.8.0/ca_core_news_sm-3.8.0-py3-none-any.whl (19.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/carles/Documentos/notebooks/.venv/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: ca-core-news-sm\n",
      "Successfully installed ca-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ca_core_news_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy.cli.download(\"ca_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6c2c0a",
   "metadata": {},
   "source": [
    "A continuació carregarem el model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fbcd4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ca_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e075ae45",
   "metadata": {},
   "source": [
    "## Tokenització\n",
    "\n",
    "Per tokenitzar amb spacy simplement cridem el mètode `nlp` amb el text que volem processar. Aquest mètode retorna un objecte `Doc` que farem servir per gran part de les operacions.\n",
    "\n",
    "Per accedir als tokens podem recorrer l'objecte `Doc` com si fos una llista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "daa463042a366512",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:38.226841Z",
     "start_time": "2024-01-24T12:01:06.507611Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Els', 'dilluns', 'a', 'la', 'tarda', 'són', 'molt', 'llargs', '.']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Els dilluns a la tarda són molt llargs.\")\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973a6d15145f4f69",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Una de les funcions més interessants d'Spacy és que ens permet obtenir la categoria gramatical de cada paraula. Per fer-ho, simplement accedim a l'atribut `pos_` de cada paraula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0185e46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Els DET\n",
      "dilluns NOUN\n",
      "a ADP\n",
      "la DET\n",
      "tarda NOUN\n",
      "són AUX\n",
      "molt ADV\n",
      "llargs ADJ\n",
      ". PUNCT\n",
      "esquerra NOUN\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad51527",
   "metadata": {},
   "source": [
    "Les categories gramaticals es poden consultar a: https://universaldependencies.org/u/pos/ o rebre una descripció amb `spacy.explain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f1469aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El DET determiner\n",
      "Barça PROPN proper noun\n",
      "és AUX auxiliary\n",
      "el DET determiner\n",
      "millor ADJ adjective\n",
      "equip NOUN noun\n",
      "d ADP adposition\n",
      "el DET determiner\n",
      "món NOUN noun\n",
      ". PUNCT punctuation\n",
      "De ADP adposition\n",
      "vegades NOUN noun\n",
      ". PUNCT punctuation\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71daa861f8f2f398",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tokenització de frases\n",
    "\n",
    "En alguns casos, també és necessari tokenitzar el text en frases. Per fer-ho, utilitzarem el métode `sents` de l'objecte `Doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a35f363ff99bedf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:44.503925Z",
     "start_time": "2024-01-24T12:01:43.683607Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Barça és el millor equip del món.\n",
      "De vegades.\n"
     ]
    }
   ],
   "source": [
    "# Tokenitzem el text en frases\n",
    "\n",
    "text = \"El Barça és el millor equip del món. De vegades.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc5c0978ac88feb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stopwords i signes de puntuació\n",
    "\n",
    "Les *stopwords* són paraules que no aporten informació rellevant per a la tasca que estem realitzant. Per exemple, en la tasca de classificació de text, les *stopwords* no aporten informació per a la classificació.\n",
    "\n",
    "Per tant, en molts casos, és recomanable eliminar les *stopwords* del text abans d'aplicar qualsevol tècnica de processament de llenguatge natural.\n",
    "\n",
    "TextBlob ens permet eliminar les *stopwords* del text. Per fer-ho, utilitzarem la propietat de *is_stop* de cadascún dels tokens del **Doc**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476130c1178cb27a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A continuació, veurem un exemple de com eliminar les *stopwords* d'un text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73809b0f6613d867",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:44.738575Z",
     "start_time": "2024-01-24T12:01:43.729312Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Barça, millor, equip, d, món, ., vegades, .]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminem les stopwords d'un text\n",
    "\n",
    "text = \"El Barça és el millor equip del món. De vegades.\"\n",
    "doc = nlp(text)\n",
    "tokens = [token for token in doc if not token.is_stop]\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d009063a838fcd0a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "També podem veure com, junt als stopwords, també podem llevar els signes de puntuació."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b8b90f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Barça, millor, equip, d, món, vegades]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [token for token in doc if not token.is_stop and not token.is_punct]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bea4cdedd34fef6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Lematització i stemming\n",
    "\n",
    "La lematització és el procés de convertir una paraula a la seva forma base. Per exemple, la paraula *està* es converteix en *estar*. L'stemming, en canvi, consisteix en eliminar els afixos de les paraules. Per exemple, la paraula *està* es converteix en *est*.\n",
    "\n",
    "Ambdues tècniques ens permeten reduir el vocabulari del text i, per tant, reduir la dimensionalitat dels vectors de paraules; cosa que ens pot ajudar a millorar el rendiment dels nostres models.\n",
    "\n",
    "Spacy no inclou un mètode per fer stemming, però si que inclou un mètode per fer lematització. Per fer-ho, accedim a l'atribut `lemma_` de cada paraula.\n",
    "\n",
    "La raó per la qual Spacy no inclou un mètode per fer stemming és perquè la lematització és més precisa que el stemming i spacy està dissenyat per a tasques de producció on la precisió és més important que la velocitat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb4548e61f0f9167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T12:01:44.773350Z",
     "start_time": "2024-01-24T12:01:43.754840Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seria',\n",
       " 'un',\n",
       " 'bo',\n",
       " 'cantant',\n",
       " ',',\n",
       " 'si',\n",
       " 'no',\n",
       " 'fora',\n",
       " 'per',\n",
       " 'el',\n",
       " 'veu',\n",
       " '.']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lematitzem i stemitzem un text\n",
    "\n",
    "text = \"Series un bon cantant, si no fora per la veu.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Lematitzem\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f60d714bf9586f2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Representació del text\n",
    "\n",
    "Un cop hem pre-processat el text, hem de representar-lo en un format que pugui ser entès per l'ordinador. \n",
    "\n",
    "Com ja hem comentat Spacy està centrat en tasques de producció i, per tant inclou els métodes més comuns actualment; especialment els basats en vectors de característiques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7984789df920d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Embeddings\n",
    "\n",
    "Spacy inclou un mètode per obtenir els embeddings del document i de la paraula. Aquests embeddings són vectors de característiques que representen el document o la paraula.\n",
    "\n",
    "Els embeddings són útils per a tasques de classificació de text, agrupació de text, etc.\n",
    "\n",
    "Com estem utilitzant el model petit (`ca_core_news_sm`), els embeddings son menys precisos i solament a nivell de document i no a nivell de paraula. Així i tot, ens poden ser útils per a tasques senzilles.\n",
    "\n",
    "Si necessitem embeddings més precisos, podem utilitzar els models mitjà (`ca_core_news_md`) o gran (`ca_core_news_lg`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "68cc01ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.7305475 , -0.12840228, -0.5396843 , -0.83717424, -1.3100742 ,\n",
       "        0.17650956, -0.1620681 ,  0.48682687,  0.06764957,  0.18184917,\n",
       "        0.8215378 , -0.40531054,  0.25735107,  0.93934184, -0.06517167,\n",
       "        0.04939266, -0.4045429 ,  0.24763483,  1.0905582 ,  0.6970573 ,\n",
       "       -0.31526443,  1.4649282 , -0.23816238, -0.4865251 , -0.09747415,\n",
       "       -0.28767216,  0.5814107 , -0.7125825 ,  0.49832496,  0.8188111 ,\n",
       "       -0.15080123,  0.1994148 ,  0.2723398 ,  0.8346703 , -0.7402689 ,\n",
       "       -1.4915727 , -0.9905634 ,  0.42298186,  0.6075589 ,  0.29053253,\n",
       "       -0.00331459, -1.3474356 , -0.58441734, -0.7741163 ,  0.05676066,\n",
       "       -0.17020352,  0.40141544,  0.6308312 , -0.40938064,  0.6958816 ,\n",
       "        0.3174107 ,  1.168798  , -0.04763202,  0.19581653,  0.08472364,\n",
       "       -0.01598583,  0.00706673, -0.8146202 , -0.65326816, -0.6593003 ,\n",
       "       -1.0186236 , -1.1242762 ,  0.1646251 ,  0.07394195, -0.7488492 ,\n",
       "       -0.5852267 , -0.8377841 , -0.47709122,  0.6825135 ,  0.5533623 ,\n",
       "        0.12207075,  0.6433471 ,  0.27980176,  0.4344176 , -0.07335876,\n",
       "        0.32564497,  0.19506216, -1.3851962 , -0.0534953 ,  0.3016157 ,\n",
       "       -0.01600329, -0.34237376, -0.9469802 , -0.3793352 ,  0.29250276,\n",
       "        0.30036998, -0.2898674 ,  0.8703751 ,  0.16369058,  0.8993411 ,\n",
       "       -0.5003162 ,  0.6861995 , -0.28650212,  1.036498  ,  0.281137  ,\n",
       "        0.81204104], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de516317",
   "metadata": {},
   "source": [
    "## Similaritat de text\n",
    "\n",
    "Aprofitant els embeddings, Spacy ens permet calcular la similitud entre dos documents o dos paraules. Aquesta similitud es calcula utilitzant la similitud del cosinus entre els vectors de característiques.\n",
    "\n",
    "Podem aprofiar aquesta funcionalitat per a recomanadors, cercadors de text, etc.\n",
    "\n",
    "Ens caldrà carregar el model mitjà (`ca_core_news_md`) o gran (`ca_core_news_lg`) per a poder calcular el vector de característiques de les paraules i per buscar similituds entre vectors.\n",
    "\n",
    "### Exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "76fb1341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ca-core-news-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ca_core_news_md-3.8.0/ca_core_news_md-3.8.0-py3-none-any.whl (49.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/carles/Documentos/notebooks/.venv/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ca_core_news_md')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "# Carreguem un model de paraules\n",
    "\n",
    "spacy.cli.download(\"ca_core_news_md\")\n",
    "nlp = spacy.load(\"ca_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b4faea22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9890047051139093"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creem dues frases i calculem la similitud\n",
    "\n",
    "text1 = \"M'agrada el futbol.\"\n",
    "\n",
    "text2 = \"M'agrada el bàsquet.\"\n",
    "\n",
    "doc1 = nlp(text1)\n",
    "doc2 = nlp(text2)\n",
    "\n",
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6c614e",
   "metadata": {},
   "source": [
    "### Sinónims\n",
    "\n",
    "Podriem utilitzar la similitud de text per trobar sinónims. Per exemple, podem calcular la similitud entre dues paraules i si la similitud és superior a un cert llindar, podem considerar que són sinónims.\n",
    "\n",
    "De tota manera, aquesta tècnica no és perfecta i no sempre funciona. Per exemple, la paraula *cotxe* i *vehicle* són sinònims, però la similitud entre elles pot no ser suficientment alta.\n",
    "\n",
    "En aquests casos, podem utilitzar un tesaure com WordNet per trobar sinònims. Utilitzarem `spacy-wordnet` per integrar WordNet amb Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c32d8ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: nltk in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from nltk) (4.66.5)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/carles/Documentos/notebooks/.venv/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Utilitzem wordnet i omw per obtenir sinònims\n",
    "\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "29039b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/carles/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw to /home/carles/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/carles/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descarreguem els recursos necessaris\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw\")\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "377f3304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ca\n",
      "canis_familiaris\n",
      "gos\n",
      "gos_domèstic\n"
     ]
    }
   ],
   "source": [
    "# Obtenim els sinònims de la paraula 'gos'\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "for synset in wn.synsets(u'gos',lang=('cat')):\n",
    "    for lemma in synset.lemma_names(u'cat'):\n",
    "        print(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed611a019c31f00c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "En aquesta pràctica hem vist com pre-processar text i com representar-lo en un format que pugui ser entès per l'ordinador. A més, hem vist com utilitzar algunes de les funcionalitats de Spacy i llibreries relacionades.\n",
    "\n",
    "Spacy és una llibreria molt potent i fàcil d'utilitzar per processar text en diferents idiomes. A més, inclou moltes funcionalitats útils per a tasques de processament de llenguatge natural."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
